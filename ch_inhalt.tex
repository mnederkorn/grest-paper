\section{Introduction}
There are practical applications (cite) which are able to be modeled by or reduced to various games. Considering the many ways there are to solve those games, the possibility to (repeatedly) reduce them to one another and various ways to implement any of those algorithms there are many potential means to solve problems on those games. We want to see how the different ways to solve problems, the reductions between those problems and the combination of the two play out and compare to one another in a real-world implementation.
\subsection{Task}
The games we concern ourselves here are \textit{Parity Games} (PG), \textit{Mean Payoff Games} (MPG), \textit{Energy Games} (EG), \textit{Discounted Payoff Games} (DPG) and (stopping) \textit{Simple Stochastic Games} (SSG). For any of those games there are multiple problems, but the ones we concern outselves here are ``what is the value of vertex $v$ under optimal strategies from both players?'' and ``what are the optimal strategies?''. For both of those problems, there may already be multiple algorithms to solve them for any specific game directly, some of which are generalizable to multiple of the games to some degree, such as Kleene Iteration and Strategy Iteration. Additionally we can reduce the games to another as follows:
\begin{center}
	\begin{tikzpicture}
		\node (A) at (0,0) {$PG$};
		\node[right=of A] (B) {$MPG$};
		\node[right=of B] (C) {$DPG$};
		\node[right=of C] (D) {$SSG$};
		\node[below=of B] (E) {$EG$};
		\path [->] (A) edge (B);
		\path [->] (B) edge (C);
		\path [->] (C) edge (D);
		\path [->] (B) edge (E);
	\end{tikzpicture}
\end{center}
The reductions themselves are computationally relatively simple compared to the problems themselves. Some of them, however, produce graphs that are bigger in some way ($PG\rightarrow MPG$, $DPG\rightarrow SSG$), which increases the complexity of subsequently applied algorithms, or recursively reduces to multiple sub-problems ($MPG\rightarrow EG$).
Another part we have control over is how exactly the algorithms are implemented. The biggest factor here being that, due to their nature, the edge-weights can be seen as an incidence matrix and can therefore make use of matrix operations, especially for simple operations such as addition and multiplication.
\subsection{Structure}
The paper is structured as follows:

In Chapter 2 we will first define the different kinds of games we are concerned with and the notion of $value$ as on objective for the respective games as well as (memory-less) strategies as a means to achieve those values.\\
In Chapter 3 we will first explain in 3.1 the idea of value and strategy iteration and then both concrete implementations of those as well as other algorithms for the respective games. In 3.2 we will then show how the different games and their underlying graphs can be translated to one another and how the problems posed on those games are finally reduced.\\
In Chapter 4 we show the specifics as to how the theoretical algorithms were internally implemented and elaborate on the choices and decisions arising in the implementations of the reductions and solutions.\\
In Chapter 5 we show how the implementations embeds to be used to solve specific problems or to demonstrate with the GUI.\\
In Chaper 6 we show the kinds of graphs and problems we used to evaluate the solutions and the potential benefit of prior reduction as well as the results of the evaluation.\\
In Chapter 7 we interpret the results of the evaluation and give suggestions and ideas for further improvement.
\section{Preliminaries}
Foundational to our task are the different kinds of \textit{Infinite Games} and how to determine the outcome of each such game. To do so, we also want a notion of \textit{strategies} on such Infinite Games. To later reduce the games to one another, we furthermore want a definition of a \textit{reduction} in the computational complexity sense.
\subsection{Directed Graphs}
Let $V$ be a finite set of Vertices, let $E\subseteq V \times V$ be a set of Edges, then $G = (V,E)$ is a \textit{Directed Graph}.
%We also define $src\colon E\rightarrow V$ as $src((u,v))=u$ and $tgt\colon E\rightarrow V$ as $tgt((u,v))=v$.
We also define
\begin{align*}
	pre\colon V\rightarrow \mathcal{P}(V) && pre(v)=\{u\in V\mid (u,v)\in E\}\\
	post\colon V\rightarrow \mathcal{P}(V) && post(v)=\{u\in V\mid (v,u)\in E\}
\end{align*}
\subsection{Arenas}
An arena is an extension of Directed Graphs, where the set of Vertices, $V$, is partitioned into two disjunct subsets $V_0$ and $V_1$, respectively denoting the regions where player 0, also represented by $\Square$, and player 1, also represented by $\Circle$, are to play. We also require that the out-degree of every vertex is at least one, so that any play on the Arena can always be prolonged.\newline
Formally, let $(V,E)$ be a non-trivial Directed Graph, $V_0\cup V_1 = V,~V_0\cap V_1 = \emptyset$ be a partition of V and $\forall v\in V\colon post(v)\ne \emptyset$,
then $A=(V,(V_0,V_1),E)$ is an Arena.

\begin{wrapfigure}[5]{l}{4.5cm}
	\begin{tikzpicture}
		\node[style={regular polygon,regular polygon sides=4}, draw=black] (A) at (0,0) {$v_0$};
		\node[style={regular polygon,regular polygon sides=4}, draw=black] (B) at (1.75,0) {$v_1$};
		\node[shape=circle, draw=black] (C) at (3.5,0) {$v_2$};
		\node[shape=circle, draw=black] (D) at (0,-1.75) {$v_3$};
		\node[shape=circle, draw=black] (E) at (3.5,-1.75) {$v_4$};
		\path [->] (A) edge (B);
		\path [->] (B) edge[out=15, in=165] (C);
		\path [->] (C) edge[out=195, in=345] (B);
		\path [->] (C) edge (E);
		\path [->] (E) edge[out=285, in=255,looseness=8] (E);
		\path [->] (A) edge (D);
		\path [->] (B) edge (D);
		\path [->] (D) edge[out=285, in=255,looseness=8] (D);
	\end{tikzpicture}
\caption{Arena A}
\end{wrapfigure}

\textbf{Example 1:}

An Arena
\begin{gather*}
	A=(\{v_0,v_1,v_2,v_3,v_4\},(\{v_0,v_1\},\{v_2,v_3,v_4\}),\\
	\{(v_0,v_1),(v_0,v_3),(v_1,v_2),(v_2,v_1),\\
	(v_2,v_4),(v_4,v_4),(v_1,v_3),(v_3,v_3)\})
\end{gather*}
\subsection{Positions, Moves}
A \textit{position}, $\pi_i=(v_0, v_1, \ldots, v_i)$, in a Game describes a finite path on the underlying graph, i.e. a position is an element of $V^+$. E.g. in Fig. 1 starting at $v_0$, $(v_0,v_1,v_2,v_4)$ could be a play.\\
A \textit{move} is an extension of a position in the graph by one more step. E.g. in Fig. 1 $(v_0,v_1,v_2)\mapsto (v_0,v_1,v_2,v_4)$ could be a move. Player i chooses the n-th move $(\ldots, v_{n-1})\mapsto (\ldots, v_{n-1}, v_n)$ for $v_{n-1}\in V_i\in (V_0,V_1)$.
\subsection{Strategies}
A \textit{strategy}, $V^*\times V_i\rightarrow V$, is a function by which player $i$ choses the next move for any given position.\newline
We call a strategy \textit{memoryless}, if for any given position the next move only depends on the last vertex of the position, i.e. $V_i\rightarrow V$.
We will refer to memoryless strategies of player 0 as $\sigma$ and of player 1 as $\tau$.\\
E.g. in Fig. 1 $\sigma = \{(v_0,v_1),(v_1,v_2)\}$ and $\tau = \{(v_2,v_1),(v_3,v_3),(v_4,v_4)\}$ could be strategies.\\
We call a memoryless strategy \textit{optimal}, if it achieves the best value possible for the respective player regardless of starting vertex. There may be multiple optimal strategies for any given game.
\subsection{Plays}
A \textit{play} describes the path of arbitrary length $\langle v_0, v_1, \ldots\rangle$ the player go trough in the process of playing the game.
We refer to the play generated by applying strategies $\sigma, \tau$ to game $G$ starting at $v_0 \in V$ as $\pi_{\sigma, \tau}(G, v_0)=\langle v_0, v_1, \ldots\rangle$\newline
E.g. if we take the previous example strategies and apply them to $\pi_{\sigma, \tau}(A,v_0)$ we get the play $\langle v_0, v_1, v_2, v_1, \ldots\rangle$. The play can be arbitrarily prolonged by applying the moves $\sigma\colon v_1\mapsto v_2$ and $\tau \colon v_2\mapsto v_1$.
\subsection{Infinite Games}
Infinite Games are a category of games played by two players on a finite, directed graph. They are infinite in the sense that we require the out-degree of every vertex to be at least one. As such, regardless of the strategies chosen by the players, they never terminate.
Simple Stochastic Games as such can be infinite, but the subcategory we mostly care about, stopping-Simple Stochastic Games is per definition finite.\newpage
\subsubsection{Parity Games}
Parity Games are played by two players, \textit{Even} or player 0 ($\Square$) and \textit{Odd} or player 1 ($\Circle$).
A Parity Game, $PG =(A,p)$, is played on an Arena $A$ with a priority function $p\colon V\rightarrow\mathbb{N}_0$.\newline
Let $\pi_{\sigma, \tau}(PG, v_0)=\langle v_0, v_1, \ldots\rangle$ be the \textit{play} resulting from applying the strategies $\sigma$ and $\tau$ to Parity Game $PG$. Let
\begin{gather*}
	\#_\infty(\pi_{\sigma, \tau}(PG, v_0))=\\
	\{i\in \{0, 1, \ldots, \left|V\right|\}\mid\forall j \in \mathbb{N}_0\colon \exists n\in \mathbb{N}_0\colon j<\left|\langle v\in \langle v_0, \ldots, v_n\rangle\colon p(v)=i \rangle\right|\}
\end{gather*}
be the set of priorities that appear arbitrarily often in the play.\\
If $max(\#_\infty(\pi_{\sigma, \tau}(PG, v_0)))\coloneqq v_{\sigma,\tau}(v_0)$, the value of vertex $v_0$ is even, then \textit{Even} wins and vice versa. Optimal strategies for \textit{Even}/\textit{Odd} are those that result in the most/least starting vertices resulting in an even/odd value.

\begin{wrapfigure}[9]{l}{5.5cm}
	\begin{tikzpicture}
		\node[style={regular polygon,regular polygon sides=4}, draw=black,label={5}] (A) at (0,0) {$v_0$};
		\node[style={regular polygon,regular polygon sides=4}, draw=black,label={4}] (C) at (3.5,0) {$v_2$};
		\node[style={regular polygon,regular polygon sides=4}, draw=black,label={below:2}] (D) at (0,-1.75) {$v_3$};
		\node[style={regular polygon,regular polygon sides=4}, draw=black,label={below:3}] (F) at (3.5,-1.75) {$v_5$};
		\node[shape=circle, draw=black,label={2}] (B) at (1.75,0) {$v_1$};
		\node[shape=circle, draw=black,label={below:0}] (E) at (1.75,-1.75) {$v_4$};
		\path [->] (A) edge[out=15, in=165] (B);
		\path [->] (B) edge[out=195, in=345] (A);
		\path [->] (B) edge[out=15, in=165] (C);
		\path [->] (C) edge[out=195, in=345] (B);
		\path [->] (D) edge (A);
		\path [->] (E) edge (B);
		\path [->] (C) edge[out=285, in=75] (F);
		\path [->] (F) edge[out=105, in=255] (C);
		\path [->] (D) edge[out=15, in=165] (E);
		\path [->] (E) edge[out=195, in=345] (D);
		\path [->] (F) edge (E);
	\end{tikzpicture}
	\caption{PG}
\end{wrapfigure}

\textbf{Example 2:}

Playing $PG$ with $\sigma=\{(v_0,v_1),(v_2,v_5),(v_3,v_4),(v_5,v_2)\}$ and $\tau=\{(v_1,v_0),(v_4,v_1)\}$, the respective optimal strategies, results in play $\pi_{\sigma, \tau}(PG, v_0)=\langle v_0, v_1, v_0, ..\rangle$ for which $v_{\sigma,\tau}(v_0)=5$ is odd and the play is therefore winning for \textit{Odd}.
\subsubsection{Mean Payoff Games}
Mean Payoff Games are played by two players, \textit{Max} or player 0 ($\Square$) and \textit{Min} or player 1 ($\Circle$).
A Mean Payoff Game, $MPG = (A,w)$, is played on an Arena $A$ and an edge-weight function $w\colon E\rightarrow\{-W, \ldots, -1, 0, 1, \ldots, W\}$, $d\in \mathbb{N}_0$.\newline
\textit{Max} aims to chose their strategy for a given play $\pi_{\sigma, \tau}(MPG, v_0)=\langle v_0, v_1, \ldots\rangle$ such as to maximize the \textit{mean payoff} 
\begin{align*}
	\liminf\limits_{n\rightarrow \infty} \left(\dfrac{1}{n}\sum_{i=0}^{n-1}w((v_i,v_{i+1}))\right).
\end{align*}
We call this the \textit{value} $v_{\sigma,\tau}(v)$ of a vertex $v$. Given that those values are real numbers, we will round them from here on out. Optimal strategies are those that maximize/minimize the mean payoff for a given MPG regardless of the initial vertex.

\begin{wrapfigure}[9]{l}{4.5cm}
	\begin{tikzpicture}
		\node[style={regular polygon,regular polygon sides=4}, draw=black] (A) at (0,0) {$v_0$};
		\node[shape=circle, draw=black] (B) at (1.75,0) {$v_1$};
		\node[shape=circle, draw=black] (C) at (3.5,0) {$v_2$};
		\node[shape=circle, draw=black] (D) at (0,-1.75) {$v_3$};
		\node[shape=circle, draw=black] (E) at (1.75,-1.75) {$v_4$};
		\path [->] (A) edge[out=105, in=75,looseness=8] node[above]{$2$} (A);
		\path [->] (B) edge node[above]{$4$} (A);
		\path [->] (C) edge node[above]{$-2$} (B);
		\path [->] (C) edge[out=105, in=75,looseness=8] node[above]{$-4$} (C);
		\path [->] (D) edge node[right]{$1$} (A);
		\path [->] (E) edge node[above]{$-1$} (D);
		\path [->] (E) edge node[right]{$2$} (B);
		\path [->] (E) edge[out=15, in=345,looseness=8] node[right]{$1$} (E);
	\end{tikzpicture}
	\caption{MPG}
\end{wrapfigure}

\textbf{Example 3:}

A Mean Payoff Game\\
$MPG=(A,\{((v_0,v_0), 2),((v_1,v_0), 4), \ldots\})$.\\
Playing $MPG$ with $\sigma=\{(v_0,v_0)\}$ and\newline
$\tau=\{(v_1,v_0),(v_2,v_2),(v_3,v_0),(v_4,v_4)\}$, the respective optimal strategies, results in the values\\
$v_{\sigma,\tau}\colon v_0\mapsto 2, v_1\mapsto 2, v_2\mapsto -4, v_3\mapsto 2, v_4\mapsto 1$\\\\\\
\subsubsection{Energy Games}
Energy Games are played by two players, \textit{Charging} or player 0 ($\Square$) and \textit{Depleting} or player 1 ($\Circle$).
An Energy Game, $EG = (A,w)$, is played on an Arena $A$ and an edge-weight function $w\colon E\rightarrow\{-W, \ldots, -1, 0, 1, \ldots, W\}$, $d\in \mathbb{N}_0$.\newline
\textit{Charging} aims to chose their strategy for a given play $\pi_{\sigma, \tau}(EG, v_0)=\langle v_0, v_1, \ldots\rangle$ such as to minimize the \textit{initial credit} $c\in \mathbb{N}_0$ needed to maintain the winning condition
\begin{align*}
	\forall k\in\mathbb{N}_0\colon\left(\sum_{i=0}^{k}w((v_i,v_{i+1}))\right)+c\geq0.
\end{align*}
We call the smallest inital credit that still maintains the winning condition for a given play the \textit{minimum initial credit} or \textit{value} $v_{\sigma,\tau}(v)$ of a vertex $v$. If no such value exists, e.g. when the vertex is of \textit{Depleting} and has a self-loop with negative edge-weight, we write $v_{\sigma,\tau}(v)=\infty$. For any given position $\langle v_0, v_1, \ldots, v_k\rangle$ in a play we call $\left(\sum_{i=0}^{k-1}w((v_i,v_{i+1}))\right) + c$ the \textit{energy level} at that position. Keep in mind that \textit{Charging} doesn't necessarily aim to maximise their energy level at \textit{any} specific position but rather to maintain a non-negative energy level at \textit{every} position.

The goal for \textit{Charging} is to avoid getting trapped in cycles with overall negative edge-weight, as these can deplete any initial credit given, as well as to minimize the initial credit necessary to the compliant with the winning condition in all other cycles.
Conversely, the goal for \textit{Depleting} is to trap \textit{Charging} in negative cycles or at least to maximize the initial credit necessary for \textit{Charging} to reach a non-negative cycle.
Optimal strategies are those that minimize the initial credit necessary for \textit{Charging} and those that either deny the winning condition entirely or maximize the initial credit necessary for \textit{Depleting}.

\textbf{Example 4:}

Let us reappropriate Arena $A$ and edge-weight function $w$ of the previous MPG example and create an Energy Game $EG=(A,0,4,\{((v_0,v_0), 2),((v_1,v_0), 4), \ldots\})$.\\
Playing $EG$ with $\sigma=\{(v_0,v_0)\}$ and $\tau=\{(v_1,v_0),(v_2,v_2),(v_3,v_0),(v_4,v_3)\}$, the respective optimal strategies, results in the values $v_{\sigma,\tau}\colon v_0\mapsto 0,v_1\mapsto 0,v_2\mapsto \infty,v_3\mapsto 0,v_4\mapsto 1$.
\subsubsection{Discounted Payoff Games}
Discounted Payoff Games are played by two players, \textit{Max} or player 0 ($\Square$) and \textit{Min} or player 1 ($\Circle$).
A Discounted Payoff Game, $DPG = (A,w,\lambda)$, is played on an Arena $A$, an edge-weight function $w\colon E\rightarrow\{-W, \ldots, -1, 0, 1, \ldots, W\}$, $d\in \mathbb{N}_0$ and a discount factor $0<\lambda<1$.\newline
\textit{Max} aims to chose their strategy for a given play $\pi_{\sigma, \tau}(DPG, v_0)=\langle v_0, v_1, \ldots\rangle$ such as to maximize the \textit{discounted payoff} 
\begin{align*}
	(1-\lambda)\left(\sum_{i=0}^{\infty}\lambda^i\cdot w((v_i,v_{i+1}))\right).
\end{align*}
Again we call this the \textit{value} $v_{\sigma,\tau}(v)$ of a vertex $v$, we will round them from here on out. Optimal strategies are those that maximize/minimize the discounted payoff for a given MPG regardless of the initial vertex.

\textbf{Example 5:}

Let us again reappropriate Arena $A$ and edge-weight function $w$ and create a Discounted Payoff Game $DPG=(A,\{((v_0,v_0), 2),((v_1,v_0), 4), \ldots\}, 0.95)$.\\
Playing $DPG$ with $\sigma=\{(v_0,v_0)\}$ and $\tau=\{(v_1,v_0),(v_2,v_2),(v_3,v_0),(v_4,v_4)\}$, the respective optimal strategies, results in the values $v_{\sigma,\tau}\colon v_0\mapsto 2,v_1\mapsto 2.1,v_2\mapsto -4,v_3\mapsto 1.95,v_4\mapsto 1$.
\subsection{Simple Stochastic Games}
Simple Stochastic Games are played by two players, \textit{Max} or player 0 ($\Square$) and \textit{Min} or player 1 ($\Circle$).
A Simple Stochastic Game, $SSG = (G, (V_{0}, V_{1}, V_{2}), \mathfrak{0}, \mathfrak{1}, p)$, is played on a Directed Graph G, with a partition $(V_{0}, V_{1}, V_{2})$, two sink vertices $\mathfrak{0}, \mathfrak{1}$ and a probability function $p\colon V_2\rightarrow (V\rightarrow [0,1])$. We require that the probabilites of all outgoing edges of each $V_2$ vertex sum to 1: $\forall v\in V_2\colon\sum_{u \in V}p(v)(u)=1$ and that $\forall (u,v)\in(V_2\times V)\setminus E\colon p_u(v)=0$.
On vertices $v\in V_2$, also called \textit{Average} or \textit{Random} ($\Diamond$) vertices, the next vertex gets chosen according to the probability function $p_v\colon V\rightarrow[0,1]$ rather than a player.
We also require, like for Arenas, that the out-degree of every vertex is at least one, with the exception of $\mathfrak{0}, \mathfrak{1}$, for which it is zero.\newline
\textit{Max} wins if the $\mathfrak{1}$ sink is reached, \textit{Min} wins if the $\mathfrak{0}$ sink is reached or the game doesn't terminate.
Since the result of a play $\pi_{\sigma, \tau}(SSG, v_0)$ can be probablistic, it is assigned a probabilty to reach the $\mathfrak{1}$ sink rather than a fixed value. We display this probability as a rounded real number.
We say that a SSG is \textit{stopping} if for every possible position in a play, so given two fixed strategies $\sigma, \tau$, there is a path to one of the sink vertices. All SSGs we will consider will be stopping-SSGs.
Since stopping-SSGs terminate, they are not Infinite Games. For simplicity, we will still include SSGs under the \textit{Infinite Games} label from here on out.
The goal for \textit{Max} in a stopping-SSG is to maximize the chance to reach the $\mathfrak{1}$ vertex, the goal for \textit{Min} to reach the $\mathfrak{0}$ vertex. Optimal strategies are those that maximize the chance to reach the desired sink-vertex of the respective player regardless of the initial vertex.
\begin{wrapfigure}[7]{l}{4.5cm}
	\begin{tikzpicture}
		\node[style={regular polygon,regular polygon sides=4}, draw=black] (A) at (0,0) {$v_0$};
		\node[shape=diamond, draw=black] (B) at (1.75,0) {$v_1$};
		\node[shape=diamond, double, draw=black] (C) at (3.5,0) {$\mathfrak{0}$};
		\node[shape=circle, draw=black] (D) at (0,-1.75) {$v_2$};
		\node[shape=diamond, draw=black] (E) at (1.75,-1.75) {$v_3$};
		\node[shape=diamond, double, draw=black] (F) at (3.5,-1.75) {$\mathfrak{1}$};
		\path [->] (A) edge (B);
		\path [->] (A) edge[out=45, in=135] (C);
		\path [->] (B) edge[color=c1] node[above,color=c1]{$.53$} (C);
		\path [->] (B) edge[color=c2] node[color=c2,xshift=8pt]{$.42$} (E);
		\path [->] (B) edge[color=c3] node[color=c3,xshift=-8pt,yshift=18pt]{$.05$} (F);
		\path [->] (D) edge (A);
		\path [->] (D) edge (B);
		\path [->] (E) edge[color=c4] node[right,color=c4]{$.16$} (A);
		\path [->] (E) edge[color=c5] node[color=c5,xshift=-10pt,yshift=-16pt]{$.3$} (C);
		\path [->] (E) edge[color=c6] node[below,color=c6]{$.55$} (D);
	\end{tikzpicture}
	\caption{SSG}
\end{wrapfigure}

\textbf{Example 6:}

A Simple Stochastic Game
\begin{gather*}
	SSG=(G,(\{v_0\},\{v_2\},\{v_1,v_3\}),\mathfrak{0},\mathfrak{1},\\
	\{(v_1,\{\textcolor{c1}{(\mathfrak{0},.53)},\textcolor{c3}{(\mathfrak{1},.05)},\textcolor{c2}{(v_3,.42)}\}),\\
	(v_3,\{\textcolor{c5}{(\mathfrak{0},.3)},\textcolor{c4}{(v_0,.16)},\textcolor{c6}{(v_2,.55)\})}\}).\\
\end{gather*}\\
Playing $SSG$ with $\sigma=\{(v_0,v_1)\}$ and $\tau=\{(v_2,v_0)\}$, the respective optimal strategies, results in the values $v_{\sigma,\tau}\colon v_0\mapsto .07,v_1\mapsto .07,v_2\mapsto .07,v_3\mapsto .05,\mathfrak{0}\mapsto 0,\mathfrak{1}\mapsto 1$.
\subsection{Reductions}
A \textit{reduction} describes an algorithm with which we can transform (reduce) one class of problems to another. For our purposes, we choose target problem classes that are already solved. Aside from reducing to an already solved problem class and therefore, per definition, also solving the original problem class, reducing and subsequent solving of instances of the original problem may be faster than any attempt to directly solve the original problem without reducing to another intermediary infinite game problem.\newline
Formally, we express our problem classes as formal languages $A$ and $B$ over the alphabets $\Sigma^*$ and $\Gamma^*$. If $f$ is totally computable and
\begin{gather*}
	f\colon \Sigma^*\rightarrow\Gamma^*, A\subseteq \Sigma^*, B\subseteq \Gamma^*\\
	\forall w\in\Sigma^*\colon w\in A \iff f(w)\in B
\end{gather*}
then $f$ is a reduction from $\Sigma^*$ to $\Gamma^*$.\newline
In practice our formal languages $A$ and $B$ will be some instances of types of infinite games together with statements about such games, such as the values under optimal play or optimal strategies. The reduction will then draw an equivalence to the other infinite game, e.g. if $w=``v(v_u)=5$ in $G$'' is a word in $A$ then $f(w) = ``v(v_v)=16$ in $H$'' is a word in $B$.\newpage
\section{Solutions and Reductions in Theory}
Before we go into the specifics of the implementation of the algorithms used to solve the problems and the reductions we first want to comprehensively explain the theory behind each of them.
\subsection{Solutions in Theory}
One of the key parts of our work is the solving of problems on our games, be it in conjunction with a reduction or not.\\
We can split our algorithms to find values into broadly three categories:
\begin{itemize}
	\item Kleene Iteration Based:
	\begin{itemize}
		\item Zwick and Paterson's algorithm for MPGs
		\item Kleene Iteration for DPGs $\dagger$
		\item Kleene Iteration for SSGs $\dagger$
	\end{itemize}
	\item Strategy Iteration Based:
	\begin{itemize}
		\item Strategy Iteration for DPGs $\ddagger$
		\item Strategy Iteration for EGs:
		\begin{itemize}
			\item from above, utilizes Linear programming $\ddagger$
			\item from below, utilizes Kleene Iteration $\ddagger$
		\end{itemize}
		\item Strategy Iteration for SSGs $\ddagger$
	\end{itemize}
	\item Unique
	\begin{itemize}
		\item Zielonka's algorithm for PGs
		\item BCDGR's algorithm for EGs	 $\ddagger$	
	\end{itemize}
\end{itemize}
Algorithms marked with $\dagger$ also solve for optimal strategies in a single pass, those marked $\ddagger$ solve for optimal strategies in a single pass for only one of the two players. Those that don't natively provide optimal strategies can be still be utilized to find them by progressively removing edges from the game and seeing if that affects the values until all but one outgoing edge per vertex have been removed from the game, which constitute edges that can form an optimal strategy.

First we shall explain the generalized idea of Kleene Iteration and Strategy Iteration, as they can be used to solve multiple of the problems posed. Then we will explain the specifics of each implementation of those as well as unique algorithms used to solve each of the games we are interested in.\newpage
\subsubsection{Kleene Iteration}
\subsubsection{Strategy Iteration}
\subsubsection{PGs}
\subsubsection{MPGs}\label{mpg_frac}
\subsubsection{DPGs}
\subsubsection{EGs}
\subsubsection{SSGs}\newpage
\subsection{Reductions in Theory}
The reductions we use here allow us to formulate the problem of finding the value or optimal strategy of one game in a way that we can find them by solving problems in another game.
For $PG\rightarrow MPG$ and $MPG\rightarrow DPG$ the graph structure stays the exact same, for $MPG\rightarrow EG$ the graph is originally the same but while solving we recursively build a binary tree that proceeds on subgraphs of the original graph. For $DPG\rightarrow SSG$ the edges of the graph of the DPG get replaced by a certain construct that maintains a similariy between the two in \textit{some} sense, but the set of vertices and edges grows considerably in doing so.

\subsubsection{PGs to MPGs}
The reduction from PGs to MPGs is based on the idea of \cite{DBLP:journals/ipl/Jurdzinski98}.
For a given $PG = (A,p)$ we  transform into a $MPG = (A,w)$, with the underlying arena $A$ being the same and the weights $w$ of $MPG$ based directly on the priority function $p$ of $PG$. Optimal strategies in $MPG$ translate one to one to $PG$ and the vertex $v$ having value $\geq0$ in $MPG$ translates to it being winning for \textit{Even} in $PG$. Recall that for both $PGs$ and $MPGs$ winning strategies can be memoryless.

Consider any play $\pi_{\sigma,\tau}$ in $A$ with $n=|V|$. Given that $A$ is finite and $\pi_{\sigma,\tau}$ can continue infinitely, any such play, regardless of starting vertex $v_0$ will necessarily have a repeat vertex after a path of length $k\leq n$ and will then, by virtue of the strategies being memoryless, indefinitely continue in a cycle of length $l\leq n-k$.

From the definition of the value of a $PG$ it follows directly that only the priorities of the nodes in the cycle matter for the outcome to the game, specifically the highest priority. For MPGs we have:
\begin{align*}
	&\liminf\limits_{n\rightarrow \infty} \left(\dfrac{1}{n}\sum_{i=0}^{n-1}w((v_i,v_{i+1}))\right)=\\
	&\liminf\limits_{n\rightarrow \infty} \left(\dfrac{1}{n}\sum_{i=k}^{n-1}w((v_i,v_{i+1}))\right)+
	\liminf\limits_{n\rightarrow \infty} \left(\dfrac{1}{n}\sum_{i=0}^{k-1}w((v_i,v_{i+1}))\right)=\\
	&\liminf\limits_{n\rightarrow \infty} \left(\dfrac{1}{n}\sum_{i=0}^{n-1}w((v_{i+k},v_{i+1+k}))\right)=\\
	&\liminf\limits_{n\rightarrow \infty} \left(\dfrac{1}{n}\sum_{i=0}^{n-1}\dfrac{1}{l}\sum_{j=0}^{l-1}w((v_{j+k},v_{j+1+k}))\right)=\\
	&\dfrac{1}{l}\sum_{j=0}^{l-1}w((v_{j+k},v_{j+1+k}))
\end{align*}
So again, the only edge-weights that matter for the value of a play are those that are part of the terminal cycle.

To equate the values of the games we want to set the edge-weights in such a way that the outgoing edge of any vertex of a given cycle alone can set the parity of the value of the game if and only if the priority of the vertex is the highest in the cycle.\\
\cite{DBLP:journals/ipl/Jurdzinski98} does this by setting $w((u,v)) = (-n)^{p(u)}$. This way, the absolute weight of the outgoing edge of the vertex with highest priority is always higher than any possible make-up of the rest of the cycle, i.e. $|(-n)^{p(u)}|>(n-1)|(-n)^{p(u)-1}|$.\\
This, however, is unsatisfying since the complexity of solving MPGs with the algorithms we use is dependent on $d$, the maximum absolute weight in the MPG. We therefore want to be conservative with the edge-weights and try to minimize them.\\
To that end, we set our weights as follows:
\begin{align*}
	w((u,v)) = (-1)^{p(u)}\cdot (sum\langle z\in \{|w((x,y))|\mid (x,y)\in E\}\mid x\in V_u\footnotemark\rangle +1^{p(u)~mod~2})
\end{align*}
\footnotetext{$V_u = \{ v\in V\mid p(x)<p(u)\land (p(x)\neq p(u)~\mathrm{mod}~2)\}$ and therefore $V_u = \emptyset$ for $u = min\{p(v)\mid v\in V\}$}
This way the vertex with highest priority has an outgoing edge which, per definition, dominates the value of the MPG since it has a higher absolute weight than all vertices of lower priority of the opponent, regardless of their counterstrategy $\tau$. Thus any play on PG whose value is \textit{Even/Odd} will have a value of $\geq 0/\leq -1$ when translated to an MPG in such a way and any optimal strategy in \textit{PG} will accordingly be an optimal strategy in \textit{MPG}.
Conversely, any MPG that will be reduced-to in such a way will have an edge in it's terminal cycle whose absolute weight will be greater than the sum of all the edges of the opponent in said cycle. The vertex said greatest edge emanates from being exactly equivalent to the vertex with highest priority in the original PG. Thus any play on such MPG whose value, or equivalently whose greatest edge, is $\geq 0/\leq -1$ will have \textit{Even/Odd} value in the original MPG and any optimal strategy in MPG will be a optimal strategy in PG.
\begin{wrapfigure}[8]{l}{4.5cm}
	\begin{tikzpicture}
		\node[style=circle, draw=black, label={1}] (A) at (0,0) {$v_0$};
		\node[style={regular polygon,regular polygon sides=4}, draw=black,label={5}] (B) at (1.75,0) {$v_1$};
		\node[shape=circle, draw=black,label={4}] (C) at (3.5,0) {$v_2$};
		\node[shape=circle, draw=black,label={below:4}] (D) at (0,-1.75) {$v_3$};
		\node[style=circle, draw=black,label={below:0}] (E) at (1.75,-1.75) {$v_4$};
		\node[style={regular polygon,regular polygon sides=4}, draw=black,label={below:3}] (F) at (3.5,-1.75) {$v_5$};
		\path [->] (A) edge (B);
		\path [->] (B) edge[out=15, in=165] (C);
		\path [->] (C) edge[out=195, in=345] (B);
		\path [->] (B) edge (E);
		\path [->] (F) edge (C);
		\path [->] (E) edge[out=60, in=210] (C);
		\path [->] (C) edge[out=240, in=30] (E);
		\path [->] (D) edge (A);
		\path [->] (D) edge (E);
		\path [->] (E) edge[out=345, in=285, looseness=4] (E);
	\end{tikzpicture}
	\caption{PG}
	\begin{tikzpicture}
		\node[style=circle, draw=black] (A) at (0,0) {$v_0$};
		\node[style={regular polygon,regular polygon sides=4}, draw=black] (B) at (1.75,0) {$v_1$};
		\node[shape=circle, draw=black] (C) at (3.5,0) {$v_2$};
		\node[shape=circle, draw=black] (D) at (0,-1.75) {$v_3$};
		\node[style=circle, draw=black] (E) at (1.75,-1.75) {$v_4$};
		\node[style={regular polygon,regular polygon sides=4}, draw=black] (F) at (3.5,-1.75) {$v_5$};
		\path [->] (A) edge node[above]{-1} (B);
		\path [->] (B) edge[out=25, in=155] node[above]{-5} (C);
		\path [->] (C) edge[out=195, in=345] node[above]{2} (B);
		\path [->] (B) edge node[left]{-5} (E);
		\path [->] (F) edge node[right]{-1} (C);
		\path [->] (E) edge[out=60, in=210] node[left]{0} (C);
		\path [->] (C) edge[out=240, in=30] node[right]{2} (E);
		\path [->] (D) edge node[left]{2} (A);
		\path [->] (D) edge node[below]{2} (E);
		\path [->] (E) edge[out=345, in=285, looseness=4] node[right]{0} (E);
	\end{tikzpicture}
	\caption{MPG}
\end{wrapfigure}\\
\textbf{Example 7:} We translate $PG$ to $MPG$.\\
The edge-weights are successively built as follows:\\\\
\begin{tabular}{c|c|c|c|c}
	$u\in V$ & $p(u)$ & $V_u$ & \footnotemark & $w((u,\_))$\\\hline
	$v_4$ & $0$ & $\emptyset$ & $\emptyset$ & 0 \\\hline
	$v_0$ & $1$ & $\{v_4\}$ & $\langle 0\rangle$ & -1 \\\hline
	$v_5$ & $3$ & $\{v_4\}$ & $\langle 0\rangle$ & -1 \\\hline
	$v_2, v_3$ & $4$ & $\{v_0, v_5\}$ & $\langle 1, 1\rangle$ & 2 \\\hline
	$v_1$ & $5$ & $\{v_2, v_3, v_4\}$ & $\langle 2, 2, 0\rangle$ & -5 \\\hline
\end{tabular}\\\\\\
\footnotetext{$\langle z\in \{|w((x,y))|\mid (x,y)\in E\}\mid x\in V_u\rangle$}
Here we can limit $d$ to $5$ instead of $|(-6)^5|=7776$.\newpage
\subsubsection{MPGs to DPGs}
The reduction from MPGs to DPGs is the one described in \cite{ZWICK1996343}. The underlying Arena $A$ and edge-weights $w$ stay the same.
Recall from 3.2.1 that a play in an Arena eventually results in a repeat vertex after a path of length $k\leq n$ and then indefinitely continues in a cycle of length $l\leq n-k$. The value of such play being:
\begin{align*}
	\dfrac{1}{l}\sum_{j=0}^{l-1}w((v_{j+k},v_{j+1+k}))
\end{align*}
For DPGs we have the value being:
\begin{align}
	v(v_0)=~&(1-\lambda)\left(\sum_{i=0}^{\infty}\lambda^i\cdot w((v_i,v_{i+1}))\right)=\nonumber\\
	&(1-\lambda)\left(\sum_{i=k}^{\infty}\lambda^i\cdot w((v_i,v_{i+1}))\right)+(1-\lambda)\left(\sum_{i=0}^{k-1}\lambda^i\cdot w((v_i,v_{i+1}))\right)\nonumber\\
	&(1-\lambda)\left(\sum_{i=0}^{\infty}\lambda^{i+k}\cdot w((v_{i+k},v_{i+1+k}))\right)+(1-\lambda)\left(\sum_{i=0}^{k-1}\lambda^i\cdot w((v_i,v_{i+1}))\right)\nonumber\\
	&(1-\lambda)\lambda^k\left(\sum_{i=0}^{\infty}\lambda^{il}\cdot \sum_{j=0}^{l-1}\lambda^{j}\cdot w((v_{j+k},v_{j+1+k}))\right)+(1-\lambda)\left(\sum_{i=0}^{k-1}\lambda^i\cdot w((v_i,v_{i+1}))\right)\nonumber\\
	&\frac{(1-\lambda)\lambda^k}{1-\lambda^l}\left(\sum_{j=0}^{l-1}\lambda^{j}\cdot w((v_{j+k},v_{j+1+k}))\right)+(1-\lambda)\left(\sum_{i=0}^{k-1}\lambda^i\cdot w((v_i,v_{i+1}))\right)\label{l_eins}
\end{align}
Now if we let $\lambda\to 1^-$ we get:
\begin{align*}
	&\lim_{\lambda\to 1^-}\left(\frac{(1-\lambda)\lambda^k}{1-\lambda^l}\left(\sum_{j=0}^{l-1}\lambda^{j}\cdot w((v_{j+k},v_{j+1+k}))\right)+(1-\lambda)\left(\sum_{i=0}^{k-1}\lambda^i\cdot w((v_i,v_{i+1}))\right)\right)=\\
	&\lim_{\lambda\to 1^-}\left(\frac{1-\lambda}{1-\lambda^l}\sum_{j=0}^{l-1}w((v_{j+k},v_{j+1+k}))\right)=\\
	&\frac{1}{l}\sum_{j=0}^{l-1}w((v_{j+k},v_{j+1+k}))
\end{align*}
Since we can't actually let $\lambda$ be 1, we have to use a $\lambda$ that is sufficiently large enough as to enable us to truncate to the nearest rational number as was done in 3.1.4.1.
We can take \hyperref[l_eins]{(1)} and rescale all the weights by W, such that all weights are $\geq0$:
\begin{align*}
	&v(v_0)+W=\\
	&\frac{(1-\lambda)\lambda^k}{1-\lambda^l}\left(\sum_{j=0}^{l-1}\lambda^{j}\cdot \left(w((v_{j+k},v_{j+1+k}))+W\right)\right)+(1-\lambda)\left(\sum_{i=0}^{k-1}\lambda^i\cdot \left(w((v_i,v_{i+1}))+W\right)\right)\geq\\
	&\frac{(1-\lambda)\lambda^k}{1-\lambda^l}\left(\sum_{j=0}^{l-1}\lambda^{j}\cdot \left(w((v_{j+k},v_{j+1+k}))+W\right)\right)\geq\\
	&\frac{(1-\lambda)\lambda^{k+l-1}}{1-\lambda^l}\left(\sum_{j=0}^{l-1}\left(w((v_{j+k},v_{j+1+k}))+W\right)\right)\geq\\
	&(1-n(1-\lambda))\left(\sum_{j=0}^{l-1}\left(w((v_{j+k},v_{j+1+k}))+W\right)\right)\geq\\
	&(1-n(1-\lambda))\left(W+\sum_{j=0}^{l-1}w((v_{j+k},v_{j+1+k}))\right)
\end{align*}
Now if we revert the scaling we arrive at:
\begin{align*}
	&v(v_0)+W\geq(1-n(1-\lambda))\left(W+\sum_{j=0}^{l-1}w((v_{j+k},v_{j+1+k}))\right)\Leftrightarrow\\
	&v(v_0)+W\geq W+\sum_{j=0}^{l-1}w((v_{j+k},v_{j+1+k}))-n(1-\lambda)\left(W+\sum_{j=0}^{l-1}w((v_{j+k},v_{j+1+k}))\right)\Leftrightarrow\\
	&v(v_0)\geq\sum_{j=0}^{l-1}w((v_{j+k},v_{j+1+k}))-n(1-\lambda)\left(W+\sum_{j=0}^{l-1}w((v_{j+k},v_{j+1+k}))\right)\geq\\
	&\sum_{j=0}^{l-1}w((v_{j+k},v_{j+1+k}))-n(1-\lambda)\cdot 2W
\end{align*}
Similarly, we can rescale by -W, such that all weights are $\leq0$ and in total we arrive at:
\begin{align*}
	\sum_{j=0}^{l-1}w((v_{j+k},v_{j+1+k}))+n(1-\lambda)\cdot 2W\geq v(v_0)\geq\sum_{j=0}^{l-1}w((v_{j+k},v_{j+1+k}))-n(1-\lambda)\cdot 2W
\end{align*}
From 3.1.4 we know that the minimum distance between two valid values of a MPG is $\frac{1}{n(n-1)}$, therefore we want to set $\lambda$ in such a way as to constrict the size of the just derived interval to:
\begin{align*}
	|2n(1-\lambda)\cdot 2W|\leq\frac{1}{n(n-1)}
\end{align*}
We get this precision with $\lambda = 1-\frac{1}{4n^3W}$.
If we reduce a MPG to a DPG with such $\lambda$, we can deduce the value for a given vertex of the original MPG by taking the corresponding value $v$ in the DPG and rounding to the unique rational number, with denominator at most $n$, in the closed interval $v\pm \frac{1}{2n(n-1)}$.

\subsubsection{MPGs to EGs}\label{rescale}
The reduction from MPGs to EGs is the one described in \cite{Brim2011}.\\
If we play an EG on $A, w$ of a MPG, any vertex for which the value is $v_{\sigma, \tau}(v)\neq\infty$ is equal to a vertex in the MPG whose value is $v_{\sigma, \tau}(v)\geq0$, since the player can force a terminal cycle whose edge-weights sum up to $\geq0$. This effectively splits the vertices of the graph into two regions for which we know that the value is $\geq0$ for one region and $<0$ for the other.
We can use this dichotomy of spliting the MPG into two areas together with the fact that we can rescale the edge-weights of MPGs.
By solving these rescaled games as EGs we can further and further restrict the possible values of any given vertex in the MPG, until only one possible solution remains.
As laid out in \hyperref[mpg_frac]{3.1.4}, the value of the vertices of a MPG are all in the set
\begin{align*}
	& S=\left\{\frac{p}{q}\in\mathbb{Q}\mid q\in\left[1, |V|\right]\subset\mathbb{N}\land p\in\left[-q\cdot W, q\cdot W\right]\subset\mathbb{Z} \right\}.
\end{align*}
In particular, it is finite and after at most $log_2(|V|\cdot W)$ dichotomies, only one rational number, the value of the vertex, remains.\\
The way we rescale the MPG is by taking the interval of the range of possible values  that are left, starting at S, and then split interval in about half by taking next lowest rational number $\frac{p}{q}\in S$ to the middle of the interval. We then rescale the edge-weights of the MPG, $w$, for the next dichotomy by $(w\cdot q)-p$ and solve it as an EG to split the vertices of the MPG into two regions with value $\geq\frac{p}{q}$ and $<\frac{p}{q}$.

To avoid having to solve the resulting EGs for the entire graph for any subdivision of $S$, we can subdivide the graph alongside the possible interval of values. In the MPG, for a given set of strategies $\sigma, \tau$, any move will land on a vertex with the same value as the previous since the play ultimately always lands in the same terminal cycle with the same mean weight. This means that, for a fixed set of strategies, vertices with different values will necessarily be disconnected if one removes the edges that aren't in play under $\sigma, \tau$ and the values of all vertices stay the same if those edges are removed.\\
What this allows us to do is that after every dichotomy wrt. the values, where we split the set of vertices into three sets, one with value $>\nu$, one with value $<\nu$ and one with known value $\nu$, we can restrict subsequent dichotomies to disjoint subsets of the graph in addition to halving the interval of possible values, since we know that vertices with different value, as shown to be the case by the dichotomy, have to be disconnected if unused edges are removed. This means subsequent solving via EGs happens on smaller graphs, making it faster.\newpage
\subsubsection{DPGs to SSGs}
The reduction from DPGs to SSGs is the one described in \cite{ZWICK1996343}.\\
Fundamentally, DPGs have values in $\left[-W,W\right]$ whereas SSGs have values in $\left[0,1\right]$. Thus, we first rescale the game, similar to how we did in \hyperref[rescale]{3.2.3}, to make the possible value range fit. We do so by rescaling the weights, $w$, of the DPG to $w'=\frac{w+W}{2W}$. This maintains their relation to each other, and thus optimal strategies and the resulting values, and allows us to scale them back to their equivalent in the original DPG $w=(w'\cdot 2W)-W$.\\
For SSGs, unlike for DPGs, there is no ``stack of previous passed edge-weights'' to calculate the value off of, so we need to emulate such behaviour.\\
For any given $e=(u,v)\in E$ we can say that, after translation to an SSG, $u$ has probability of $\lambda$ to reach $v$. This means that, whatever value $v$ has, it will be proportionally represented in the value of $u$ if $e$ is played. Since the decision to play $e$ is made by a player, but the probability to reach $v$ from $u$ is exactly that, a probability, and thus necessarily originates from an \textit{Average} vertex we have to insert an additional Average vertex, $u'$, for each edge of the original DPG.\\
The equivalent aspect of the weight of the edge in the DPG, $w(e)$, also needs to be mirrored in the SSG. We can do so by giving the remaining probability $1-\lambda$ the corresponding chance to reach $\mathfrak{0}$ and $\mathfrak{1}$. Using the previously rescaled edge-weights $w'$ we can easily achieve this by setting $p(u')(\mathfrak{0})=(1-\lambda)\cdot (1-w'(e))$ and  $p(u')(\mathfrak{1})=(1-\lambda)\cdot w'(e)$ respectively. Effectively we are replacing every edge with the following construct:
\begin{center}
	\begin{tikzpicture}
		\begin{scope}
			\node[style={regular polygon,regular polygon sides=4}, draw=black,minimum size=34pt] (u) at (0,0) {$u$};
			\node[style={regular polygon,regular polygon sides=4},  draw=black,minimum size=34pt] (v) at (1.75,0) {$v$};
			\path[->] (u) edge node[above] (3) {$w$} (v);
			v			\graphboxthick[boxeins]{(u) (v) (3)}
		\end{scope}
		\begin{scope}[shift={($(boxeins.east)+(1.5,0)$)}]
			\node[style={regular polygon,regular polygon sides=4}, draw=black,minimum size=34pt] (u) at (0,0) {$u$};
			\node[shape=diamond, draw=black,minimum size=34pt] (u') at (1.75,0) {$u'$};
			\node[style={regular polygon,regular polygon sides=4},  draw=black,minimum size=34pt] (v) at (3.5,0) {$v$};
			\node[shape=diamond,  draw=black,minimum size=34pt] (0) at (3.5,1.25) {$\mathfrak{0}$};
			\node[shape=diamond,  draw=black,minimum size=34pt] (1) at (3.5,-1.25) {$\mathfrak{1}$};
			\path[->] (u) edge node[above] (3) {} (u');
			\path[->] (u') edge node[above] (4) {$\lambda$} (v);
			\path[->] (u') edge node[above,xshift=-40pt] (4) {$(1-\lambda)\cdot (1-w'(e))$} (0);
			\path[->] (u') edge node[below,xshift=-30pt] (4) {$(1-\lambda)\cdot w'(e)$} (1);
			\graphboxthick[boxzwei]{(u) (v) (3) (4) (0) (1)}
		\end{scope}
		\path[->] (boxeins.east) edge node[above] {} ($(boxzwei.west)$);
		\node at ($(boxeins.north)+(0,0.3)$) {$DPG$};
		\node at ($(boxzwei.north)+(0,0.3)$) {$SSG$};
	\end{tikzpicture}
\end{center}
Average vertices then have value $v(u')=\lambda\cdot v(v)+(1-\lambda)\cdot w'(e)$ which exactly equal to the value of $u$ in the DPG with rescaled edge-weights given that $e$ is chosen as the outgoing edge. Since all outgoing edges will be replaced by this construct, the value of $u$ is equal in the rescaled DPG and the SSG regardless of the strategies played.
To translate back the optimal strategies generated on SSG one simply has to translate back every $(u,u')$ that is part of a strategy in SSG to it's equivalent $(u,v)$ in the DPG.\newpage
\section{Solutions and Reductions in Practice}
While the theoretical descriptions of the algorithms do a good job of explaining the idea of how the approaches work or ought to work, they leave a lot of room for interpretation about how exactly those should be implemented. We want to expand in detail how we implemented them specifically and what changes we made for the sake of performance.

All the games have, at their core, some graph as a key component. The vertices of that graph belong to some player and the edges may have some value attached to them (MPGs, DPGs, EGs) or not (PGs). In SSGs, some do have an attached probability (those outgoing from \textit{Average} vertices) and the rest don't.

The vertices don't inherently have any ordering or relation, so we think of them as just a numbering through the size of the set of vertices. We will call the generic size of the set $n =|V|$ and use 0-indexing from here on out, i.e. the ``first'' vertex is $v_0$ and the last is $v_{n-1}$.

The most fundamental question is how the represent the graph structure programmatically. There's two obvious choices:\\
Two or more parallel lists of length $|E|$ representing the edges of the game with the first two representing source vertices and target vertices and further lists representing additional values associated with that edge, e.g. edge-weights in MPGs \textit{or}\\
An incidence matrix of size $n\times n$, The values of the matrix being the value that is associated with that edge, with a special value signaling that the edge doesn't exist and a simple boolean signaling doesn't/does exist for PGs and SSGs.\\
The first method has the advantage of using less memory, since we only use memory for edges that actually exist. The second method has the advantage that edges that have the same source or target vertex are memory-aligned, thus easy and fast to work with, as they are the same row or column in the matrix.\\
Given that a lot of operations include the semantic of ``all edges from/to a certain vertex'' and that even bigger graphs are manageable in memory size ($|V|=16384$ with $int32$ means about 1GB) to the point where the runtime of algorithms applied will become a problem before memory size does we opt for the second variant.
The \textit{special value} that signals that the edge the matrix entry represents doesn't exist will generally, depending on context, be the minimum/maximum value the datatype can represent if it's an integer and NAN if it's a float. If we represent matrices here, we will use $\times$ to represent the special values. For operations on those matrices we will pretend here that those values are effectively invisible.\\
For example, the edges of a  graph may be represented by a matrix as follows:

\begin{figure}[H]
	\centering
	\begin{minipage}{.5\textwidth}
		\centering
		\begin{tikzpicture}
			\node[style={regular polygon,regular polygon sides=4}, draw=black] (A) at (0,0) {$v_0$};
			\node[shape=circle, draw=black] (B) at (1.75,0) {$v_1$};
			\node[shape=circle, draw=black] (C) at (3.5,0) {$v_2$};
			\node[shape=circle, draw=black] (D) at (0,-1.75) {$v_3$};
			\node[shape=circle, draw=black] (E) at (1.75,-1.75) {$v_4$};
			\path [->] (A) edge[out=105, in=75,looseness=8] node[above]{$2$} (A);
			\path [->] (B) edge node[above]{$4$} (A);
			\path [->] (C) edge node[above]{$-2$} (B);
			\path [->] (C) edge[out=105, in=75,looseness=8] node[above]{$-4$} (C);
			\path [->] (D) edge node[right]{$1$} (A);
			\path [->] (E) edge node[above]{$-1$} (D);
			\path [->] (E) edge node[right]{$2$} (B);
			\path [->] (E) edge[out=15, in=345,looseness=8] node[right]{$1$} (E);
		\end{tikzpicture}
	\end{minipage}%
	\begin{minipage}{.5\textwidth}
		\centering
		\[
		\begin{bmatrix}
			2 & \times & \times & \times & \times\\
			4 & \times & \times & \times & \times\\
			\times & -2 & -4 & \times & \times\\
			1 & \times & \times & \times & \times\\
			\times & 2 & \times & -1 & 1
		\end{bmatrix}
		\]
	\end{minipage}
\end{figure}
We shall simply call such matrices representing edges $E$. Similarly, the ownership of the vertices would be denoted as:
\[
\begin{bmatrix}
	0 & 1 & 1 & 1 & 1
\end{bmatrix}
\]
And we call such ownership matrices $O$.\\
We will be dissecting a lot of matrices and the notation we will use for that is as follows: Given a matrix A, for which we only want to keep the edges for which the source and target vertex are in $su = [0,1,4]$, we write $A[su,su]$.
\begin{figure}[H]
	\centering
	\begin{minipage}{.33\textwidth}
		\centering
		\[
		A=
		\begin{bmatrix}
			2 & \times & \times & \times & \times\\
			4 & \times & \times & \times & \times\\
			\times & -2 & -4 & \times & \times\\
			1 & \times & \times & \times & \times\\
			\times & 2 & \times & -1 & 1
		\end{bmatrix}
		\]
	\end{minipage}%
	\begin{minipage}{.33\textwidth}
		\centering
		\[
		A[su,su]=
		\begin{bmatrix}
			2 & \times & \times\\
			4 & \times & \times\\
			\times & 2 & 1
		\end{bmatrix}
		\]
	\end{minipage}%
	\begin{minipage}{.33\textwidth}
	\centering
	\[
	A[:,su]=
	\begin{bmatrix}
		2 & \times & \times\\
		4 & \times & \times\\
		\times & -2 & \times\\
		1 & \times & \times\\
		\times & 2  & 1
	\end{bmatrix}
	\]
	\end{minipage}
\end{figure}
We may do similar disections on vectors or on only some of the dimensions of a matrix, where $:$, e.g. $A[:,su]$, means to maintain all the entries, i.e. keep all rows but only the columns in $su$.
\begin{figure}[H]
	\centering
	\begin{minipage}{.5\textwidth}
		\centering
		\[
		B=
		\begin{bmatrix}
			2 & \times & -4 & 5 & 7
		\end{bmatrix}
		\]
	\end{minipage}%
	\begin{minipage}{.5\textwidth}
		\centering
		\[
		B[su]=
		\begin{bmatrix}
			2 & \times & 7
		\end{bmatrix}
		\]
	\end{minipage}%
\end{figure}
\subsection{Solutions in Practice}
\subsubsection{PGs}
Parity games additionally have priorities associated with it's vertices. We shall represent them similarly to how we represent the ownership of the vertices except their range is over $\mathbb{N}_0$ instead of a boolean and called $P$.

\textbf{Zielonka's algorithm:}

\begin{lstlisting}[numbers=left,breaklines=true,breakatwhitespace=true,escapechar=|]
def zielonka(O, E, P):
    if len(O) == 0:
        return []
    else:
        max_prio = max(P)
        player = max_prio % 2
        A = find_attractor(player, O, E, P == max_prio|\footnote{As in the list of indices of P where the value is equal to max\_prio}|)
        z1 = zielonka(O[A|$^{-1}$|]|\footnote{A$^{-1}$ as in the inverse of A with respect to the preexisting indexing of O}|, E[A|$^{-1}$|,A|$^{-1}$|], P[A|$^{-1}$|])
        if not any(z1 == player|$^{-1}$|):
            return [player]*len(O)|\footnote{A vector of length len(O) only containing the value ``player''}|
        else:
            losing = [False]*len(O)
            losing[A|$^{-1}$|[z1 == |$\neg$|player]] = True
            B = find_attractor(|$\neg$|player, O, E, losing)
            z2 = zielonka(owner[B|$^{-1}$|], edges[B|$^{-1}$|], priorities[B|$^{-1}$|])
            ret = [|$\neg$|player]*len(O)
            ret[B|$^{-1}$|[z2 == player]] = player
\end{lstlisting}

\textbf{L.2-3:} The recursion in Zielonka's algorithm eventually hit it's bottom by reaching an empty graph. Since obviously there is no vertex to be won, we return an empty vector.\\
\textbf{L.5-6:} Each iteration starts with finding the attractor set of the highest priority. We determine such region and the associated player.\\
\textbf{L.7:} To find the attractor we need the player for whom the attractor set works and the set of initial vertices that actually posses the priority in question. In return we receive the set of vertices from which the player can and the opponent cannot avoid reaching the set of highest priority effectively making it an \textit{attractor}.\\
\textbf{L.8:} The first recursive call.\\
\textbf{L.9-10:} If the first recursive call returns that the game contains no winning region for the opponent then we can conclude that the current game is entirely won by the current player.\\
\textbf{L.12-14:} If there is a winning region for the opponent in the first recursive call on the game sans the first attractor, that means that the opponent can win in the attractor of such winning region since they can avoid A entirely.\\
\textbf{L.15:} We know that B is a winning region for the opponent. We continue to recursively find winning regions for subgames without B, until we reach an empty game by courtesy of L.2-3.\\
\textbf{L.16-17:} Such winning regions then get forwarded back through the recursion, together with B, as a win or lose of the respective player.

\textbf{The algorithm used to find the attractors:}

\begin{lstlisting}[numbers=left,breaklines=true,breakatwhitespace=true,escapechar=|]
def find_attractor(player, O, E, Nh):
    if len(Nh) == 0:
        return []
    us = owner == player
    them = owner != player
    atr = Nh
    while True:
        old = atr
        us_add = any|\footnote{Which rows contain any \textit{True} value; Effectively: All vertices of \textit{player} that have edges into the attractor; see https://numpy.org/doc/stable/reference/generated/numpy.any.html}|(edges[us,atr], axis=1)
        them_add = any(edges[them, atr|$^{-1}$|], axis=1)|$^{-1}$|
        atr[us[us_add]] = True
        atr[them[them_add]] = True
        if all(atr == old):
            return atr
\end{lstlisting}

\textbf{L.1:} The algorithm is of course dependent on the graph (O, E) but also on the player whose attractor we want to find, since one player want to avoid it, i.e. have edges that \textit{don't} lead into the attractor and the other one wants to reach it. It also needs an initial set of vertices, Nh, which the attractor actually attracts to.\\
\textbf{L.2-3:} Same as with Zielonka's algorithm, an empty set will of course have no attractor.\\
\textbf{L.4-5:} We precompute once the indices of the vertices that belong to the player of the opponent respectively.\\
\textbf{L.6-8:} We iterate over the original set Nh until we hit an iteration that didn't lead to the attractor growing in size.\\
\textbf{L.9:} We gather all the indices of vertices belonging to the player that have edges into the current iteration of the attractor set.\\
\textbf{L.10:} We gather all the indices of vertices belonging to the opponent that have no edges that don't go into the current iteration of the attractor set. Note the double negation.
\textbf{L.11-12:} The vertices we gathered in L.9-10 get added to the attractor set.\\
\textbf{L.13-14:} If L.11-12 didn't lead to a change in the attractor set then we return the current, final, attractor set.

Given any algorithm to find values, we can use it to also find strategies in a costly manner. We show this implementation here exemplary for Zielonka's algorithm.

\textbf{Finding strategies via Zielonka's algorithm:}\label{strat_solve}

\begin{lstlisting}[numbers=left,breaklines=true,breakatwhitespace=true,escapechar=|]
def solve_strat_zielonka(self|\footnote{\textit{self} refers to a game object with the fields $O$, $E$ and $P$, it's ownership, edge and priority vector/matrix}|):
    z = self.solve_value_zielonka()	
    ret = [-1]*len(self.O)
    edges = self.E
    for i,v in enumerate|\footnote{enumerate iterates over an enumerable and provides the index (i) together with the element (v)}|(edges):
        w = v == True
        while True:
            cl = ceil(len(w) / 2)
            one, two = w[:cl], w[cl:]
            e = edges
            e[i] = False
            e[i, one] = True
            x = ParityGame(self.O, e, self.P).solve_value_zielonka()
            if all(x == z):
                if len(one) == 1:
                    ret[i] = one[0]
                    break
                else:
                    w = one
            else:
                w = two
        edges[i] = [False]*len(self.O)
        edges[i, ret[i]] = True
    return ret
\end{lstlisting}

\textbf{L.2:} z are the values of the vertices which we compare to while removing all but one outgoing edge for each vertex.\\
\textbf{L.3:} This holds the strategy, we start with $-1$ i.e. no strategy for each vertex.\\
\textbf{L.4:} We create a copy of the edges from which we progressively delete edge.\\
\textbf{L.5-7:} For each vertex we iteration until we find an edge that is part of an optimal strategy.\\
\textbf{L.8-12:} We part the outgoing edges of our current vertex into two sets and create an edge matrix that only contains the edges of the first set.\\
\textbf{L.13-14:} We solve for the values for our game with restricted edges and check if the values stay the same. If they do, clearly the removed edges are not necessary for an optimal strategy.\\
\textbf{L.15-19:} If the values did stay the same then the edge part of optimal strategies are in the ``one'' half of the edges. If there is only one edge, this is \textit{the} edge for an optimal strategy.\\
\textbf{L.20-21:} If the values change, the edges for optimal strategies must have been ni the ``two'' half.\\
\textbf{L.22-23:} After we choose a successor vertex for a vertex we need to make the corresponding edge mandatory, since optimal strategies aren't necessarily unique but may depend on the decision we made for the current vertex. Otherwise we may choose an edge for subsequent nodes that are part of \textit{some} optimal strategy, but not an optimal strategy that is compatible with the choice we already made here.

\subsubsection{MPGs}
Mean Payoff games are solely descibed by the ownership vector, $O$, and the edge matrix, $E$ that also contains the edge-weights.

\textbf{Zwick and Paterson's algorithm:}

\begin{lstlisting}[numbers=left,breaklines=true,breakatwhitespace=true,escapechar=|]
def solve_value_zwick_paterson(O, E):
    W = max(abs(E))
    k = 4 * (len(O) ** 3) * W
    v = [0]*len(O)
    for _ in range(k):
        edges_weight = edges + v
        v[O] = min(edges_weight, axis=1)
        v[|$\neg$|O] = max(edges_weight, axis=1)
    v = v / k
    return trunc(len(O), v)
\end{lstlisting}

\textbf{L.2-3:} First we determine the iteration depth required by the algorithm.\\
\textbf{L.4-5:} We instantiate with 0 and then iterate as often as the algorithm requires.\\
\textbf{L.6:} For every iteration we generate the k-th iteration of all possible vertices and outgoing edges combinations.\\
\textbf{L.7-8:} For vertices belonging to \textit{Min} we take the vertex-edge combination that leads to the lower value, for \textit{Max} the one that leads to the highest.\\
\textbf{L.9:} Since the tabulation is cumulative we need to rescale once at the end.\\
\textbf{L.10:} Since this approach only gives approximations the algorithm prescribes rounding to the appropriate rational number at the end.

\newpage

\textbf{The algorithm used to truncate to the appropriate rational number:}

\begin{lstlisting}[numbers=left,breaklines=true,breakatwhitespace=true,escapechar=|]
def trunc(d, v):	
    lower = v - (1 / (2 * d * (d - 1)))
    upper = v + (1 / (2 * d * (d - 1)))
    for v_n in range(d):
        for denominator in range(1, d + 1):
            f = floor(lower[v_n] * denominator)
            c = ceil(upper[v_n] * denominator)
            num = np.arange(f, c + 1) / denominator
            if np.any((lower[v_n] < num) & (upper[v_n] > num)):
                v[v_n] = num[((lower[v_n] < num) & (upper[v_n] > num))][0]
                break	
    return v
\end{lstlisting}

\textbf{L.1:} Given a maximum denominator and a vector of values, we truncate to the nearest valid rational number.\\
\textbf{L.2-3:} From the algorithm we know that this is the interval in which the actual rational values have to be.\\
\textbf{L.4-7:} For every entry in the vector and every possible denominator we determine a lower and an upper bound for possible numerators.\\
\textbf{L.8:} We create a range of possible rational numbers that correspond to the rational numbers that possibly lie in the interval.\\
\textbf{L.9-11:} If we find a number that fits into the interval it must be the rational number we are looking for and we proceed with the next number.

\textbf{Finding strategies via Zwick and Paterson's algorithm:} Similar to finding Strategies for PGs under Zielonka's algorithms we can perform the same kind of search here. Since it's perfectly analogous, with the exception of solving for the values of a MPG, we won't repeat it here.\newpage
\subsubsection{EGs}
Similarly to Mean Payoff games, Energy games are solely descibed by the ownership vector and the edge matrix.

\textbf{BCDGR's algorithm:}

\begin{lstlisting}[numbers=left,breaklines=true,breakatwhitespace=true,escapechar=|]
def solve_both_bcdgr(O, E):
    l = [False]*len(O)
    for v in (O == False):
        if all(E[v] < 0):
            l[v] = True
    for v in (O == True):
        if any(E[v] < 0):
            l[v] = True
    f = [0]*len(O)
    cnt = [0]*len(O)
    for v in (O == False):
        for w in E[v]:
            if leq|\footnote{The modified kind of $\preceq$ as laid out in the formal description of the algorithm}|(minus|\footnote{The modified kind of $\ominus$ as laid out in the formal description of the algorithm}|(f[w], E[v,w]), f[v]):
                cnt[v] += 1
    while any(l):
        v = argmax(l)
        l[v] = False
        old = f[v]
        if not O[v]:
            f[v], _ = min*|\footnote{Gives the value and the index of the minimum as a tuple}|([minus(f[w], E[v, w]) for w in E[v]])
        else:
            f[v], _ = max*([minus(f[w], E[v, w]) for w in E[v]])
        if not O[v]:
            cnt[v] = 0
            for w in E[v]:
                if leq(minus(f[w], E[v, w]), f[v]):
                    cnt[v] += 1
        for u in [u for u in E[:, v] if not leq(minus(f[v], E[u, v]), f[u])]:
            if not O[u]:
                if leq(minus(old, E[u, v]), f[u]):
                    cnt[u] -= 1
                if cnt[u] <= 0:
                    l[u] = True
            else:
                l[u] = True
    return_strat = [-1]*len(E)
    for i in range(len(O)):
        if not O[i]:
            _, cand = min*([minus(f[w], E[i, w]) for w in E[i])
            return_strat[i] = E[i,cand]
    return f, return_strat
\end{lstlisting}

\textbf{L.2:} l is the list of vertices that potentially have an outdated progress measure.\\
\textbf{L.3-8:} We initialize l with the vertices that can't have a progress measure of 0.\\
\textbf{L.9-14:} f is the current progress measure and cnt is count as laid out in the formal description fo the algorithm and it's initialization.\\
\textbf{L.15-18:} We go through the list of vertices that we have to update, keeping a copy of f[v]\\
\textbf{L.19-22:} We update the current progress measure of the vertex depending on whose vertex it is with the minimum/maximum progress measure possible, depending on the successor nodes.\\
\textbf{L.23-27:} We update the count of nodes that depend on the current node $v$.\\
\textbf{L.28:} For reach vertex in pre(v) that is now outdated:\\
\textbf{L.29-33:} If vertex if of \textit{Charging} decrement count and add vertex to l\\
\textbf{L.34-35:} If vertex if of \textit{Depleting} add vertex to l\\
\textbf{L.36-40:} Read out optimal strategy for \textit{Charging} based off of the lowest cost successor.
\subsection{Reductions in Practice}
\section{Implementation}\newpage
\section{Evaluation}
Our goal was to compare the different algorithms used to solve problems, especially under reduction to other games, and to see how they compare. To that end we generated random games with multiple parameters and benchmarked all methods we had to solve the respective problems. For any combination of problem, parameters for generation and algorithm used to solve the problem we have $N = 100$ and the times shown repesent the average time of those 100 runs. For their generation, all games have a parameter $n =|V|$ that measures the size of the graph by means of the number of vertices and a parameter $p$ which describes the probability that any possible edge $e\in (V\times V)$ is part of the generated game, meaning the expected out-degree of every vertex is $|V|\cdot p$. Since we require for all games that each vertex has outdegree $\geq 1$, we consequently require that $\frac{1}{|V|} \leq p \leq 1$. We then assign each vertex one random edge and all other potential outgoing edges of that vertex have probability $\frac{(p\cdot |V|)-1}{|V|-1}$ of existing. $p$ can be understood as a measure of how connected a graph is. In fact, towards the extreme ends, for $p=\frac{1}{|V|}$ and sufficiently large $|V|$ one would expect a disconnected graph on one end and for $p=1$ a complete graph on the other end. Each vertex is randomly assigned to a player with equal probability, including the ``Random'' player in SSGs. Games may have additional parameters that are described seperately for each game. The parameters were chosen in a way that they intuitively represent extremes towards each end and then some reasonable middle ground. For all cases, the runtimes are either weakly dependent on $p$ other parameters or they scale monotonously wrt. to those parameters. One can therefore think of intermediate parameterizations as some interpolation between the shown values. For readability we only show some algorithms here. Those that work without reduction to another game and any other that are of interest. The full results are in the appendix.
\subsection{PGs}
For Parity games, the additional parameter is $w$, which describes the range over which the priorities are handed out. Similar to the ownership of the vertices, the priority of vertices is randomly chosen from the possible range with equal chance for each. In theory as well as in this implementation, for the purpose of solving the problems one can rescale the range of the priorities to $\{0, 1, ... |V|\}$ or smaller without affecting the values or optimal strategies of the game. A higher $w$ however decreases the overlap of priorities between vertices on generation, which has a minor impact on the runtime of Zielonka's algorithm.

In theory, Strategy Iteration via DPGs ought to be a contender for large enough $n$ for both the value and stratey problem. The reason for why it isn't depends on the reduction from MPGs to DPGs and is laid out in the subsection for MPGs.

Strategy Iteration from above via EGs can outperform Zielonka's algorithm for large $|V|$ and reasonably dense graphs. However, the break-even point is only reached for such large $|V|$ and consequently such long runtimes that gathering statistically significant runtime data is impractical, it can however be confirmed for individual instances. We therefore cannot describe exact inflection points for the parameterization when the Strategy Iteration starts outperforming Zielonka's algorithm precisely.

Other routes of reduction and applied algorithms fail to be useful, because the edge-weights in the resulting MPG scale very strongly wrt. to the size of the PG and it's range of priorities. Many subsequent algorithms scale in runtime in dependence of the range of the edge-weights of the resulting MPG or equivalent parameter that are downstream from the range of the edge-weights under further reduction (range of edge-weights of EGs, discount factor of DPGs...). Strategy Iteration from above via EGs works decently here precisely because it performs indifferently to the range of edge-weights.

\subsubsection{Value Problem of PGs}
For solving the Value Problem of PGs the result are very clear. For all cases tested, Zielonka's Algorithm is straight up the fastest. It's simplicity demands very little overhead and as such small graphs are solved near instantaneously. It's recursive nature allows it to scale very well to big graphs as well. Less connectivity, as shown by smaller $p$, mean the attractors as laid out in the algorithm tend to not reach as far -- meaning more levels of recursion are needed to cover the entire graph. Similargly, bigger $w$ means that the winning regions that act as a crystallisation point for the attractors tend to start out smaller and thus form smaller attractors.
\begin{figure}[H]
	\centering
	\input{part_graphs/pg_value}
	\caption{Solving the value problem of PGs}
\end{figure}\newpage
\subsubsection{Strategy Problem of PGs}
Unfortunately our naive approach to finding optimal strategies via Zielonka's algorithm is $|V|\cdot log_2(\frac{|E|}{|V|})$ times slower than finding the values alone, meaning the big lead it had in finding the values of games is diminished significantly, bringing it more in line with other algorithms. Depending on the exact $p$ and $w$, the BCDGR algorithm via EGs, which has similarly low overhead to Zielonka's algorithm, but doesn't require repeated invokation to deliver optimal stragies, has better runtimes for reasonably small graphs. However, since it scales worse, such advantage is quickly lost with growing $|V|$. After that, Zielonka's algorithm, even with the malus it suffer for finding optimal strategies, is the fastest available method until Strategy Iteration from above via EGs may take over .
\begin{figure}[H]
	\centering
\input{part_graphs/pg_strat}
\caption{Solving the strategy problem of PGs}
\end{figure}\newpage
\subsection{MPGs}
The additional parameter is $w$, describing the range over which the edge-weights are handed out and for our random generation. They are once again randomly and uniformly distributed over their range.
Unlike with PGs, in this case the meaning of $w$ cannot be scaled down to make subsequently applied algorithms faster. As a result we see some algorithms scale strongly in response to changing $w$ across it's range and even beyond.

\label{mpg_dpg}
In theory, Strategy Iteration via DPGs or SSGs is faster than other algorithms for large enough $n$. In practice however this isn't easily achieved. The method by which MPGs are reduced to DPGs results in discount factors very close to $1$. So close in fact, that for big enough MPGs the difference $1-\lambda$ approaches the limit of the precision standard floating point implementations can represent \cite{754}. As a result, standard Linear Program solvers are unable to solve such Linear Programs that are part of the strategy iteration with adequate precision. One could conceive a Linear Program solver that works with larger floats that enable higher precision. However this only raises the ceiling of what is solveable marginally. It also eventually forces the floats to exceed standard register sizes, meaning the floating point arithmetic fundamental to the Linear Program solver would have to resort to emulating floating point arithmetic for larger floats, which would take more clock cycles per operation and ultimately longer runtimes, undermining the usefulness of the approach in the first place. Effectively this mean that, even if this approach is plausible, it is never pratically viable for MPGs reduced to DPGs.

Kleene Iteration via DPGs or SSGs also fails to be useful because of the discount factor being close to one. The convergence rate of the fundamental fix-point iteration depends directly on the discount factor, with a higher discount factor directly leading to slower convergence.\newpage
\subsubsection{Value Problem of MPGs}
Unlike for PGs the native approach used here, Zwick and Paterson's algorithm isn't a clear winner. Intuitively the $k = 4\cdot |V|^3\cdot d$ iterations the algorithm goes through to guarantee correcteness are way in excess of what's practically needed. This means it's only competetive for graphs with very small $|V|$. As with PGs, solving with BCDGR via EG offers low overhead and thus reasonably fast runtimes for smaller graphs, but again as with PGs the approach scales mediocrely wrt. $|V|$.\\
As with PGs, Strategy Iteration from above via EGs eventually outperforms BCDGR via EG for some parameterizations. \textit{Unlike} with PGs, we don't suffer from an as large $d$ any more, meaning the approach becomes competitive much sooner. The exact inflection point depend on the specific MPG in question, with a relatively large overlap where either may be superior depending on the underlying MPG in question.
\begin{figure}[H]
	\centering
	\input{part_graphs/mpg_value}
	\caption{Solving the value problem of MPGs}
\end{figure}\newpage
\subsubsection{Strategy Problem of MPGs}
As with Zielonka's algorithm on PGs, Zwick and Paterson's algorithm for optimal strategies has an additional cost of $|V|\cdot log_2(\frac{|E|}{|V|})$ over finding the values of the game. Unlike with Zielonka's algorithm on PGs this makes Zwick and Paterson's algorithm non-viable for all conditions for finding strategies. Since BCDGR and Strategy Iteration from above via EG both don't have any malus when finding optimal strategies over values, their relative performance stays the same is was for finding values, with BCDGR being the choice for smaller graphs and Strategy Iteration for larger graphs.
\begin{figure}[H]
	\centering
	\input{part_graphs/mpg_strat}
	\caption{Solving the strategy problem of MPGs}
\end{figure}\newpage
\subsection{EGs}
Once more the additional parameter $w$ describes the range over which the edge-weights are handed out and they are once again randomly and uniformly distributed over their range.

Since all algorithms of interest perform the same for solving for values and optimal strategies, we do not distinguish between the two problems here.

Strategy Iteration from above/below differ internally in whether they use Linear Programming/Kleene Iteration to solve the partial problem within them. Kleene Iteration has very little constant overhead, which allows Strategy Iteration from below to outperform Strategy Iteration from above for smaller graphs. Since it scales significantly worse than the solving of Linear programs, Strategy Iteration from above eventually overtakes for all paramterizations. The exact inflection point varies widely, with the tendency of Strategy Iteration from below to scale worse the less connected the graph is while Strategy Iteration from above scales very indepedant from anything but size.

BCDGR outperforms both for all parameterizations. Especially at the low end, but it maintains a $>3$ fold advantage across all cases tested. However, BCDGR and Strategy Iteration from above seem to scale very similarly asymptotically. If one can improve the runtime of Strategy Iteration from above by just a \textit{constant} factor of $>3$, which is easily conceivable, then it stands to be a contender for fastest method for at least some parameterizations.
\begin{figure}[H]
	\centering
	\input{part_graphs/eg_both}
	\caption{Solving both EG problems}
\end{figure}
From looking at the performance of solving MPGs via EGs one might expect BCDGR to underperform compared to Strategy Iteration from above. However the relative performance of the two is heavily dependent on the way the reduction from MPGs to EGs works. The sub-problems spawned during the reduction from MPGs to EGs do no represent a random sample and are significantly skewed in a way that it disadvantages BCDGR.
\subsection{DPGs}
For DPGs we could have two additional parameters: $\lambda$ which is the discount factor and $w$ which again describes the range of the edge-weights. Since all approaches are indifferent to $w$, we omit it.

As with EGs, all algorithms solve for the value as well as optimal strategies, so we do not distinguish them here.

As already hinted at while solving MPGs by Kleene Iteration via DPGs, Kleene Iteration for DPGs has a convergence rate directly tied to the discount factor while being indifferent towards the connectivity of the graph. Meanwhile Strategy Iteration is weakly dependent on the discount factor while being strongly dependent on the connectivity of the graph, especially towards the lower end.\\
This leads to an interesting situation where instead of one approach eventually outperforming all others for some $|V|$ threshold, it equally depends on the connectivity and the discount factor which approach is favourable.\\
Strategy Iteration and Kleene Iteration via SSGs are viable, but consistently slower by some constant factor.
\footnotetext[5]{$S(x)=\frac{1}{1+e^{-x}}$}
\begin{figure}[H]
	\centering
	\input{part_graphs/dpg_both}
	\caption{Solving both DPG problems}
\end{figure}
\subsection{SSGs}
For evaluating SSGs we run into a problem: Strategy Iteration only works for stopping-SSGs and we have no generalized way to generate those. What we do is we compare the runtimes for DPGs converted to SSGs, which \textit{will} be stopping-SSGs but \textit{will not} be a random sample of stopping-SSGs. Here, $p$, $\lambda$ and $n$ refer to the original DPG before reduction. Again, both approaches solve for both value and optimal strategies.

The result is, unsurprisingly, reminiscent of the results for Strategy Iteration and Kleene Iteration for DPGs. Computationally speaking, SSGs are solved very similarly to DPGs for both Strategy Iteration as well as Kleene Iteration. For both, the sink vertices and the introduction of the \textit{Average} vertices don't have significant implications towards the complexity of the underlying Linear Programs or fix-point iterations. As a result, the runtime behaviour is essentially the same as for DPGs when adjusted for the contortions caused by the reduction from DPGs to SSGs. Particularly the diffraction of the runtime of the Kleene Iteration wrt. $p_{DPG}$ is an artefact of the reduction, where a higher $p_{DPG}$, i.e. a higher $|E_{DPG}|$, leads to a higher $|V_{SSG}|$ in the resulting graph which in turn reduces the convergence rate of the fix-point iteration. Whereas the Linear Program \textit{does} grows fourfold in dimensionality with respect to $|E_{DPG}|$, three of those are equality relationships and only one of them is a half-space, effectively only impacting the runtime complexity of the Linear Program by a constant factor as can be seen by the difference between solving DPGs via Strategy Iteration natively on DPGs versus after reducing to SSGs.
\begin{figure}[H]
	\centering
	\input{part_graphs/ssg_both}
	\caption{Solving both stopping-SSG problems}
\end{figure}
\section{Conclusion and Future}
\subsection{Conclusion}
We set out to see how the different ways to solve problems, the reductions between those problems and the combination of the two play out and compare to one another in a real-world implementation.
To that end we implemented all algorithms and reduction and benchmarked all combinations of those.

As laid out in the Introduction, during the reduction the games tend to grow in some way. $PG\rightarrow MPG$ causes the range of the edge-weights in $MPG$ to grow faster than the range of priorities in $PG$, in $DPG\rightarrow SSG$ both $|V_{SSG}|$ and $|E_{SSG}|$ grow faster than $|V_{SSG}|$ and $|E_{SSG}|$ and in $MPG\rightarrow EG$ the underlying graph stays unchanged, but we need to recursively solve multiple instances of EGs to solve one MPG. Additionally, for the algorithms we tested, $MPG\rightarrow DPG$ resulting in a discount factor very close to one means that the resulting DPG becomes more demanding to solve.\\
This may make it seem like approaches that involve reduction are futile, however this is not always the case.\\
One exception is that despite the disadvantage, a reduction and algorithm may still outperform algorithms that are native to the original game. This is the case for $MPG\rightarrow EG$, where BCDGR via EGs, despite the necessary recursion it outperforms Zwick and Paterson's algorithm on the original MPG. This was in fact a major motivation for the BCDGR algorithm \cite{Brim2011} and we can confirm that this theoretical advantage holds in practice.\\
Another is that the algorithm does get outperformed asymptotically in part due to the reduction, but performs well enough for smaller problems that it's still viable for some range of problems. This happens while solving for optimal strategies via $PG\rightarrow EG$. Here BCDGR via EGs suffers from the large range of edge-weights the reduction $PG\rightarrow MPG$ creates, but isn't affected enough for smaller games as to be immediately non-viable.

The most noteable cases are those involving reduction and Strategy Iteration utilizing Linear Programming (i.e. all but Strategy Iteration from below for EGs).\\
For PGs, the reduction to EGs and solving by Strategy Iteration from above, particularly for optimal strategies, does seem to have some use cases. However these are for relatively large games and Zielonka's algorithm is usefull for a wide range of games, in part by a huge margin.\\
The really interesting case seems to be reducing MPGs to EGs and then solving by Strategy Iteration from above. Beyond a certain size of the game, this method seems to outperform all other approaches for all cases.
In theory the same logic should extend to solving MPGs via Strategy Iteration via DPGs, however as laid out in \hyperref[mpg_dpg]{6.2}, this fails due to the inability to get a satisfying amount of numerical precision.

Keep in mind that for all approaches the exact runtimes depend on the specific implementation in question and can thus change to some degree. The general relations between the runtimes of the approaches would be expected to stay broadly the same, but details, particularly the inflection points of when exactly one approach is expected to start to outperform another are subject to the specific implementation and can change in response to a change in implementation.\\
The exception would be BCDGR versus Strategy Iteration from above on EGs, which display similar asymptotic behaviour with seemingly only some constant difference in runtime. If Strategy Iteration from above could be sped up significantly, if could potentially not just change the inflection point but the whole relation of the two where Strategy Iteration from above is strictly faster for some or all EGs above a certain size.

\subsection{Future}
If the objective is the absolute speed of solving any of the problems, the way to go would be an implementation dedicated to that.
Here, we focused on relative speed rather than absolute speed.
A first step would be an implementation in a compiled language, rather than Python. Explorative tests showed a speed-up of about 30 times in C++ for Kleene Iterations and Strategy Iteration with Linear Programming.\\
Another improvement that can be made use of is the continual development and improvement of Linear Programm solvers. For all but the smallest graph, the Strategy Iteration algorithms that utilize Linear Programming do so in a way that Linear Programming represents most of the actual workload. If significant advances are made here, these would be expected to translate very well into the runtime behaviour of Linear Program based Strategy Iteration.\\
Some of the approaches such as all those utilizing Kleene Iteration and Zwick and Paterson's algorithm for MPGs make heavy use of matrix and vector operations in a way that they could greatly benefit from hardware acceleration such as on a GPU.

Ultimately the most significant improvements are probably possible by way of new algorithms altogether. For some problems there are already algorithms that are theoretically better, like \cite{lmcs:8953} for PGs, but a common theme is that those do not actually perform better or even worse in practise than previous approaches.\\
Ideally we'd want a polynomial-time algorithm for any of our problems, but so far those have eluded us \cite{10.1007/978-3-319-41540-6_15}.