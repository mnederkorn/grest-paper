-\section{Introduction}
There are practical applications (cite) which are able to be modeled by or reduced to various games. Considering the many ways there are to solve those games, the possibility to (repeatedly) reduce them to one another and various ways to implement any of those algorithms there are many potential means to solve problems on those games. We want to see how the different ways to solve problems, the reductions between those problems and the combination of the two play out and compare to one another in a real-world implementation.
\subsection{Task}
The games we concern ourselves here are \textit{Parity Games} (PG), \textit{Mean Payoff Games} (MPG), \textit{Energy Games} (EG), \textit{Discounted Payoff Games} (DPG) and (stopping) \textit{Simple Stochastic Games} (SSG). For any of those games there are multiple problems, but the ones we concern outselves here are ``what is the value of vertex $v$ under optimal strategies from both players?'' and ``what are the optimal strategies?''. For both of those problems, there may already be multiple algorithms to solve them for any specific game directly, some of which are generalizable to multiple of the games to some degree, such as Kleene Iteration and Strategy Iteration. Additionally we can reduce the games to another as follows:
\begin{center}
	\begin{tikzpicture}
		\node (A) at (0,0) {$PG$};
		\node[right=of A] (B) {$MPG$};
		\node[right=of B] (C) {$DPG$};
		\node[right=of C] (D) {$SSG$};
		\node[below=of B] (E) {$EG$};
		\path [->] (A) edge (B);
		\path [->] (B) edge (C);
		\path [->] (C) edge (D);
		\path [->] (B) edge (E);
	\end{tikzpicture}
\end{center}
The reductions themselves are computationally relatively simple compared to the problems themselves. Some of them, however, produce graphs that are bigger in some way ($PG\rightarrow MPG$, $DPG\rightarrow SSG$), which increases the complexity of subsequently applied algorithms, or recursively reduces to multiple sub-problems ($MPG\rightarrow EG$).
Another part we have control over is how exactly the algorithms are implemented. The biggest factor here being that, due to their nature, the edge-weights can be seen as an incidence matrix and can therefore make use of matrix operations, especially for simple operations such as addition and multiplication.
\subsection{Structure}
The paper is structured as follows:

In Chapter 2 we will first define the different kinds of games we are concerned with and the notion of $value$ as on objective for the respective games as well as (memory-less) strategies as a means to achieve those values.\\
In Chapter 3 we will first explain in 3.1 the idea of value and strategy iteration and then both concrete implementations of those as well as other algorithms for the respective games. In 3.2 we will then show how the different games and their underlying graphs can be translated to one another and how the problems posed on those games are finally reduced.\\
In Chapter 4 we show the specifics as to how the theoretical algorithms were internally implemented and elaborate on the choices and decisions arising in the implementations of the reductions and solutions.\\
In Chapter 5 we show how the implementations embeds to be used to solve specific problems or to demonstrate with the GUI.\\
In Chaper 6 we show the kinds of graphs and problems we used to evaluate the solutions and the potential benefit of prior reduction as well as the results of the evaluation.\\
In Chapter 7 we interpret the results of the evaluation and give suggestions and ideas for further improvement.
\section{Preliminaries}
Foundational to our task are the different kinds of \textit{Infinite Games} and how to determine the outcome of each such game. To do so, we also want a notion of \textit{strategies} on such Infinite Games. To later reduce the games to one another, we furthermore want a definition of a \textit{reduction} in the computational complexity sense.
\subsection{Directed Graphs}
Let $V$ be a finite set of Vertices, let $E\subseteq V \times V$ be a set of Edges, then $G = (V,E)$ is a \textit{Directed Graph}.
%We also define $src\colon E\rightarrow V$ as $src((u,v))=u$ and $tgt\colon E\rightarrow V$ as $tgt((u,v))=v$.
We also define
\begin{align*}
	pre\colon V\rightarrow \mathcal{P}(V) && pre(v)=\{u\in V\mid (u,v)\in E\}\\
	post\colon V\rightarrow \mathcal{P}(V) && post(v)=\{u\in V\mid (v,u)\in E\}
\end{align*}
as the set of vertices \textit{from} which a vertex $v$ is reachable (\textit{pre}) and the set of vertices that \textit{to} which a vertex $v$ can reach (\textit{post}).
\subsection{Arenas}
An arena is an extension a Directed Graph, where the set of Vertices, $V$, is partitioned into two disjunct subsets $V_0$ and $V_1$, respectively denoting the regions where Player 0, also represented by $\Square$, and Player 1, also represented by $\Circle$, are to play. We also require that the out-degree of every vertex is at least one, so that a play on an Arena never stops.\newline
Formally, let $(V,E)$ be a non-trivial Directed Graph, $V_0\cup V_1 = V,~V_0\cap V_1 = \emptyset$ be a partition of V and for all $v\in V\colon post(v)\ne \emptyset$,
then $A=(V,(V_0,V_1),E)$ is an Arena.

\begin{wrapfigure}[5]{l}{4.5cm}
	\begin{tikzpicture}
		\node[style={regular polygon,regular polygon sides=4}, draw=black] (A) at (0,0) {$v_0$};
		\node[style={regular polygon,regular polygon sides=4}, draw=black] (B) at (1.75,0) {$v_1$};
		\node[shape=circle, draw=black] (C) at (3.5,0) {$v_2$};
		\node[shape=circle, draw=black] (D) at (0,-1.75) {$v_3$};
		\node[shape=circle, draw=black] (E) at (3.5,-1.75) {$v_4$};
		\path [->] (A) edge (B);
		\path [->] (B) edge[out=15, in=165] (C);
		\path [->] (C) edge[out=195, in=345] (B);
		\path [->] (C) edge (E);
		\path [->] (E) edge[out=285, in=255,looseness=8] (E);
		\path [->] (A) edge (D);
		\path [->] (B) edge (D);
		\path [->] (D) edge[out=285, in=255,looseness=8] (D);
	\end{tikzpicture}
\caption{Arena A}
\end{wrapfigure}

\textbf{Example 1:}

An Arena
\begin{gather*}
	A=(\{v_0,v_1,v_2,v_3,v_4\},(\{v_0,v_1\},\{v_2,v_3,v_4\}),\\
	\{(v_0,v_1),(v_0,v_3),(v_1,v_2),(v_2,v_1),\\
	(v_2,v_4),(v_4,v_4),(v_1,v_3),(v_3,v_3)\})
\end{gather*}
\subsection{Positions, Moves}
Starting at $v_0\in V$, a \textit{position} $\pi_i=(v_0, v_1, \ldots, v_i)$ in a Game describes a finite path on the underlying graph, for which $\forall n\in\left\{0, ..., i-1\right\}\colon (v_n,v_{n+1})\in E$. E.g. in Fig.~1 starting at $v_0$, $(v_0,v_1,v_2,v_4)$ could be a play.\\
A \textit{move} is an extension of a position in the graph by one more step. E.g. in Fig.~1 $(v_0,v_1,v_2)\mapsto (v_0,v_1,v_2,v_4)$ could be a move. Player i chooses the n-th move $(\ldots, v_{n-1})\mapsto (\ldots, v_{n-1}, v_n)$ for $v_{n-1}\in V_i\in (V_0,V_1)$.
\subsection{Strategies}
A \textit{strategy} $V^*\times V_i\rightarrow V$, with $V^*\in \mathcal{P}(V)$ and $V^*\times V_i$ being a valid \textit{position}, is a function by which Player $i$ assigns the next move for any given position.\newline
We call a strategy \textit{memoryless}, if for any given position the next move only depends on the last vertex of the position, i.e. it is a function: $V_i\rightarrow V$.
We will refer to memoryless strategies of Player 0 as $\sigma$ and of Player 1 as $\tau$.
E.g. in Fig.~1 $\sigma = \{(v_0,v_1),(v_1,v_2)\}$ and $\tau = \{(v_2,v_1),(v_3,v_3),(v_4,v_4)\}$ could be strategies.\\
We call a memoryless strategy \textit{optimal}, if it optimizes the value of the game that the opponent can force with their strategy. Notice that there may be multiple optimal strategies for any given game.
\subsection{Plays}
A \textit{play} describes the path of arbitrary length $\langle v_0, v_1, \ldots\rangle$ the player go trough in the process of playing the game.
We refer to the play generated by applying strategies $\sigma, \tau$ to game $G$ starting at $v_0 \in V$ as $\pi_{\sigma, \tau}(G, v_0)=\langle v_0, v_1, \ldots\rangle$\newline
E.g. if we take the previous example's strategies and apply them to $\pi_{\sigma, \tau}(A,v_0)$ we get the play $\langle v_0, v_1, v_2, v_1, \ldots\rangle$. The play can be arbitrarily prolonged by applying moves from $\sigma, \tau$.
\subsection{Infinite Games}
Infinite Games are a category of games played by two players on a finite, directed graph. They are infinite in the sense that we require the out-degree of every vertex to be at least one and as such, regardless of the strategies chosen by the players, they never terminate.\newpage
\subsubsection{Parity Games}
Parity Games are played by two players, \textit{Even} or Player 0 ($\Square$) and \textit{Odd} or Player 1 ($\Circle$).
A Parity Game, $PG =(A,p)$, is played on an Arena $A$ with a priority function $p\colon V\rightarrow\mathbb{N}_0$.\newline
Let $\pi_{\sigma, \tau}(PG, v_0)=\langle v_0, v_1, \ldots\rangle$ be the \textit{play} resulting from applying the strategies $\sigma$ and $\tau$ to Parity Game $PG$. Let
\begin{gather*}
	\#_\infty(\pi_{\sigma, \tau}(PG, v_0))=\\
	\{i\in \{0, 1, \ldots, \left|V\right|\}\mid\forall j \in \mathbb{N}_0\colon \exists n\in \mathbb{N}_0\colon j<\left|\langle v\in \langle v_0, \ldots, v_n\rangle\colon p(v)=i \rangle\right|\}
\end{gather*}
be the set of priorities that appear arbitrarily often in the play.\\
If $max(\#_\infty(\pi_{\sigma, \tau}(PG, v_0)))\coloneqq v_{\sigma,\tau}(v_0)$, the value of vertex $v_0$ is even, then \textit{Even} wins and vice versa. Optimal strategies for \textit{Even}/\textit{Odd} are those that result in the most starting vertices resulting in an even/odd value.

\begin{wrapfigure}[9]{l}{5.5cm}
	\begin{tikzpicture}
		\node[style={regular polygon,regular polygon sides=4}, draw=black,label={5}] (A) at (0,0) {$v_0$};
		\node[style={regular polygon,regular polygon sides=4}, draw=black,label={4}] (C) at (3.5,0) {$v_2$};
		\node[style={regular polygon,regular polygon sides=4}, draw=black,label={below:2}] (D) at (0,-1.75) {$v_3$};
		\node[style={regular polygon,regular polygon sides=4}, draw=black,label={below:3}] (F) at (3.5,-1.75) {$v_5$};
		\node[shape=circle, draw=black,label={2}] (B) at (1.75,0) {$v_1$};
		\node[shape=circle, draw=black,label={below:0}] (E) at (1.75,-1.75) {$v_4$};
		\path [->] (A) edge[out=15, in=165] (B);
		\path [->] (B) edge[out=195, in=345] (A);
		\path [->] (B) edge[out=15, in=165] (C);
		\path [->] (C) edge[out=195, in=345] (B);
		\path [->] (D) edge (A);
		\path [->] (E) edge (B);
		\path [->] (C) edge[out=285, in=75] (F);
		\path [->] (F) edge[out=105, in=255] (C);
		\path [->] (D) edge[out=15, in=165] (E);
		\path [->] (E) edge[out=195, in=345] (D);
		\path [->] (F) edge (E);
	\end{tikzpicture}
	\caption{PG}
\end{wrapfigure}

\textbf{Example 2:}

Playing $PG$ with $\sigma=\{(v_0,v_1),(v_2,v_5),(v_3,v_4),(v_5,v_2)\}$ and $\tau=\{(v_1,v_0),(v_4,v_1)\}$, the respective optimal strategies, results in play $\pi_{\sigma, \tau}(PG, v_0)=\langle v_0, v_1, v_0, ..\rangle$ for which $v_{\sigma,\tau}(v_0)=5$ is odd and the play is therefore winning for \textit{Odd}.
\subsubsection{Mean Payoff Games}
Mean Payoff Games are played by two players, \textit{Max} or Player 0 ($\Square$) and \textit{Min} or Player 1 ($\Circle$).
A Mean Payoff Game, $MPG = (A,w)$, is played on an Arena $A$ and an edge-weight function $w\colon E\rightarrow\{-W, \ldots, -1, 0, 1, \ldots, W\}$, $W\in \mathbb{N}_0$.\newline
\textit{Max} aims to chose their strategy for a given play $\pi_{\sigma, \tau}(MPG, v_0)=\langle v_0, v_1, \ldots\rangle$ such as to maximize the \textit{mean payoff} 
\begin{align*}
	\liminf\limits_{n\rightarrow \infty} \left(\dfrac{1}{n}\sum_{i=0}^{n-1}w((v_i,v_{i+1}))\right).
\end{align*}
We call this the \textit{value} $v_{\sigma,\tau}(v)$ of a vertex $v$. Given that those values are real numbers, we will round them from here on out. Optimal strategies are those that maximize/minimize the mean payoff for a given MPG regardless of the initial vertex.

\begin{wrapfigure}[9]{l}{4.5cm}
	\begin{tikzpicture}
		\node[style={regular polygon,regular polygon sides=4}, draw=black] (A) at (0,0) {$v_0$};
		\node[shape=circle, draw=black] (B) at (1.75,0) {$v_1$};
		\node[shape=circle, draw=black] (C) at (3.5,0) {$v_2$};
		\node[shape=circle, draw=black] (D) at (0,-1.75) {$v_3$};
		\node[shape=circle, draw=black] (E) at (1.75,-1.75) {$v_4$};
		\path [->] (A) edge[out=105, in=75,looseness=8] node[above]{$2$} (A);
		\path [->] (B) edge node[above]{$4$} (A);
		\path [->] (C) edge node[above]{$-2$} (B);
		\path [->] (C) edge[out=105, in=75,looseness=8] node[above]{$-4$} (C);
		\path [->] (D) edge node[right]{$1$} (A);
		\path [->] (E) edge node[above]{$-1$} (D);
		\path [->] (E) edge node[right]{$2$} (B);
		\path [->] (E) edge[out=15, in=345,looseness=8] node[right]{$1$} (E);
	\end{tikzpicture}
	\caption{MPG}
\end{wrapfigure}

\textbf{Example 3:}

A Mean Payoff Game\\
$MPG=(A,\{((v_0,v_0), 2),((v_1,v_0), 4), \ldots\})$.\\
Playing $MPG$ with $\sigma=\{(v_0,v_0)\}$ and\newline
$\tau=\{(v_1,v_0),(v_2,v_2),(v_3,v_0),(v_4,v_4)\}$, the respective optimal strategies, results in the values\\
$v_{\sigma,\tau}\colon v_0\mapsto 2, v_1\mapsto 2, v_2\mapsto -4, v_3\mapsto 2, v_4\mapsto 1$\\\\\\
\subsubsection{Energy Games}
Energy Games are played by two players, \textit{Charging} or Player 0 ($\Square$) and \textit{Depleting} or Player 1 ($\Circle$).
An Energy Game, $EG = (A,w)$, is played on an Arena $A$ and an edge-weight function $w\colon E\rightarrow\{-W, \ldots, -1, 0, 1, \ldots, W\}$, $d\in \mathbb{N}_0$.\newline
\textit{Charging} aims to chose their strategy for a given play $\pi_{\sigma, \tau}(EG, v_0)=\langle v_0, v_1, \ldots\rangle$ such as to minimize the \textit{initial credit} $c\in \mathbb{N}_0$ needed to maintain the winning condition
\begin{align*}
	\forall k\in\mathbb{N}_0\colon\left(\sum_{i=0}^{k}w((v_i,v_{i+1}))\right)+c\geq0.
\end{align*}
We call the smallest inital credit that still maintains the winning condition for a given play the \textit{minimum initial credit} or \textit{value} $v_{\sigma,\tau}(v)$ of a vertex $v$. If no such value exists, e.g. when the vertex is of \textit{Depleting} and has a reflexive edge with negative weight, we write $v_{\sigma,\tau}(v)=\infty$. For any given position $\langle v_0, v_1, \ldots, v_k\rangle$ in a play we call $\left(\sum_{i=0}^{k-1}w((v_i,v_{i+1}))\right) + c$ the \textit{energy level} at that position. Keep in mind that \textit{Charging} doesn't necessarily aim to maximise their energy level at \textit{any} specific position but rather to maintain a non-negative energy level at \textit{every} position.

The goal for \textit{Charging} is to avoid getting trapped in cycles with overall negative edge-weight, as these can deplete any initial credit given, as well as to minimize the initial credit necessary to the compliant with the winning condition in all other cycles.
Conversely, the goal for \textit{Depleting} is to trap \textit{Charging} in negative cycles or at least to maximize the initial credit necessary for \textit{Charging} to reach a non-negative cycle.
Optimal strategies are those that minimize the initial credit necessary for \textit{Charging} and those that either deny the winning condition entirely or maximize the initial credit necessary for \textit{Depleting}.

\textbf{Example 4:}

We reuse $A$ and $w$ of Example 3 and create an Energy Game\\
$EG=(A,0,4,\{((v_0,v_0), 2),((v_1,v_0), 4), \ldots\})$.
Playing $EG$ with $\sigma=\{(v_0,v_0)\}$ and $\tau=\{(v_1,v_0),(v_2,v_2),(v_3,v_0),(v_4,v_3)\}$, respective optimal strategies, results in the values $v_{\sigma,\tau}\colon v_0\mapsto 0,v_1\mapsto 0,v_2\mapsto \infty,v_3\mapsto 0,v_4\mapsto 1$.
\subsubsection{Discounted Payoff Games}
Discounted Payoff Games are played by two players, \textit{Max} or Player 0 ($\Square$) and \textit{Min} or Player 1 ($\Circle$).
A Discounted Payoff Game, $DPG = (A,w,\lambda)$, is played on an Arena $A$, an edge-weight function $w\colon E\rightarrow\{-W, \ldots, -1, 0, 1, \ldots, W\}$, $d\in \mathbb{N}_0$ and a discount factor $0<\lambda<1$.\newline
\textit{Max} aims to chose their strategy for a given play $\pi_{\sigma, \tau}(DPG, v_0)=\langle v_0, v_1, \ldots\rangle$ such as to maximize the \textit{discounted payoff} 
\begin{align*}
	(1-\lambda)\left(\sum_{i=0}^{\infty}\lambda^i\cdot w((v_i,v_{i+1}))\right).
\end{align*}
Again we call this the \textit{value} $v_{\sigma,\tau}(v)$ of a vertex $v$, we will round them from here on out. Optimal strategies are those that maximize/minimize the discounted payoff for a given MPG regardless of the initial vertex.

\textbf{Example 5:}

Let us again reappropriate Arena $A$ and edge-weight function $w$ and create a Discounted Payoff Game $DPG=(A,\{((v_0,v_0), 2),((v_1,v_0), 4), \ldots\}, 0.95)$.\\
Playing $DPG$ with $\sigma=\{(v_0,v_0)\}$ and $\tau=\{(v_1,v_0),(v_2,v_2),(v_3,v_0),(v_4,v_4)\}$, the respective optimal strategies, results in the values $v_{\sigma,\tau}\colon v_0\mapsto 2,v_1\mapsto 2.1,v_2\mapsto -4,v_3\mapsto 1.95,v_4\mapsto 1$.
\subsubsection{Simple Stochastic Games}
Simple Stochastic Games are played by two players, \textit{Max} or Player 0 ($\Square$) and \textit{Min} or Player 1 ($\Circle$).
A Simple Stochastic Game, $SSG = (G, (V_{0}, V_{1}, V_{2}), \mathfrak{0}, \mathfrak{1}, p)$, is played on a Directed Graph G, with a partition $(V_{0}, V_{1}, V_{2})$, two sink vertices $\mathfrak{0}, \mathfrak{1}$ and a probability function $p\colon V_2\rightarrow (V\rightarrow [0,1])$. We require that the probabilites of all outgoing edges of each $V_2$ vertex sum to 1: $\forall v\in V_2\colon\sum_{u \in V}p(v)(u)=1$ and that $\forall (u,v)\in(V_2\times V)\setminus E\colon p_u(v)=0$.
On vertices $v\in V_2$, called \textit{Random} ($\Diamond$) vertices, the next vertex gets chosen according to the probability function $p_v\colon V\rightarrow[0,1]$ rather than a player.
We also require, like for Arenas, that the out-degree of every vertex is at least one. For $\mathfrak{0}, \mathfrak{1}$ it is exactly one with each only having a reflexive edge.\newline
\textit{Max} wins if the $\mathfrak{1}$ sink is reached, \textit{Min} wins if the $\mathfrak{0}$ sink is reached or the game doesn't reach a sink vertex.
Since the result of a play $\pi_{\sigma, \tau}(SSG, v_0)$ can be probablistic, it is assigned a probabilty to reach the $\mathfrak{1}$ sink rather than a fixed value. We display this probability as a rounded real number.
We say that a SSG is \textit{stopping} if for every possible combination of strategies $\sigma, \tau$, there still is a path to one of the sink vertices. All SSGs we will consider from here on out will be stopping-SSGs.\\
The objective for \textit{Max} in a stopping-SSG is to maximize the chance to reach the $\mathfrak{1}$ vertex and for \textit{Min} to reach the $\mathfrak{0}$ vertex. Optimal strategies are those that maximize the chance to reach the desired sink-vertex of the respective player.\\
\begin{wrapfigure}[7]{l}{5cm}
	\begin{tikzpicture}
		\node[style={regular polygon,regular polygon sides=4}, draw=black] (A) at (0,0) {$v_0$};
		\node[shape=diamond, draw=black] (B) at (1.75,0) {$v_1$};
		\node[shape=diamond, double, draw=black] (C) at (3.5,0) {$\mathfrak{0}$};
		\node[shape=circle, draw=black] (D) at (0,-1.75) {$v_2$};
		\node[shape=diamond, draw=black] (E) at (1.75,-1.75) {$v_3$};
		\node[shape=diamond, double, draw=black] (F) at (3.5,-1.75) {$\mathfrak{1}$};
		\path [->] (A) edge (B);
		\path [->] (A) edge[out=45, in=135] (C);
		\path [->] (B) edge[color=c1] node[above,color=c1]{$.53$} (C);
		\path [->] (B) edge[color=c2] node[color=c2,xshift=8pt]{$.42$} (E);
		\path [->] (B) edge[color=c3] node[color=c3,xshift=-8pt,yshift=18pt]{$.05$} (F);
		\path [->] (D) edge (A);
		\path [->] (D) edge (B);
		\path [->] (E) edge[color=c4] node[right,color=c4]{$.16$} (A);
		\path [->] (E) edge[color=c5] node[color=c5,xshift=-10pt,yshift=-16pt]{$.3$} (C);
		\path [->] (E) edge[color=c6] node[below,color=c6]{$.55$} (D);
		\path [->] (C) edge[out=30, in=330, looseness=6] (C);
		\path [->] (F) edge[out=30, in=330, looseness=6] (F);
	\end{tikzpicture}
	\caption{SSG}
\end{wrapfigure}

\textbf{Example 6:}

A Simple Stochastic Game
\begin{gather*}
	SSG=(G,(\{v_0\},\{v_2\},\{v_1,v_3\}),\mathfrak{0},\mathfrak{1},\\
	\{(v_1,\{\textcolor{c1}{(\mathfrak{0},.53)},\textcolor{c3}{(\mathfrak{1},.05)},\textcolor{c2}{(v_3,.42)}\}),\\
	(v_3,\{\textcolor{c5}{(\mathfrak{0},.3)},\textcolor{c4}{(v_0,.16)},\textcolor{c6}{(v_2,.55)\})}\}).\\
\end{gather*}\\
Playing $SSG$ with $\sigma=\{(v_0,v_1)\}$ and $\tau=\{(v_2,v_0)\}$, the respective optimal strategies, results in the values $v_{\sigma,\tau}\colon v_0\mapsto .07,v_1\mapsto .07,v_2\mapsto .07,v_3\mapsto .05,\mathfrak{0}\mapsto 0,\mathfrak{1}\mapsto 1$.
\subsection{Reductions}
A \textit{reduction} describes an algorithm with which we can transform (reduce) one class of problems to another. For our purposes, we choose target problem classes that are already solved. Aside from reducing to an already solved problem class and therefore, per definition, also solving the original problem class, reducing and subsequent solving of instances of the original problem may be faster than any attempt to directly solve the original problem without reducing to another intermediary infinite game problem.\newline
Formally, we express our problem classes as formal languages $A$ and $B$ over the alphabets $\Sigma^*$ and $\Gamma^*$. If $f$ is totally computable and
\begin{gather*}
	f\colon \Sigma^*\rightarrow\Gamma^*, A\subseteq \Sigma^*, B\subseteq \Gamma^*\\
	\forall w\in\Sigma^*\colon w\in A \iff f(w)\in B
\end{gather*}
then $f$ is a reduction from $\Sigma^*$ to $\Gamma^*$.\newline
In practice our formal languages $A$ and $B$ will be some instances of types of infinite games together with statements about such games, such as the values under optimal play or optimal strategies. The reduction will then draw an equivalence to the other infinite game, e.g. if $w=``v(v_u)=5$ in $G$'' is a word in $A$ then $f(w) = ``v(v_v)=16$ in $H$'' is a word in $B$.\newpage
\section{Solutions and Reductions in Theory}
Before we go into the specifics of the implementation of the algorithms used to solve the problems and the reductions we first want to comprehensively explain the theory behind each of them.
\subsection{Solutions in Theory}
One of the key parts of our work is the solving of problems on our games, be it in conjunction with a reduction or not.\\
We can split our algorithms to find values into broadly three categories:
\begin{itemize}
	\item Kleene Iteration Based:
	\begin{itemize}
		\item Kleene Iteration for DPGs $\dagger$
		\item Kleene Iteration for SSGs $\dagger$
	\end{itemize}
	\item Strategy Iteration Based:
	\begin{itemize}
		\item Strategy Iteration for DPGs $\ddagger$
		\item Strategy Iteration for EGs:
		\begin{itemize}
			\item from above, utilizes Linear programming $\ddagger$
			\item from below, utilizes Kleene Iteration $\ddagger$
		\end{itemize}
		\item Strategy Iteration for SSGs $\ddagger$
	\end{itemize}
	\item Unique
	\begin{itemize}
		\item Zwick and Paterson's algorithm for MPGs
		\item Zielonka's algorithm for PGs
		\item BCDGR's algorithm for EGs	 $\ddagger$	
	\end{itemize}
\end{itemize}
Algorithms marked with $\dagger$ also provide optimal strategies alongside the values, those marked $\ddagger$ provide an optimal strategy for one of the players. Those that don't natively provide optimal strategies can be still be utilized to find them by progressively removing edges from the game and seeing if that affects the values until all but one outgoing edge per vertex have been removed from the game, which constitute edges that can form an optimal strategy.

First we shall explain the generalized idea of Kleene Iteration and Strategy Iteration, as they can be used to solve multiple of the problems posed. Then we will explain the specifics of each implementation of those as well as unique algorithms used to solve each of the games we are interested in.\newpage
\subsubsection{Kleene Iteration}
\subsubsection{Strategy Iteration}
\subsubsection{PGs}
\subsubsection{MPGs}\label{mpg_frac}
\subsubsection{DPGs}
\subsubsection{EGs}
\subsubsection{SSGs}\newpage
\subsection{Reductions in Theory}
The reductions we use here allow us to formulate the problem of finding the value or optimal strategy of one game in a way that we can find them by solving problems in another game.
For $PG\rightarrow MPG$ and $MPG\rightarrow DPG$ the graph structure stays the exact same, for $MPG\rightarrow EG$ the graph is originally the same but while solving we recursively build a binary tree that proceeds on subgraphs of the original graph. For $DPG\rightarrow SSG$ the edges of the graph of the DPG get replaced by a certain construct that maintains a similarity between the two in \textit{some} sense, but the set of vertices and edges grows considerably in doing so.

\subsubsection{PGs to MPGs}
The reduction from PGs to MPGs is based on the idea of \cite{DBLP:journals/ipl/Jurdzinski98}.
For a given Parity Game $PG = (A,p)$ we transform it into a Mean Payoff Game $MPG = (A,w)$, with the underlying arena $A$ being the same and the weights $w$ of $MPG$ based directly on the priority function $p$ of $PG$. Optimal strategies in $MPG$ translate one to one to $PG$ and the vertex $v$ having value $\geq0$ in $MPG$ translates to it being winning for \textit{Even} in $PG$. Recall that for both $PGs$ and $MPGs$ winning strategies can be memoryless.

Consider any play $\pi_{\sigma,\tau}$ in $A$ with $n=|V|$. Given that $A$ is finite and $\pi_{\sigma,\tau}$ can continue infinitely, any such play, regardless of starting vertex $v_0$ will necessarily have a repeat vertex after a path of length $k\leq n$ and will then, by virtue of the strategies being memoryless, indefinitely continue in a cycle of length $l\leq n-k$.

From the definition of the value of a $PG$ it follows directly that only the priorities of the vertices in the cycle matter for the outcome to the game, specifically the highest priority. For MPGs we have:
\begin{align*}
	&\liminf\limits_{n\rightarrow \infty} \left(\dfrac{1}{n}\sum_{i=0}^{n-1}w((v_i,v_{i+1}))\right)=\\
	&\liminf\limits_{n\rightarrow \infty} \left(\dfrac{1}{n}\sum_{i=k}^{n-1}w((v_i,v_{i+1}))\right)+
	\liminf\limits_{n\rightarrow \infty} \left(\dfrac{1}{n}\sum_{i=0}^{k-1}w((v_i,v_{i+1}))\right)=\\
	&\liminf\limits_{n\rightarrow \infty} \left(\dfrac{1}{n}\sum_{i=0}^{n-1}w((v_{i+k},v_{i+1+k}))\right)=\\
	&\liminf\limits_{n\rightarrow \infty} \left(\dfrac{1}{n}\sum_{i=0}^{n-1}\dfrac{1}{l}\sum_{j=0}^{l-1}w((v_{j+k},v_{j+1+k}))\right)=\\
	&\dfrac{1}{l}\sum_{j=0}^{l-1}w((v_{j+k},v_{j+1+k}))
\end{align*}
So again, the only edge-weights that matter for the value of a play are those that are part of the terminal cycle.

To equate the values of the games we want to set the edge-weights in such a way that the outgoing edge of any vertex of a given cycle alone can set the parity of the value of the game if and only if the priority of the vertex is the highest in the cycle.\\
\cite{DBLP:journals/ipl/Jurdzinski98} does this by setting $w((u,v)) = (-n)^{p(u)}$. This way, the absolute weight of the outgoing edge of the vertex with highest priority is always higher than any possible make-up of the rest of the cycle, i.e. $|(-n)^{p(u)}|>(n-1)|(-n)^{p(u)-1}|$.\\
\label{pg_2_mpg}This, however, is unsatisfying since the complexity of solving MPGs with the algorithms we use is dependent on $d$, the maximum absolute weight in the MPG. We therefore want to be conservative with the edge-weights and try to minimize them.

Rather than making sure that the weight is larger, in absolute terms, compared to vertices of lower priority regardless of the count and parity of the priority of the vertices with lower priority, we want to account for both of those.\\
To aid with that we construct an intermediary function $w'$, which assigns weights to the vertices instead of the edges.
Then we define the set $V_u$ which for each vertex $u$ represents the set of vertices with lower and opposing parity of priority:
\begin{align*}
	V_u = \{ v\in V\mid p(x)<p(u)\land (p(x)\neq p(u)~\mathrm{mod}~2)\}
\end{align*}
The goal then is to define the weights of the vertices in such a way that they are always exactly as large as needed. For a given vertex $u$, we want it's weight to be exactly as large as the sum of the weights of the vertices in $V_u$ if $u$ belongs to \textit{Even/Max} and exactly one larger than the sum if $u$ belongs to \textit{Odd/Min}. We thus construct $w'$ as follows:
\begin{align*}
	w'(u) = (-1)^{p(u)}\cdot \left(\left(\sum_{x\in V_u}|w'(x)|\right) +1^{p(u)~mod~2}\right)
\end{align*}
Finally we simply define the edge-weights as $w((u,v))=w'(u)$. This way any vertex will have an outgoing edge with weight larger or equal to what the opponent can produce without visiting a vertex of higher priority, regardless of their counterstrategy $\tau$. Thus any play utilizing memoryless strategies on PG whose value is \textit{Even/Odd} will have a value of $\geq 0/\leq -1$ when translated to an MPG in such a way and any optimal strategy in \textit{PG} will accordingly be an optimal strategy in \textit{MPG}.
Conversely, any MPG that will be reduced-to in such a way will have an edge in it's terminal cycle whose absolute weight will be greater or equal to the sum of all the edges of the opponent in said cycle. The vertex said greatest edge emanates from being exactly equivalent to the vertex with highest priority in the original PG. Thus any play on such MPG whose value, or equivalently whose greatest edge, is $\geq 0/\leq -1$ will have \textit{Even/Odd} value in the original MPG and any optimal strategy in MPG will be a optimal strategy in PG.\newpage
\begin{wrapfigure}[8]{l}{4.5cm}
	\begin{tikzpicture}
		\node[style=circle, draw=black, label={1}] (A) at (0,0) {$v_0$};
		\node[style={regular polygon,regular polygon sides=4}, draw=black,label={5}] (B) at (1.75,0) {$v_1$};
		\node[shape=circle, draw=black,label={4}] (C) at (3.5,0) {$v_2$};
		\node[shape=circle, draw=black,label={below:4}] (D) at (0,-1.75) {$v_3$};
		\node[style=circle, draw=black,label={below:0}] (E) at (1.75,-1.75) {$v_4$};
		\node[style={regular polygon,regular polygon sides=4}, draw=black,label={below:3}] (F) at (3.5,-1.75) {$v_5$};
		\path [->] (A) edge (B);
		\path [->] (B) edge[out=15, in=165] (C);
		\path [->] (C) edge[out=195, in=345] (B);
		\path [->] (B) edge (E);
		\path [->] (F) edge (C);
		\path [->] (E) edge[out=60, in=210] (C);
		\path [->] (C) edge[out=240, in=30] (E);
		\path [->] (D) edge (A);
		\path [->] (D) edge (E);
		\path [->] (E) edge[out=345, in=285, looseness=4] (E);
	\end{tikzpicture}
	\caption{PG}
	\begin{tikzpicture}
		\node[style=circle, draw=black] (A) at (0,0) {$v_0$};
		\node[style={regular polygon,regular polygon sides=4}, draw=black] (B) at (1.75,0) {$v_1$};
		\node[shape=circle, draw=black] (C) at (3.5,0) {$v_2$};
		\node[shape=circle, draw=black] (D) at (0,-1.75) {$v_3$};
		\node[style=circle, draw=black] (E) at (1.75,-1.75) {$v_4$};
		\node[style={regular polygon,regular polygon sides=4}, draw=black] (F) at (3.5,-1.75) {$v_5$};
		\path [->] (A) edge node[above]{-1} (B);
		\path [->] (B) edge[out=25, in=155] node[above]{-5} (C);
		\path [->] (C) edge[out=195, in=345] node[above]{2} (B);
		\path [->] (B) edge node[left]{-5} (E);
		\path [->] (F) edge node[right]{-1} (C);
		\path [->] (E) edge[out=60, in=210] node[left]{0} (C);
		\path [->] (C) edge[out=240, in=30] node[right]{2} (E);
		\path [->] (D) edge node[left]{2} (A);
		\path [->] (D) edge node[below]{2} (E);
		\path [->] (E) edge[out=345, in=285, looseness=4] node[right]{0} (E);
	\end{tikzpicture}
	\caption{MPG}
\end{wrapfigure}
\textbf{Example 7:} We translate $PG$ to $MPG$.\\
The edge-weights are successively built as follows:\\\\
\begin{tabular}{c|c|c|c}
	$u\in V$ & $p(u)$ & $V_u$ & $w'(u)$\\\hline
	$v_4$ & $0$ & $\emptyset$ & 0 \\\hline
	$v_0$ & $1$ & $\{v_4\}$ & -1 \\\hline
	$v_5$ & $3$ & $\{v_4\}$ & -1 \\\hline
	$v_2, v_3$ & $4$ & $\{v_0, v_5\}$ & 2 \\\hline
	$v_1$ & $5$ & $\{v_2, v_3, v_4\}$ & -5 \\\hline
\end{tabular}\\\\
Here we can limit $d$ to $5$ instead of $|(-6)^5|=7776$.\\\\\\\\\\\\\\\\
\subsubsection{MPGs to DPGs}
The reduction from MPGs to DPGs is the one described in \cite{ZWICK1996343}. The underlying Arena $A$ and edge-weights $w$ stay the same.
Recall from 3.2.1 that a play in an Arena eventually results in a repeat vertex after a path of length $k\leq n$ and then indefinitely continues in a cycle of length $l\leq n-k$. In a MPG the value of such play being:
\begin{align*}
	\dfrac{1}{l}\sum_{j=0}^{l-1}w((v_{j+k},v_{j+1+k}))
\end{align*}
For DPGs we have the value being:
\begin{align}
	v(v_0)&=~(1-\lambda)\sum_{i=0}^{\infty}\left(\lambda^i\cdot w((v_i,v_{i+1}))\right)\nonumber\\
	\intertext{We want to rearrange that value, in dependance of $\lambda$, as to approach the value of an equivalent MPG. We factor out the part that pertains to the path of length $k$ that leads us to our terminal cycle.}
	&=~(1-\lambda)\sum_{i=k}^{\infty}\left(\lambda^i\cdot w((v_i,v_{i+1}))\right)+(1-\lambda)\sum_{i=0}^{k-1}\left(\lambda^i\cdot w((v_i,v_{i+1}))\right)\nonumber\\
	&=~(1-\lambda)\lambda^k\sum_{i=0}^{\infty}\left(\lambda^{i}\cdot w((v_{i+k},v_{i+1+k}))\right)+(1-\lambda)\sum_{i=0}^{k-1}\left(\lambda^i\cdot w((v_i,v_{i+1}))\right)\nonumber\\
	\intertext{The terminal circle can then be reimagined: Instead of summing up every single step one by one we split the summation into two parts. One part, with the index $j$, describing the individual steps of the cycle and one part, with the index $i$, for each of the infinite circumnavigations of that cycle.}
	&=~(1-\lambda)\lambda^k\sum_{i=0}^{\infty}\lambda^{il}\cdot \sum_{j=0}^{l-1}\left(\lambda^{j}\cdot w((v_{j+k},v_{j+1+k}))\right)+(1-\lambda)\sum_{i=0}^{k-1}\left(\lambda^i\cdot w((v_i,v_{i+1}))\right)\nonumber\\
	&=\footnotemark{}~\frac{(1-\lambda)\lambda^k}{1-\lambda^l}\sum_{j=0}^{l-1}\left(\lambda^{j}\cdot w((v_{j+k},v_{j+1+k}))\right)+(1-\lambda)\sum_{i=0}^{k-1}\left(\lambda^i\cdot w((v_i,v_{i+1}))\right)\label{l_eins}
\end{align}\footnotetext{Being a geometric series, $\sum_{i=0}^{\infty}\lambda^{il}=\sum_{i=0}^{\infty}(\lambda^l)^i=\frac{1}{1-\lambda^l}$, knowing that we have $l\geq1$ and $|\lambda|\leq1$.}
Now if we let $\lambda\to 1^-$ we get:
\begin{align*}
	&~\lim_{\lambda\to 1^-}\left(\frac{(1-\lambda)\lambda^k}{1-\lambda^l}\sum_{j=0}^{l-1}\left(\lambda^{j}\cdot w((v_{j+k},v_{j+1+k}))\right)+(1-\lambda)\sum_{i=0}^{k-1}\left(\lambda^i\cdot w((v_i,v_{i+1}))\right)\right)\\
	=&~\lim_{\lambda\to 1^-}\left(\frac{1-\lambda}{1-\lambda^l}\sum_{j=0}^{l-1}w((v_{j+k},v_{j+1+k}))\right)\\
	=&\footnotemark{}~\frac{1}{l}\sum_{j=0}^{l-1}w((v_{j+k},v_{j+1+k}))
\end{align*}\footnotetext{$\lim_{\lambda\to 1^-}\frac{1-\lambda}{1-\lambda^l}=\frac{1}{l}$ via L'H\^opital's rule}

We can't let $\lambda$ be 1, since both the normalizing $(1-\lambda)$ would result in a division by zero as well as the actual series diverging, we have to use a $\lambda<1$ that is close enough to 1, and by extension the value close enough to that of the equivalent MPG, as to enable us to truncate to the nearest rational number as was done in 3.1.4.1.
We can take \hyperref[l_eins]{(1)} and add W to all the weights. This makes is so that all weights are effectively $\geq0$, the value of the game is increased by $W$ and we can simplify to a lower bound of $v(v_0)$:
\begin{align*}
	&v(v_0)+W\\
	=~&\frac{(1-\lambda)\lambda^k}{1-\lambda^l}\sum_{j=0}^{l-1}\left(\lambda^{j}\cdot \left(w((v_{j+k},v_{j+1+k}))+W\right)\right)+(1-\lambda)\sum_{i=0}^{k-1}\left(\lambda^i\cdot \left(w((v_i,v_{i+1}))+W\right)\right)\\
	\geq~&\frac{(1-\lambda)\lambda^k}{1-\lambda^l}\sum_{j=0}^{l-1}\left(\lambda^{j}\cdot \left(w((v_{j+k},v_{j+1+k}))+W\right)\right)\\
	\geq~&\frac{(1-\lambda)\lambda^{k+l-1}}{1-\lambda^l}\sum_{j=0}^{l-1}\left(w((v_{j+k},v_{j+1+k}))+W\right)\\
	\geq~&\frac{l(1-\lambda)\lambda^{k+l-1}}{1-\lambda^l}\left(W+\frac{1}{l}\sum_{j=0}^{l-1}w((v_{j+k},v_{j+1+k}))\right)\\
	\geq~&(1-n(1-\lambda))\left(W+\frac{1}{l}\sum_{j=0}^{l-1}w((v_{j+k},v_{j+1+k}))\right)\\
	\geq~&W+\frac{1}{l}\sum_{j=0}^{l-1}w((v_{j+k},v_{j+1+k}))-n(1-\lambda)\left(W+\frac{1}{l}\sum_{j=0}^{l-1}w((v_{j+k},v_{j+1+k}))\right)
\end{align*}
Now if we revert the scaling we arrive at:
\begin{align*}
	v(v_0)\geq~&\frac{1}{l}\sum_{j=0}^{l-1}w((v_{j+k},v_{j+1+k}))-n(1-\lambda)\left(W+\frac{1}{l}\sum_{j=0}^{l-1}w((v_{j+k},v_{j+1+k}))\right)\\
	\geq~&\frac{1}{l}\sum_{j=0}^{l-1}w((v_{j+k},v_{j+1+k}))-n(1-\lambda)\cdot 2W
\end{align*}
Similarly, we can rescale by -W, such that all weights are $\leq0$ and in total we arrive at:
\begin{gather*}
	\frac{1}{l}\sum_{j=0}^{l-1}w((v_{j+k},v_{j+1+k}))+n(1-\lambda)\cdot 2W\\
	\geq v(v_0)\geq\\
	\frac{1}{l}\sum_{j=0}^{l-1}w((v_{j+k},v_{j+1+k}))-n(1-\lambda)\cdot 2W
\end{gather*}
From 3.1.4 we know that the minimum distance between two valid values of a MPG is $\frac{1}{n(n-1)}$, therefore we want to set $\lambda$ in such a way as to constrict the size of the just derived interval to at most:
\begin{align*}
	|2n(1-\lambda)\cdot 2W|\leq\frac{1}{n(n-1)}
\end{align*}
We get this precision with $\lambda = 1-\frac{1}{4n^3W}$.
If we reduce a MPG to a DPG with such $\lambda$, we can deduce the value for a given vertex of the original MPG by taking the corresponding value $v$ in the DPG and rounding to the unique rational number, with denominator at most $n$, in the closed interval $v\pm \frac{1}{2n(n-1)}$.

\subsubsection{MPGs to EGs}\label{rescale}
The reduction from MPGs to EGs is the one described in \cite{Brim2011}.\\
If we play an EG on $A, w$ of a MPG, any vertex for which the value is $v_{\sigma, \tau}(v)\neq\infty$ is equal to a vertex in the MPG whose value is $v_{\sigma, \tau}(v)\geq0$, since the player can force a terminal cycle whose edge-weights sum up to $\geq0$. This effectively splits the vertices of the graph into two regions for which we know that the value is $\geq0$ for one region and $<0$ for the other.
We can use this dichotomy of spliting the MPG into two areas together with the fact that we can rescale the edge-weights of MPGs.
By solving these rescaled games as EGs we can further and further restrict the possible values of any given vertex in the MPG, until only one possible solution remains.
As laid out in \hyperref[mpg_frac]{3.1.4}, the value of the vertices of a MPG are all in the set
\begin{align*}
	& S=\left\{\frac{p}{q}\in\mathbb{Q}\mid q\in\left[1, |V|\right]\subset\mathbb{N}\land p\in\left[-q\cdot W, q\cdot W\right]\subset\mathbb{Z} \right\}.
\end{align*}
In particular, it is finite and after at most $log_2(|V|\cdot W)$ dichotomies, only one rational number, the value of the vertex, remains.\\
The way we rescale the MPG is by taking the interval of the range of possible values  that are left, starting at S, and then split interval in about half by taking next lowest rational number $\frac{p}{q}\in S$ to the middle of the interval. We then rescale the edge-weights of the MPG, $w$, for the next dichotomy by $(w\cdot q)-p$ and solve it as an EG to split the vertices of the MPG into two regions with value $\geq\frac{p}{q}$ and $<\frac{p}{q}$.

To avoid having to solve the resulting EGs for the entire graph for any subdivision of $S$, we can subdivide the graph alongside the possible interval of values. In the MPG, for a given set of strategies $\sigma, \tau$, any move will land on a vertex with the same value as the previous since the play ultimately always lands in the same terminal cycle with the same mean weight. This means that, for a fixed set of strategies, vertices with different values will necessarily be disconnected if one removes the edges that aren't in play under $\sigma, \tau$ and the values of all vertices stay the same if those edges are removed.\\
What this allows us to do is that after every dichotomy wrt. the values, where we split the set of vertices into three sets, one with value $>\nu$, one with value $<\nu$ and one with known value $\nu$, we can restrict subsequent dichotomies to disjoint subsets of the graph in addition to halving the interval of possible values, since we know that vertices with different value, as shown to be the case by the dichotomy, have to be disconnected if unused edges are removed. This means subsequent solving via EGs happens on smaller graphs, making it faster.\newpage
\subsubsection{DPGs to SSGs}
The reduction from DPGs to SSGs is the one described in \cite{ZWICK1996343}.\\
The values of DPGs range from $-W$ to $W$ whereas in SSGs they range from $0$ to $1$.
We want to rescale our edge-weights in such a way that the values map to the new range while simultaneously maintaining their weight relative to one another.\\
We do so by rescaling the weights of the DPG to $w'=\frac{w+W}{2W}$ in the SSG. This maintains their relation to each other and by extension the relation between values of resulting plays and thus also the optimal strategies. After solving for the values of the SSG we can get the equivalent value in the DPG by simply reversing this rescaling.\\
For SSGs, unlike for DPGs, there is no ``stack of previous passed edge-weights'' to calculate the value off of, so we need to emulate such behaviour.\\
For any given $e=(u,v)\in E$ we can say that, after translation to an SSG, $u$ has probability of $\lambda$ to reach $v$. This means that, whatever value $v$ has, it will be proportionally probabilistically represented in the value of $u$ if $e$ is played. Since the decision to play $e$ is made by a player, but the probability to reach $v$ from $u$ is exactly that, a probability, and thus necessarily originates from a \textit{Random} vertex, we have to insert an additional Random vertex, $u'$, for each edge of the original DPG.\\
The equivalent aspect of the weight of the edge in the DPG, $w(e)$, also needs to be mirrored in the SSG. We can do so by giving the remaining probability $1-\lambda$ the corresponding chance to reach $\mathfrak{0}$ and $\mathfrak{1}$. Using the previously rescaled edge-weights $w'$ we can easily achieve this by setting $p(u')(\mathfrak{0})=(1-\lambda)\cdot (1-w'(e))$ and  $p(u')(\mathfrak{1})=(1-\lambda)\cdot w'(e)$ respectively. Effectively we are replacing every edge with the following construct:
\begin{center}
	\begin{tikzpicture}
		\begin{scope}
			\node[style={regular polygon,regular polygon sides=4}, draw=black,minimum size=34pt] (u) at (0,0) {$u$};
			\node[style={regular polygon,regular polygon sides=4},  draw=black,minimum size=34pt] (v) at (1.75,0) {$v$};
			\path[->] (u) edge node[above] (3) {$w$} (v);
			v			\graphboxthick[boxeins]{(u) (v) (3)}
		\end{scope}
		\begin{scope}[shift={($(boxeins.east)+(1.5,0)$)}]
			\node[style={regular polygon,regular polygon sides=4}, draw=black,minimum size=34pt] (u) at (0,0) {$u$};
			\node[shape=diamond, draw=black,minimum size=34pt] (u') at (1.75,0) {$u'$};
			\node[style={regular polygon,regular polygon sides=4},  draw=black,minimum size=34pt] (v) at (3.5,0) {$v$};
			\node[shape=diamond,  draw=black,minimum size=34pt] (0) at (3.5,1.25) {$\mathfrak{0}$};
			\node[shape=diamond,  draw=black,minimum size=34pt] (1) at (3.5,-1.25) {$\mathfrak{1}$};
			\path[->] (u) edge node[above] (3) {} (u');
			\path[->] (u') edge node[above] (4) {$\lambda$} (v);
			\path[->] (u') edge node[above,xshift=-40pt] (4) {$(1-\lambda)(1-w'(e))$} (0);
			\path[->] (u') edge node[below,xshift=-30pt] (4) {$(1-\lambda)w'(e)$} (1);
			\graphboxthick[boxzwei]{(u) (v) (3) (4) (0) (1)}
		\end{scope}
		\path[->] (boxeins.east) edge node[above] {} ($(boxzwei.west)$);
		\node at ($(boxeins.north)+(0,0.3)$) {$DPG$};
		\node at ($(boxzwei.north)+(0,0.3)$) {$SSG$};
	\end{tikzpicture}
\end{center}
Random vertices then have value $v(u')=\lambda\cdot v(v)+(1-\lambda)\cdot w'(e)$ which exactly equal to the value of $u$ in the DPG with rescaled edge-weights given that $e$ is chosen as the outgoing edge. Since all outgoing edges will be replaced by this construct, the value of $u$ is equal in the rescaled DPG and the SSG regardless of the strategies played.
To translate back the optimal strategies generated on SSG one simply has to translate back every $(u,u')$ that is part of a strategy in SSG to it's equivalent $(u,v)$ in the DPG.\newpage
\section{Solutions and Reductions in Practice}
While the theoretical descriptions of the algorithms do a good job of explaining the idea of how the approaches work or ought to work, they may leave some room for interpretation about how exactly those should be implemented. We want to explain in detail how specifically we implemented them and what accommodations we made for the sake of performance.

All the games have, at their core, some directed graph as a key component. In addition to that there are some recurring themes that are exhibited by some or all of the games: The vertices of a graph belong to a player, the edges may have a weight or probability attached and for PGs, the vertices themselves have a value attached.

The vertices don't inherently have any ordering, so we think of them as just a numbering through the size of the set of vertices. We will call the generic size of that set $n =|V|$ and use 0-indexing from here on out, i.e. the ``first'' vertex is $v_0$ and the last is $v_{n-1}$.

The most fundamental question is how we represent the graph structure programmatically. There's two obvious choices:\\
Two or more parallel lists of length $|E|$ representing the edges of the game with the first two representing source vertices and target vertices and further lists representing additional values associated with that edge, e.g. edge-weights in MPGs \textit{or}\\
An incidence matrix of size $n\times n$, The values of the matrix being the value that is associated with that edge, with a special value signaling that the edge doesn't exist and a simple boolean signaling doesn't/does exist for PGs and SSGs.\\
The first method has the advantage of using less memory, since we only use memory for edges that actually exist. The second method has the advantage that edges that have the same source or target vertex are memory-aligned, thus easy and fast to work with, as they are the same row or column in the matrix.\\
Given that a lot of operations include the semantic of ``all edges from/to a certain vertex'' and that even bigger graphs are manageable in memory size ($|V|=16384$ with $int32$ means about 1GB) to the point where the runtime of algorithms applied will become a problem before memory size does we opt for the second variant.
The \textit{special value} that signals that the edge the matrix entry represents doesn't exist will generally, depending on context, be the minimum/maximum value the datatype can represent or NAN. If we represent matrices here, we will use $\times$ to represent these special values. Here we will pretend that operations on the matrices will effectively ignore these special values in all cases. Most prominently this means that their indices or values will never be returned by (arg)min or (arg)max operations. In practise there is some further logic involved in the handling of those special values. This doesn't add much complexity over how the implementations are explained here, but it does add varying degrees of runtime.\\

For example, the edges of a  graph may be represented by a matrix as follows:

\begin{figure}[H]
	\centering
	\begin{minipage}{.5\textwidth}
		\centering
		\begin{tikzpicture}
			\node[style={regular polygon,regular polygon sides=4}, draw=black] (A) at (0,0) {$v_0$};
			\node[shape=circle, draw=black] (B) at (1.75,0) {$v_1$};
			\node[shape=circle, draw=black] (C) at (3.5,0) {$v_2$};
			\node[shape=circle, draw=black] (D) at (0,-1.75) {$v_3$};
			\node[shape=circle, draw=black] (E) at (1.75,-1.75) {$v_4$};
			\path [->] (A) edge[out=105, in=75,looseness=8] node[above]{$2$} (A);
			\path [->] (B) edge node[above]{$4$} (A);
			\path [->] (C) edge node[above]{$-2$} (B);
			\path [->] (C) edge[out=105, in=75,looseness=8] node[above]{$-4$} (C);
			\path [->] (D) edge node[right]{$1$} (A);
			\path [->] (E) edge node[above]{$-1$} (D);
			\path [->] (E) edge node[right]{$2$} (B);
			\path [->] (E) edge[out=15, in=345,looseness=8] node[right]{$1$} (E);
		\end{tikzpicture}
	\end{minipage}%
	\begin{minipage}{.5\textwidth}
		\centering
		\[
		\begin{bmatrix}
			2 & \times & \times & \times & \times\\
			4 & \times & \times & \times & \times\\
			\times & -2 & -4 & \times & \times\\
			1 & \times & \times & \times & \times\\
			\times & 2 & \times & -1 & 1
		\end{bmatrix}
		\]
	\end{minipage}
\end{figure}
We shall simply call such matrices representing edges $E$. Similarly, the ownership of the vertices would be denoted as:
\[
\begin{bmatrix}
	0 & 1 & 1 & 1 & 1
\end{bmatrix}
\]
We refer to such ownership matrices as $O$. $0$ or $False$ denotes ownership to Player 0, $1$ or $True$ to Player 1 and for SSGs we also have $2$ for \textit{Random} vertices.

For slicing vectors and matrices we will use the following notation: Given a matrix A, for which we only want to keep the edges for which the source and target vertices' indices are in $su = [0,1,4]$, we write $A[su,su]$.
If a dimension should be unaffected by slicing, i.e. if we want to maintain all the entries from that dimensions we write ``$:$''. E.g. $A[:,su]$ means to keep all rows but only the columns with their index in $su$.
\begin{figure}[H]
	\centering
	\begin{minipage}{.33\textwidth}
		\centering
		\[
		A=
		\begin{bmatrix}
			2 & \times & \times & \times & \times\\
			4 & \times & \times & \times & \times\\
			\times & -2 & -4 & \times & \times\\
			1 & \times & \times & \times & \times\\
			\times & 2 & \times & -1 & 1
		\end{bmatrix}
		\]
	\end{minipage}%
	\begin{minipage}{.33\textwidth}
		\centering
		\[
		A[su,su]=
		\begin{bmatrix}
			2 & \times & \times\\
			4 & \times & \times\\
			\times & 2 & 1
		\end{bmatrix}
		\]
	\end{minipage}%
	\begin{minipage}{.33\textwidth}
	\centering
	\[
	A[:,su]=
	\begin{bmatrix}
		2 & \times & \times\\
		4 & \times & \times\\
		\times & -2 & \times\\
		1 & \times & \times\\
		\times & 2  & 1
	\end{bmatrix}
	\]
	\end{minipage}
\end{figure}
\begin{figure}[H]
	\centering
	\begin{minipage}{.5\textwidth}
		\centering
		\[
		B=
		\begin{bmatrix}
			2 & \times & -4 & 5 & 7
		\end{bmatrix}
		\]
	\end{minipage}%
	\begin{minipage}{.5\textwidth}
		\centering
		\[
		B[su]=
		\begin{bmatrix}
			2 & \times & 7
		\end{bmatrix}
		\]
	\end{minipage}%
\end{figure}
The code shown henceforth is a simplification of the actual code. Especially peculiarities of python or numpy, such as the notation of slicing matrices or the handling of aforementioned special values, are simplified without affect the logic of the underlying algorithm.
\subsection{Solutions in Practice}
\subsubsection{PGs}
Parity games additionally have priorities associated with it's vertices. We shall represent them similarly to how we represent the ownership of the vertices except their range is over $\mathbb{N}_0$ instead of a boolean and called $P$.

\textbf{Zielonka's algorithm:}

\begin{lstlisting}[numbers=left,breaklines=true,breakatwhitespace=true,escapechar=|]
def zielonka(O, E, P):
    if len(O) == 0:
        return []
    else:
        max_prio = max(P)
        player = max_prio % 2
        A = find_attractor(player, O, E, P == max_prio|\footnote{As in the list of indices of P where the value is equal to max\_prio}|)
        z1 = zielonka(O[A|$^{-1}$|]|\footnote{A$^{-1}$ as in the inverse of A with respect to the preexisting indexing of O}|, E[A|$^{-1}$|,A|$^{-1}$|], P[A|$^{-1}$|])
        if not any(z1 == player|$^{-1}$|):
            return [player]*len(O)|\footnote{A vector of length len(O) only containing the value ``player''}|
        else:
            losing = [False]*len(O)
            losing[A|$^{-1}$|[z1 == |$\neg$|player]] = True
            B = find_attractor(|$\neg$|player, O, E, losing)
            z2 = zielonka(owner[B|$^{-1}$|], edges[B|$^{-1}$|], priorities[B|$^{-1}$|])
            ret = [|$\neg$|player]*len(O)
            ret[B|$^{-1}$|[z2 == player]] = player
\end{lstlisting}

\textbf{L.2-3:} The recursion in Zielonka's algorithm eventually hit it's bottom by reaching an empty graph. Since obviously there is no vertex to be won, we return an empty vector.\\
\textbf{L.5-6:} Each iteration starts with finding the attractor set of the highest priority. We determine such region and the associated player.\\
\textbf{L.7:} To find the attractor we need the player for whom the attractor set works and the set of initial vertices that actually posses the priority in question. In return we receive the set of vertices from which the player can and the opponent cannot avoid reaching the set of highest priority effectively making it an \textit{attractor}.\\
\textbf{L.8:} The first recursive call.\\
\textbf{L.9-10:} If the first recursive call returns that the game contains no winning region for the opponent then we can conclude that the current game is entirely won by the current player.\\
\textbf{L.12-14:} If there is a winning region for the opponent in the first recursive call on the game sans the first attractor, that means that the opponent can win in the attractor of such winning region since they can avoid A entirely.\\
\textbf{L.15:} We know that B is a winning region for the opponent. We continue to recursively find winning regions for subgames without B, until we reach an empty game by courtesy of L.2-3.\\
\textbf{L.16-17:} Such winning regions then get forwarded back through the recursion, together with B, as a win or lose of the respective player.

\textbf{The algorithm used to find the attractors:}

\begin{lstlisting}[numbers=left,breaklines=true,breakatwhitespace=true,escapechar=|]
def find_attractor(player, O, E, Nh):
    if len(Nh) == 0:
        return []
    us = owner == player
    them = owner != player
    atr = Nh
    while True:
        old = atr
        us_add = any|\footnote{Which rows contain any \textit{True} value; Effectively: All vertices of \textit{player} that have edges into the attractor; see https://numpy.org/doc/stable/reference/generated/numpy.any.html}|(edges[us,atr], axis=1)
        them_add = any(edges[them, atr|$^{-1}$|], axis=1)|$^{-1}$|
        atr[us[us_add]] = True
        atr[them[them_add]] = True
        if all(atr == old):
            return atr
\end{lstlisting}

\textbf{L.1:} The algorithm is of course dependent on the graph (O, E) but also on the player whose attractor we want to find, since one player want to avoid it, i.e. have edges that \textit{don't} lead into the attractor and the other one wants to reach it. It also needs an initial set of vertices, Nh, which the attractor actually attracts to.\\
\textbf{L.2-3:} Same as with Zielonka's algorithm, an empty set will of course have no attractor.\\
\textbf{L.4-5:} We precompute once the indices of the vertices that belong to the player of the opponent respectively.\\
\textbf{L.6-8:} We iterate over the original set Nh until we hit an iteration that didn't lead to the attractor growing in size.\\
\textbf{L.9:} We gather all the indices of vertices belonging to the player that have edges into the current iteration of the attractor set.\\
\textbf{L.10:} We gather all the indices of vertices belonging to the opponent that have no edges that don't go into the current iteration of the attractor set. Note the double negation.
\textbf{L.11-12:} The vertices we gathered in L.9-10 get added to the attractor set.\\
\textbf{L.13-14:} If L.11-12 didn't lead to a change in the attractor set then we return the current, final, attractor set.

Given any algorithm to find values, we can use it to also find strategies in a costly manner. We show this implementation here exemplary for Zielonka's algorithm.

\textbf{Finding strategies via Zielonka's algorithm:}\label{strat_solve}

\begin{lstlisting}[numbers=left,breaklines=true,breakatwhitespace=true,escapechar=|]
def solve_strat_zielonka(self|\footnote{\textit{self} refers to a game object with the fields $O$, $E$ and $P$, it's ownership, edge and priority vector/matrix}|):
    z = self.solve_value_zielonka()	
    ret = [-1]*len(self.O)
    edges = self.E
    for i,v in enumerate|\footnote{enumerate iterates over an enumerable and provides the index (i) together with the element (v)}|(edges):
        w = v == True
        while True:
            cl = ceil(len(w) / 2)
            one, two = w[:cl], w[cl:]
            e = edges
            e[i] = False
            e[i, one] = True
            x = ParityGame(self.O, e, self.P).solve_value_zielonka()
            if all(x == z):
                if len(one) == 1:
                    ret[i] = one[0]
                    break
                else:
                    w = one
            else:
                w = two
        edges[i] = [False]*len(self.O)
        edges[i, ret[i]] = True
    return ret
\end{lstlisting}

\textbf{L.2:} z are the values of the vertices which we compare to while removing all but one outgoing edge for each vertex.\\
\textbf{L.3:} This holds the strategy, we start with $-1$ i.e. no strategy for each vertex.\\
\textbf{L.4:} We create a copy of the edges from which we progressively delete edge.\\
\textbf{L.5-7:} For each vertex we iteration until we find an edge that is part of an optimal strategy.\\
\textbf{L.8-12:} We part the outgoing edges of our current vertex into two sets and create an edge matrix that only contains the edges of the first set.\\
\textbf{L.13-14:} We solve for the values for our game with restricted edges and check if the values stay the same. If they do, clearly the removed edges are not necessary for an optimal strategy.\\
\textbf{L.15-19:} If the values did stay the same then the edge part of optimal strategies are in the ``one'' half of the edges. If there is only one edge, this is \textit{the} edge for an optimal strategy.\\
\textbf{L.20-21:} If the values change, the edges for optimal strategies must have been in the ``two'' half.\\
\textbf{L.22-23:} After we choose a successor vertex for a vertex we need to make the corresponding edge mandatory, since optimal strategies aren't necessarily unique but may depend on the decision we made for the current vertex. Otherwise we may choose an edge for subsequent vertices that are part of \textit{some} optimal strategy, but not an optimal strategy that is compatible with the choice we already made here.

\subsubsection{MPGs}
Mean Payoff games are solely descibed by the ownership vector, $O$, and the edge matrix, $E$ that also contains the edge-weights.

\textbf{Zwick and Paterson's algorithm:}

\begin{lstlisting}[numbers=left,breaklines=true,breakatwhitespace=true,escapechar=|]
def solve_value_zwick_paterson(O, E):
    W = max(abs(E))
    k = 4 * (len(O) ** 3) * W
    v = [0]*len(O)
    for _ in range(k):
        edges_weight = edges + v
        v[O] = min(edges_weight, axis=1)
        v[|$\neg$|O] = max(edges_weight, axis=1)
    v = v / k
    return trunc(len(O), v)
\end{lstlisting}

\textbf{L.2-3:} First we determine the iteration depth required by the algorithm.\\
\textbf{L.4-5:} We instantiate with 0 and then iterate as often as the algorithm requires.\\
\textbf{L.6:} For every iteration we generate the k-th iteration of all possible vertices and outgoing edges combinations.\\
\textbf{L.7-8:} For vertices belonging to \textit{Min} we take the vertex-edge combination that leads to the lower value, for \textit{Max} the one that leads to the highest.\\
\textbf{L.9:} Since the tabulation is cumulative we need to rescale once at the end.\\
\textbf{L.10:} Since this approach only gives approximations the algorithm prescribes rounding to the appropriate rational number at the end.

\newpage

\textbf{The algorithm used to truncate to the appropriate rational number:}

\begin{lstlisting}[numbers=left,breaklines=true,breakatwhitespace=true,escapechar=|]
def trunc(d, v):	
    lower = v - (1 / (2 * d * (d - 1)))
    upper = v + (1 / (2 * d * (d - 1)))
    for v_n in range(d):
        for denominator in range(1, d + 1):
            f = floor(lower[v_n] * denominator)
            c = ceil(upper[v_n] * denominator)
            num = range(f, c + 1) / denominator
            if any((lower[v_n] < num) & (upper[v_n] > num)):
                v[v_n] = num[((lower[v_n] < num) & (upper[v_n] > num))][0]
                break	
    return v
\end{lstlisting}

\textbf{L.1:} Given a maximum denominator and a vector of values, we truncate to the nearest valid rational number.\\
\textbf{L.2-3:} From the algorithm we know that this is the interval in which the actual rational values have to be.\\
\textbf{L.4-7:} For every entry in the vector and every possible denominator we determine a lower and an upper bound for possible numerators.\\
\textbf{L.8:} We create a range of possible rational numbers that correspond to the rational numbers that possibly lie in the interval.\\
\textbf{L.9-11:} If we find a number that fits into the interval it must be the rational number we are looking for and we proceed with the next number.

\textbf{Finding strategies via Zwick and Paterson's algorithm:} Similar to finding Strategies for PGs under Zielonka's algorithms we can perform the same kind of search here. Since it's perfectly analogous, with the exception of solving for the values of a MPG, we won't repeat it here.\newpage
\subsubsection{EGs}
Similarly to Mean Payoff games, Energy games are solely descibed by the ownership vector and the edge matrix.

\textbf{BCDGR's algorithm:}

\begin{lstlisting}[numbers=left,breaklines=true,breakatwhitespace=true,escapechar=|]
def solve_both_bcdgr(O, E):
    l = [False]*len(O)
    for v in (O == False):
        if all(E[v] < 0):
            l[v] = True
    for v in (O == True):
        if any(E[v] < 0):
            l[v] = True
    f = [0]*len(O)
    cnt = [0]*len(O)
    for v in (O == False):
        for w in E[v]:
            if leq|\footnote{The modified kind of $\preceq$ as laid out in the formal description of the algorithm}|(minus|\footnote{The modified kind of $\ominus$ as laid out in the formal description of the algorithm}|(f[w], E[v,w]), f[v]):
                cnt[v] += 1
    while any(l):
        v = argmax(l)
        l[v] = False
        old = f[v]
        if not O[v]:
            f[v], _ = min*|\footnote{Gives the value and the index of the minimum as a tuple}|([minus(f[w], E[v, w]) for w in E[v]])
        else:
            f[v], _ = max*([minus(f[w], E[v, w]) for w in E[v]])
        if not O[v]:
            cnt[v] = 0
            for w in E[v]:
                if leq(minus(f[w], E[v, w]), f[v]):
                    cnt[v] += 1
        for u in [u for u in E[:, v] if not leq(minus(f[v], E[u, v]), f[u])]:
            if not O[u]:
                if leq(minus(old, E[u, v]), f[u]):
                    cnt[u] -= 1
                if cnt[u] <= 0:
                    l[u] = True
            else:
                l[u] = True
    return_strat = [-1]*len(E)
    for i in range(len(O)):
        if not O[i]:
            _, cand = min*([minus(f[w], E[i, w]) for w in E[i])
            return_strat[i] = E[i,cand]
    return f, return_strat
\end{lstlisting}

\textbf{L.2:} l is the list of vertices that potentially have an outdated progress measure.\\
\textbf{L.3-8:} We initialize l with the vertices that can't have a progress measure of 0.\\
\textbf{L.9-14:} f is the current progress measure and cnt is count as laid out in the formal description fo the algorithm and it's initialization.\\
\textbf{L.15-18:} We go through the list of vertices that we have to update, keeping a copy of f[v].\\
\textbf{L.19-22:} We update the current progress measure of the vertex depending on whose vertex it is with the minimum/maximum progress measure possible, depending on the successor vertices.\\
\textbf{L.23-27:} We update the count of vertices that depend on the current vertex $v$.\\
\textbf{L.28:} For reach vertex in pre(v) that is now outdated:\\
\textbf{L.29-33:} If vertex if of \textit{Charging} decrement count and add vertex to l\\
\textbf{L.34-35:} If vertex if of \textit{Depleting} add vertex to l\\
\textbf{L.36-40:} Read out optimal strategy for \textit{Charging} based off of the lowest cost successor.

\textbf{Strategy Iteration from below:}

\begin{lstlisting}[numbers=left,breaklines=true,breakatwhitespace=true,escapechar=|]
def solve_both_strat_iter_below(self):
    p1 = self.O == True
    nW = len(self.O) * max(abs(self.E))
    E_p1 = self.E[p1, p1]
    cycle_nodes, neg_strat = find_all_negative_cycle_nodes(E_p1)
    O = self.O + [False]
    E = self.E
    E[len(O)+1] = |$\times$|
    E[:len(O)+1] = |$\times$|
    E[len(O)+1, len(O)+1] = 0
    E[self.O, len(O)+1] = -2 * nW
    restriction = p1[cycle_nodes]|$^{-1}$|
    O = O[restriction]
    E = E[restriction, restriction]
    strat = [-1]*len(O)
    strat[O] = apply_along_axis(lambda v: random.choice(v), 1, E[O])
    while True:
        strat_hist = strat.copy()
        f = [0]*len(O)
        while True:
            old = f.copy()
            E_weight = maximum(minimum(f - E, 3 * nW), 0)
            f[O] = E_weight[O, strat]
            f[|$\neg$|O] = min(E_weight[|$\neg$|O], axis=1)
            if f == old:
                break
        g = E_weight[:, strat] < E_weight[:, argmax(E_weight, axis=1)]
        strat[O&g] = argmax(E_weight, 1)[O&g]
        if strat_hist==strat:
            break
    final_f = [-1]*len(self.O)
    f[|$\neg$|(f < nW)] = -1
    final_f[p1[cycle_nodes]|$^{-1}$|] = f
    return_strat = [-1]*len(self.E)
    return_strat[p1[cycle_nodes]] = p1[neg_strat]
    return_strat[p1[cycle_nodes]|$^{-1}$|][strat!=-1] = restriction[strat][strat!=-1] 
    return final_f, return_strat
\end{lstlisting}

\textbf{L.2-14:} First we add the $s\in V_0$ vertex and restrict the game to a version without negative cycles that are in total control of \textit{Depleting}.\\
\textbf{L.15-16:} We start of with a random strategy. For every vertex of \textit{Depleting} we choose a random successor.\\
\textbf{L.17-18,29-30:} We iterate until our strategy doesn't change anymore, i.e. we have found an optimal stratey for \textit{Depleting}.\\
\textbf{L.19-26:} We solve a game based on the fixed strategy of \textit{Depleting} via Kleene Iteration.\\
\textbf{L.27-28:} We check for which vertices we have an actual improvement and change the strategy accordingly.\\
\textbf{L.31-37:} Return the values and strategy after some scaling back wrt the negative cycles removed previously. Set values in excess of nW to infinite.

\textbf{Strategy Iteration from above:}

\begin{lstlisting}[numbers=left,breaklines=true,breakatwhitespace=true,escapechar=|]
def solve_both_strat_iter_above(self):
    p1 = self.O == True
    nW = len(self.O) * max(abs(self.E))
    E_p1 = self.E[p1, p1]
    cycle_nodes, neg_strat = find_all_negative_cycle_nodes(E_p1)
    O = self.O + [False]
    E = self.E
    E[len(O)+1] = |$\times$|
    E[:len(O)+1] = |$\times$|
    E[len(O)+1, len(O)+1] = 0
    E[self.O, len(O)+1] = -2 * nW
    restriction = p1[cycle_nodes]|$^{-1}$|
    O = O[restriction]
    E = E[restriction, restriction]
    strat = [-1]*len(O)
    strat[|$\neg$|O] = [len(O)-1]*len(O)
    while True:
        strat_hist = strat.copy()
        solver = pywraplp.Solver.CreateSolver("GLOP")
        v = [solver.NumVar(float(0), float(3 * nW), str(x)) for x in range(len(O))]
        for s, p in enumerate(O[:-1]):
            if not p:
                solver.Add(v[s] >= (v[strat[s]] - float(E[s, strat[s]])))
            else:
                for t in where(E[s] != mini)[0]:
                    solver.Add(v[s] >= (v[t] - float(E[s, t])))
        solver.Add(v[-1] == float(0))
        obj_func = v[0]
        for v_n in v[1:]:
            obj_func += v_n
        solver.Minimize(obj_func)
        status = solver.Solve()
        v = [v_n.solution_value() for v_n in v]
        while True:
            strat[|$\neg$|O] = argmin(clip(v - E, 0, None), 1)
            if any(strat != strat_hist):
                break
            v_ = [True]*len(v)
            while True:
                v_h = v_.copy()
                c1 = v != 0
                c2 = v.transpose() == clip(v - E, 0, 3 * nW)
                c3 = 0 < v
                c4 = 0 < clip(v - E, 0, None)
                c5 = clip(v - E, 0, None) <= (3 * nW)
                c6 = v_
                V_p0 = c1 & any(c2 & c3 & c4 & c5 & c6, 1)
                V_p1 = c1 & all((|$\neg$|c2 or (c3 & c4 & c5 & c6)), 1)
                v_[O] = V_p1
                v_[|$\neg$|O] = V_p0
                if all(v_ == v_h):
                    break
            if not any(v_):
                final_f = [-1]*len(self.O)
                v[v < nW] =  v[v < nW]
                final_f[p1[cycle_nodes]|$^{-1}$|] = v
                return_strat = [-1] * len(self.E)
                return_strat[p1[cycle_nodes]] = p1[neg_strat]
                return_strat[p1[cycle_nodes]|$^{-1}$|][strat != -1] restriction[strat]
                return_strat[return_strat == len(self.O)] = apply_along_axis(lambda v: random.choice(v),1,self.E)
                return final_f, return_strat
            else:
                v[v_] -= 1
\end{lstlisting}

\textbf{L.2-14:} We again add the $s\in V_0$ vertex and restrict the game to a version without negative cycles that are in total control of \textit{Depleting}.\\
\textbf{L.15-16:} The starting strategy for \textit{Charging} is for every vertex to go to the special sink vertex.\\
\textbf{L.18-33:} We solve a linear program based on the current strategy for \textit{Charging}.\\
\textbf{L.35-37:} The find an optimal counter strategy for \textit{Depleting}. If the strategy changed we iterate to the next linear program, if it didn't we check if our fix-point is the least.\\
\textbf{L.38-52:} We determine $\bar{V}$.\\
\textbf{L.53-61:} If $\bar{V}$ is empty we have our least fix-point. We return the values and strategy minus reverting the preprocessing as we did for the Strategy Iteration from below.\\
\textbf{L.62-63:} If $\bar{V}$ isn't empty we decrement it's contents and try to find a another counter-strategy.
\newpage
\textbf{The algorithm used to find all negative cycles:}

Since Bellman-Ford only gives at least some negative cycles but not all, we need to iterate to find all such cycles. 

\begin{lstlisting}[numbers=left,breaklines=true,breakatwhitespace=true,escapechar=|]
def find_all_negative_cycle_nodes(E):
    neg_strat = [-1]*len(E)
    cycle_nodes = [False]*len(E)
    while True:
        restriction = |$\neg$|cycle_nodes
        ret, ret_strat = find_negative_cycle_nodes(E[restriction, restriction])
        if len(ret) == 0:
            break
        else:
            neg_strat[restriction[ret]] = restriction[ret_strat]
            cycle_nodes[restriction[ret]] = True
    if any(cycle_nodes):
        while True:
            old = cycle_nodes.copy()
            adds = any(E[:, cycle_nodes], axis=1)
            adds[cycle_nodes] = False
            neg_strat[adds] = cycle_nodes[argmax(E[adds, cycle_nodes],axis=1)]
            cycle_nodes = (cycle_nodes or adds)
            if all(cycle_nodes == old):
                break
    return cycle_nodes, neg_strat[cycle_nodes]
\end{lstlisting}

\textbf{L.2-11:} We start with no knowledge of any vertices being part of any negative cycle and then progressively add those that we find.\\
\textbf{L.6:} Any search may give us cycles and the strategy that would be deployed to stay in such circle, which is what \textit{Depleting} wants to do.\\
\textbf{L.12-21:} Once we have found all negative cycles, we also need to include all all vertices that inevitably lead into those cycles.
\newpage
\textbf{The Bellman-Ford algorithm derivate used to find negative cycles:}

We add a special vertex that connects to every other vertex. If after $|V|$ steps, i.e. after we necessarily have entered a loop somewhere, we can still lower the cost to reach a certain vertex, we must have discovered a negative cycle.

\begin{lstlisting}[numbers=left,breaklines=true,breakatwhitespace=true,escapechar=|]
def find_negative_cycle_nodes(E):
    edges_src = E
    edges_src[:len(E)] = 0
    edges_src[len(E)] = |$\times$|
    dist = [|$\infty$|]*len(edges_src)
    dist[len(E)] = 0
    pred = [-1]*len(edges_src)
    for i in range(1, len(edges_src) + 1):
        src_is_pred = (dist != |$\infty$|)*len(edges_src).transpose()
        new_dist = dist.transpose() + edges_src
        shorter = new_dist < dist
        valids = edges_src & src_is_pred
        exists_shorter = any(valids & shorter, axis=0)
        shorter_idx = argmin(new_dist |$\textit{if}$| valids |$\textit{else}$| |$\infty$|, axis=0)
        pred = shorter_idx |$\textit{if}$| exists_shorter |$\textit{else}$| pred
        dist[exists_shorter] = new_dist[shorter_idx]
    cycle_nodes_t = [False]*len(E)
    for n in (exists_shorter == True):
        cycle_nodes = [False]*len(E)
        s = [n]
        for x in range(len(edges_src) - 1):
            if pred[n] == s[0]:
                cycle_nodes[s] = True
                break
            else:
                s += pred[n]
                n = pred[n]
        cycle_nodes_t = (cycle_nodes_t or cycle_nodes)
    return pred[ret], cycle_nodes_t
\end{lstlisting}

\textbf{L.2-4:} We start by adding a special vertex that has an edge to every other vertex.\\
\textbf{L.5-7:} We start of by knowing no path towards any other vertex.\\
\textbf{L.8-16:} We iterate until we surpass the size of the vertex, so that every other vertex will have been able to be reached. For every iteration we check if we can discover a new, shorter path to any of the vertices and note the distance and predecessor.\\
\textbf{L.17-29:} The previous step only shows us vertices that are part of a negative cycle. To find \textit{all} the vertices that are part of such cycle we need to unwind them.
\newpage
\subsubsection{DPGs}
DPGs get the additional field $D$ for their discount. \textit{player} describes from whose perspective we want to find the values and strategy, with \textit{False} meaning \textit{Max}.

\textbf{Strategy Iteration:}

\begin{lstlisting}[numbers=left,breaklines=true,breakatwhitespace=true,escapechar=|]
def solve_both_strat_iter(self, player):
    strat = [-1]*len(self.O)
    strat[player==self.O] = apply_along_axis(lambda x: random.choice(x), 1, self.E)
    while True:
        strat_hist = strat.copy()
        W = max(abs(self.E))
        solver = pywraplp.Solver.CreateSolver("GLOP")
        v = [solver.NumVar(float(-W), float(W), str(x)) for x in range(len(self.O))]
        if not player:
            for s, p in enumerate(self.O):
                if not p:
                    solver.Add(v[s] == (1 - float(self.D)) * float(self.E[s, strat[s]]) + float(self.D) * v[strat[s]])
                else:
                    for t in self.E[s]:
                        solver.Add(v[s] <= (1 - float(self.D)) * float(self.E[s, t]) + float(self.D) * v[t])
        else:
            for s, p in enumerate(self.O):
                if p:
                    solver.Add(v[s] == (1 - float(self.D)) * float(self.E[s, strat[s]]) + float(self.D) * v[strat[s]])
                else:
                    for t in self.E[s]:
                        solver.Add(v[s] >= (1 - float(self.D)) * float(self.E[s, t]) + float(self.D) * v[t])
        obj_func = v[0]
        for v_n in v[1:]:
            obj_func += v_n
        if not player:
            solver.Maximize(obj_func)
        else:
            solver.Minimize(obj_func)
        status = solver.Solve()
        if not player:
            strat[|$\neg$|self.O] = argmax(((1 - self.D) * self.E) + (self.D * ([v_n.solution_value() for v_n in v])),axis=1)
        else:
            strat[self.O] = argmin(((1 - self.D) * self.E) + (self.D * ([v_n.solution_value() for v_n in v])),axis=1)
        if all(strat_hist==strat):
            break
    return [v_n.solution_value() for v_n in v], strat
\end{lstlisting}

\textbf{L.2-3:} We start of with a random strategy. For every vertex of \textit{player} we choose a random successor.\\
\textbf{L.4-5,35-37:} We iterate until the strategy doesn't change any more and return the current strategy and values.\\
\textbf{L.6-30:} We solve the Linear Program for the given strategy to find the values of the game under optimal play from the opponent.\\
\textbf{L.9-15,16-22:} We in particular distinguish whether we are minimizing or maximizing (the inequality on L15 \& 22) for the vertices of the opponent depending on whose perspective we are searching from.\\
\textbf{L.31-34:} Depending on the perspective we change strategy to one that maximizes or minimized the value based on the current values.

\textbf{Kleene Iteration:}

\begin{lstlisting}[numbers=left,breaklines=true,breakatwhitespace=true,escapechar=|]
def solve_both_kleene(O, E, D):
    W = max(abs(E, 0))
    cur = [-W]*len(O)
    while True:
        old = cur.copy()
        E_weight = ((1 - D) * E) + D * cur
        cur = [0]*len(O)
        cur[|$\neg$|O] = max(edges_weight, axis=1)
        cur[O] = min(edges_weight, axis=1)
        if max(abs(cur - old)) < 1e-14:
            break
    strat = [-1]*len(O)
    strat[|$\neg$|O] = argmax(((1 - D) * E) + D * cur, axis=1)
    strat[O] = argmin(((1 - D) * E) + D * cur, axis=1)
    return cur, strat
\end{lstlisting}

\textbf{L.2-3:} We start our iteration with the bottom element.\\
\textbf{L.4-5,10-11:} We iteration until the the maximum change in any value falls below a certain threshold. The threshold can in theory be 0 which would be equal to iterating until the (theoretical) change in values is smaller than the precision floats numerically can handle.\\
\textbf{L.6:} For every iteration we generate the entire matrix of possibilites of outgoing edges for every vertex and the accompanying value.\\
\textbf{L.7-9:} We continue with the maximum/minimum value for the respective player's vertices.\\
\textbf{L.12-14:} Given the values of the game we can also read out optimal strategies based off of the successor vertices needed to reach the already calculated values.
\subsubsection{SSGs}
For SSGs we have an additional matrix, \textit{AVG}, that gives us the transition probability from any \textit{Random} vertex to it's successor vertices.

\textbf{Strategy Iteration:}

\begin{lstlisting}[numbers=left,breaklines=true,breakatwhitespace=true,escapechar=|]
def solve_both_strat_iter(self, player):
    strat = [-1]*len(self.O)
    strat[self.O == player] = apply_along_axis(lambda x: random.choice(x), 1, self.E)
    while True:
        strat_hist = strat.copy()
        solver = pywraplp.Solver.CreateSolver("GLOP")
        v = ([solver.NumVar(float(0), float(1), str(x)) for x in range(len(self.O))] +
            [solver.NumVar(float(0), float(0), str(str(len(self.O) + 1)))] +
            [solver.NumVar(float(1), float(1), str(str(len(self.O) + 2)))])
        for s, p in enumerate(self.O):
            if p == player:
                solver.Add(v[s] == v[strat[s]])
            else:
                if p == 0:
                    for t in self.E[s]:
                        solver.Add(v[s] >= v[t])
                elif p == 1:
                    for t in self.E[s]:
                        solver.Add(v[s] <= v[t])
                else:
                    val = 0
                    for t in self.E[s]:
                        val += v[t] * self.AVG[s, t]
                    solver.Add(v[s] == val)
        obj_func = v[0]
        for v_n in v[1:]:
            obj_func += v_n
        if not player:
            solver.Maximize(obj_func)
        else:
            solver.Minimize(obj_func)
        status = solver.Solve()
        if not player:
            strat[|$\neg$|self.O] = argmax(self.E * [v_n.solution_value() for v_n in v]), axis=1)
        else:
            strat[self.O] = argmin(self.E * [v_n.solution_value() for v_n in v]), axis=1)
        if all(strat_hist == strat):
            break
    return [v_n.solution_value() for v_n in v], strat
\end{lstlisting}

\textbf{L.2-3:} We start of with a random strategy. For every vertex of \textit{player} we choose a random successor.\\
\textbf{L.4-5,37-39:} We iterate until the strategy doesn't change any more and return the current strategy and values.\\
\textbf{L.6-32:} We solve the Linear Program for the given strategy to find the values of the
game under optimal play from the opponent.\\
\textbf{L.21-24:} Unlike the degrees of freedom the opponent has in choosing a successor vertex which results in a half-space in the Linear Program, \textit{Random} just has an equality relationship in dependance of it's successors and transition probabilities.\\
\textbf{L.33-36:} Depending on the perspective we change strategy to one that maximizes or minimized the value based on the current values.

\textbf{Kleene Iteration:}

\begin{lstlisting}[numbers=left,breaklines=true,breakatwhitespace=true,escapechar=|]
def solve_both_kleene(O, E, AVG):
    cur = [0]*len(O) + [0] + [1]
    while True:
        old = cur.copy()
        edges_weight = E*cur
        cur[O == 2] = sum(AVG * cur, 1)
        cur[O == 1] = min(edges_weight, axis=1)
        cur[O == 0] = max(edges_weight, axis=1)
        max_err = max(abs(cur - old))
        if max_err < 1e-14:
            break
    strat = [-1]*len(O)
    strat[O==0] = argmax(cur, axis=1)
    strat[O==1] = argmin(cur, axis=1)
    return cur, strat
\end{lstlisting}

\textbf{L.2:} We start our iteration with the bottom element.\\
\textbf{L.3-4,10-11:} We iteration until the the maximum change in any value falls below a certain threshold. The threshold can in theory be 0 which would be equal to iterating until the (theoretical) change in values is smaller than the precision floats numerically can handle.\\
\textbf{L.6:} For every iteration we generate the entire matrix of possibilites of outgoing edges for every vertex and the accompanying value.\\
\textbf{L.7-9:} We continue with the maximum/minimum value for the respective player's vertices.\\
\textbf{L.12-14:} Given the values of the game we can also read out optimal strategies based off of the successor vertices needed to reach the already calculated values. For \textit{Random} we simply leave these as -1 as to not disturb the indexing.
\newpage
\subsection{Reductions in Practice}
\subsubsection{PGs to MPGs}

As already shown in \hyperref[pg_2_mpg]{3.2.1}, the original algorithm in \cite{DBLP:journals/ipl/Jurdzinski98} stipulates the weights to be rewritten as $w((u,v)) = (-n)^{p(u)}$ which is wasteful wrt. the size of the range of weights in the resulting MPG.

\textbf{Reducing a PG to a MPG:}
\begin{lstlisting}[numbers=left,breaklines=true,breakatwhitespace=true,escapechar=|]
def to_mpg(self):
    prios_k = argsort(self.P)
    prios = self.P[prios_k]
    prios_even = prios % 2 == 0
    weight = [1]*len(self.O)
    for i, pr in enumerate(prios):
        if pr % 2 == 0:
            weight[i] = sum(weight[:i][|$\neg$|prios_even[:i]])
        else:
            weight[i] = sum(weight[:i][prios_even[:i]]) + 1
    weight[|$\neg$|prios_even] = -weight[|$\neg$|prios_even]
    y = [0]*len(self.O)
    y[prios_k] = weight
    E = [[|$\times$|]*len(self.O)]*len(self.O)
    E = y*len(self.O)
    return MeanPayoffGame(self.O, E)
\end{lstlisting}

\textbf{L.2-3:} We produce a sorted copy, \textit{prios} of the priority list along with a list of indices that sort the original priority list.\\
\textbf{L.4:} We make a boolean list can masks \textit{prios} wrt parity.\\
\textbf{L.5:} We intialize the list specifically with one, since if we start with an odd number, the following even number would not actually be greater.\\
\textbf{L.6-10:} We go through the list one by one. It the priority is even, it's value is the sum of all prior odd numbers. Since a value of $\geq0$ is winning for \textit{Max} in the MPG, reaching equality is sufficient. For odd priorities, at sum all the prior even numbers plus one, to ensure win in a MPG that would otherwise have a value of zero.\\
\textbf{L.11:} The calculated weights so far are absolutes. We negate the priorities associated with \textit{Odd} to get the desired effect of the reduction.\\
\textbf{L.13:} We undo the prior reindexing to sort the list.\\
\textbf{L.14-15:} We scale the resulting list to a matrix to correspond with the edges in the MPG in a way that is congruent with the incidence matrix of the PG, i.e. we translate over the $\times$.
\newpage
\subsubsection{MPGs to EGs}

There's two significant differences to \cite{Brim2011} compared to how we implemented the idea here.\\
Firstly, in the original the algorithm solves four energy games at every ``level of recursion''. This means that two possible values can be ``detected'' for vertices at every level but also means that the runtime is significantly larger for every level. To depress the runtime we only run two energy games per level. This means every level is faster but also only detects for one value and we might have to iterate deeper but since deeper iteration runs on subgames it is faster.\\
Secondly, we also extract the strategies from the subgames. Out of the original proof of the algorithm one can relatively straight forward reason that strategies on subgames are strategies in the total game.

\textbf{Reducing the value \& strategy problem in a MPG to a EG:}
\begin{lstlisting}[numbers=left,breaklines=true,breakatwhitespace=true,escapechar=|]
def solve_both_eg_alg(O, E, lower, upper):
    r = range(1, len(O) + 1)
    l = r * (lower + upper) / 2
    l1 = floor(l)
    l2 = ceil(l)
    ar1 = argmax(l1 / r)
    ar2 = argmin(l2 / r)
    a1 = [l1[ar1], r[ar1]]
    a2 = [l2[ar2], r[ar2]]
    e1 = (a1[1] * E) - a1[0]
    e2 = (-a1[1] * E) + a1[0]
    f1, s1 = EnergyGame(O, e1).solve_both()
    f2, s2 = EnergyGame(|$\neg$|O, e2).solve_both()
    v = [-1]*len(O)
    s = [-1]*len(O)
    v[(f1 != -1) & (f2 != -1)] = a1[0] / a1[1]
    s[(f1 != -1) & (f2 != -1)] = s1 if s1 != -1 else s2
    v1 = (f1 == -1)
    v2 = (f2 == -1)
    if len(v1) != 0:
        v[v1], st = solve_both_eg_alg(O[v1], E[v1, v2)], lower, a1[0] / a1[1])
        s[v1] = v1[st]
    if len(v2) != 0:
        v[v2], st = solve_both_eg_alg(O[v2], E[v2, v2)], a2[0] / a2[1], upper)
        s[v2] = v2[st]
    return v, s
\end{lstlisting}

\textbf{L.2-9:} First we calculate the two rational numbers closest to middle of our interval. Since actually need the numerator and denominator, we save each rational number as a tuple of those two.\\
\textbf{L.10-13:} We calculate the two reweighted edge-matrices based on the rational number next lowest to the middle of the interval and solve them via some EG algorithm that solves for both value and optimal strategy.\\
\textbf{L.16:} If the value of a vertex if finite for both games then the rational number we rescaled for must be the value of the corresponding vertex in the original MPG.\\
\textbf{L.17:} The strategy of that vertex can be read from one of the games, depending on whose vertex it is.\\
\textbf{L.20-25:} The remaining vertices get split into two recursive searches based on if their values was higher or lower than the rational number we used as a division for this level. Critically, for the ``higher'' search we use rational number that was \textit{higher} than the middle of our current interval.
\newpage
\subsubsection{MPGs to DPGs}

The reduction from MPGs to DPGs is trivial but for completeness sake we include it:

\textbf{Reducing a PG to a MPG:}
\begin{lstlisting}[numbers=left,breaklines=true,breakatwhitespace=true,escapechar=|]
def to_dpg(self):
    W = max(abs(self.edges))
    discount = 1 - (1 / (4 * (len(self.O) ** 3) * W))
    return DiscountedPayoffGame(self.owner, self.edges, discount)
\end{lstlisting}

The owner vector and edge matrix stay exactly the same, the discount gets calculated off of the size of the graph and range of the edge-weights as described in the theory.
\newpage
\subsubsection{DPGs to SSGs}
During the process of translating a DPG into an SSG edges get rewired. To be able to correctly align the vertices for the purpose of finding strategies for the DPG and for the purpose of scaling back the values to what they were previous to the translate this function also gives back \textit{strat\_map} and \textit{W} with which the strategies and values and trivially be corrected wrt the translation.

\textbf{Reducing a DPG to a SSG:}
\begin{lstlisting}[numbers=left,breaklines=true,breakatwhitespace=true,escapechar=|]
def to_ssg(self):
    W = max(abs(self.E, 0))
    E = (self.E + W) / (2 * W)
    f3 = count_nonzero(self.E)|\footnote{As in count the number of non-$\times$ edges}|
    vertices = len(self.O) + f3
    ssg_E = ([False]*vertices)*(vertices + 2)
    O = self.O + [2]*f3
    AVG = ([0]*f3)*(vertices + 2)
    strat_map = [0]*f3
    for i, edge in enumerate(self.E):
        ssg_E[edge[0], i + len(self.O)] = True
        ssg_E[i + len(self.O), edge[1]] = True
        strat_map[i] = edge[1]
        ssg_E[i + len(self.O), -1] = True
        ssg_E[i + len(self.O), -2] = True
        AVG[i, edge[1]] = self.D
        AVG[i, -2] = (1 - self.D) * (1 - (E[edge]))
        AVG[i, -1] = (1 - self.D) * E[edge]
    return SimpleStochasticGame(O, ssg_E, AVG, True), strat_map, W
\end{lstlisting}

\textbf{L.2-3:} We rescale the weights to fit in the [0,1] interval appropriate for the SSG.\\
\textbf{L.4:} We count the number of edges in the DPG. This the amount by which we need to increase the ownership vector, edge matrix and probability matrix.\\
\textbf{L.5-9:} We prepare the aforementioned vector and matrices as well as a vector for translating back the strategies formulated in the SSG.\\
\textbf{L.10-18:} For every edge in the DPG we add the equivalent construct in the SSG as well as it's implications in the \textit{strat\_map}.
\newpage
\section{Implementation}\newpage
\section{Evaluation}
Our goal was to compare the different algorithms used to solve problems, especially under reduction to other games, and to see how they compare. To that end we generated random games with multiple parameters and benchmarked all methods we had to solve the respective problems. For any combination of problem, parameters for generation and algorithm used to solve the problem we have $N = 100$ and the times shown repesent the average time of those 100 runs. For their generation, all games have a parameter $n =|V|$ that measures the size of the graph by means of the number of vertices and a parameter $p$ which describes the probability that any possible edge $e\in (V\times V)$ is part of the generated game, meaning the expected out-degree of every vertex is $|V|\cdot p$. Since we require for all games that each vertex has outdegree $\geq 1$, we consequently require that $\frac{1}{|V|} \leq p \leq 1$. We then assign each vertex one random edge and all other potential outgoing edges of that vertex have probability $\frac{(p\cdot |V|)-1}{|V|-1}$ of existing. $p$ can be understood as a measure of how connected a graph is. In fact, towards the extreme ends, for $p=\frac{1}{|V|}$ and sufficiently large $|V|$ one would expect a disconnected graph on one end and for $p=1$ a complete graph on the other end. Each vertex is randomly assigned to a player with equal probability, including the ``Random'' player in SSGs. Games may have additional parameters that are described seperately for each game. The parameters were chosen in a way that they intuitively represent extremes towards each end and then some reasonable middle ground. For all cases, the runtimes are either weakly dependent on $p$ other parameters or they scale monotonously wrt. to those parameters. One can therefore think of intermediate parameterizations as some interpolation between the shown values. For readability we only highlight some algorithms here. Those that work without reduction to another game and any other that are of interest. The other ones are faded out into the background, with the full results in the appendix.\\
Some of the algorithms for solving for optimal strategies only return partial strategies, i.e. the strategy for only one of the players. If that is the case, we mark them $\Square$ and $\Circle$ depending on whose strategy they can solve for. If they can solve for both but not at once, running the algorithm twice, once for each player, yields the entire strategy.
\subsection{PGs}
For Parity games, the additional parameter is $w$, which describes the range over which the priorities are handed out. Similar to the ownership of the vertices, the priority of vertices is randomly chosen from the possible range with equal chance for each. In theory as well as in this implementation, for the purpose of solving the problems one can rescale the range of the priorities to $\{0, 1, ... |V|\}$ or smaller without affecting the values or optimal strategies of the game. A higher $w$ however decreases the overlap of priorities between vertices on generation, which has a minor impact on the runtime of Zielonka's algorithm.

In theory, Strategy Iteration via DPGs ought to be a contender for large enough $n$ for both the value and stratey problem. The reason for why it isn't depends on the reduction from MPGs to DPGs and is laid out in the subsection for MPGs.

Strategy Iteration from above via EGs can outperform Zielonka's algorithm for large $|V|$ and reasonably dense graphs. However, the break-even point is only reached for such large $|V|$ and consequently such long runtimes that gathering statistically significant runtime data is impractical, it can however be confirmed for individual instances. We therefore cannot describe exact inflection points for the parameterization when the Strategy Iteration starts outperforming Zielonka's algorithm precisely.

Other routes of reduction and applied algorithms fail to be useful, because the edge-weights in the resulting MPG scale very strongly wrt. to the size of the PG and it's range of priorities. Many subsequent algorithms scale in runtime in dependence of the range of the edge-weights of the resulting MPG or equivalent parameter that are downstream from the range of the edge-weights under further reduction (range of edge-weights of EGs, discount factor of DPGs...). Strategy Iteration from above via EGs works decently here precisely because it performs indifferently to the range of edge-weights.

\subsubsection{Value Problem of PGs}
For solving the Value Problem of PGs the result are very clear. For all cases tested, Zielonka's Algorithm is straight up the fastest. It's simplicity demands very little overhead and as such small graphs are solved near instantaneously. It's recursive nature allows it to scale very well to big graphs as well. Less connectivity, as shown by smaller $p$, mean the attractors as laid out in the algorithm tend to not reach as far -- meaning more levels of recursion are needed to cover the entire graph. Similargly, bigger $w$ means that the winning regions that act as a crystallisation point for the attractors tend to start out smaller and thus form smaller attractors.
\begin{figure}[H]
	\centering
	\input{part_graphs/pg_value}
	\caption{Solving the value problem of PGs}
\end{figure}\newpage
\subsubsection{Strategy Problem of PGs}
Unfortunately our naive approach to finding optimal strategies via Zielonka's algorithm is $|V|\cdot log_2(\frac{|E|}{|V|})$ times slower than finding the values alone, meaning the big lead it had in finding the values of games is diminished significantly, bringing it more in line with other algorithms. Depending on the exact $p$ and $w$, the BCDGR algorithm via EGs, which has similarly low overhead to Zielonka's algorithm, but doesn't require repeated invokation to deliver optimal strategies, has better runtimes for reasonably small graphs. However, since it scales worse, such advantage is quickly lost with growing $|V|$. After that, Zielonka's algorithm, even with the malus it suffer for finding optimal strategies, is the fastest available method until Strategy Iteration from above via EGs may take over .
\begin{figure}[H]
	\centering
\input{part_graphs/pg_strat}
\caption{Solving the strategy problem of PGs}
\end{figure}\newpage
\subsection{MPGs}
The additional parameter is $w$, describing the range over which the edge-weights are handed out and for our random generation. They are once again randomly and uniformly distributed over their range.
Unlike with PGs, in this case the meaning of $w$ cannot be scaled down to make subsequently applied algorithms faster. As a result we see some algorithms scale strongly in response to changing $w$ across it's range and even beyond.

\label{mpg_dpg}
In theory, Strategy Iteration via DPGs or SSGs is faster than other algorithms for large enough $n$. In practice however this isn't easily achieved. The method by which MPGs are reduced to DPGs results in discount factors very close to $1$. So close in fact, that for big enough MPGs the difference $1-\lambda$ approaches the limit of the precision standard floating point implementations can represent \cite{754}. As a result, standard Linear Program solvers are unable to solve such Linear Programs that are part of the strategy iteration with adequate precision. One could conceive a Linear Program solver that works with larger floats that enable higher precision. However this only raises the ceiling of what is solveable marginally. It also eventually forces the floats to exceed standard register sizes, meaning the floating point arithmetic fundamental to the Linear Program solver would have to resort to emulating floating point arithmetic for larger floats, which would take more clock cycles per operation and ultimately longer runtimes, undermining the usefulness of the approach in the first place. Effectively this mean that, even if this approach is plausible, it is never pratically viable for MPGs reduced to DPGs.

Kleene Iteration via DPGs or SSGs also fails to be useful because of the discount factor being close to one. The convergence rate of the fundamental fix-point iteration depends directly on the discount factor, with a higher discount factor directly leading to slower convergence.\newpage
\subsubsection{Value Problem of MPGs}
Unlike for PGs the native approach used here, Zwick and Paterson's algorithm isn't a clear winner. Intuitively the $k = 4\cdot |V|^3\cdot d$ iterations the algorithm goes through to guarantee correcteness are way in excess of what's practically needed. This means it's only competetive for graphs with very small $|V|$. As with PGs, solving with BCDGR via EG offers low overhead and thus reasonably fast runtimes for smaller graphs, but again as with PGs the approach scales mediocrely wrt. $|V|$.\\
As with PGs, Strategy Iteration from above via EGs eventually outperforms BCDGR via EG for some parameterizations. \textit{Unlike} with PGs, we don't suffer from an as large $d$ any more, meaning the approach becomes competitive much sooner. The exact inflection point depend on the specific MPG in question, with a relatively large overlap where either may be superior depending on the underlying MPG in question.
\begin{figure}[H]
	\centering
	\input{part_graphs/mpg_value}
	\caption{Solving the value problem of MPGs}
\end{figure}\newpage
\subsubsection{Strategy Problem of MPGs}
As with Zielonka's algorithm on PGs, Zwick and Paterson's algorithm for optimal strategies has an additional cost of $|V|\cdot log_2(\frac{|E|}{|V|})$ over finding the values of the game. Unlike with Zielonka's algorithm on PGs this makes Zwick and Paterson's algorithm non-viable for all conditions for finding strategies. Since BCDGR and Strategy Iteration from above via EG both don't have any malus when finding optimal strategies over values, their relative performance stays the same is was for finding values, with BCDGR being the choice for smaller graphs and Strategy Iteration for larger graphs.
\begin{figure}[H]
	\centering
	\input{part_graphs/mpg_strat}
	\caption{Solving the strategy problem of MPGs}
\end{figure}\newpage
\subsection{EGs}
Once more the additional parameter $w$ describes the range over which the edge-weights are handed out and they are once again randomly and uniformly distributed over their range.

Since all algorithms of interest perform the same for solving for values and optimal strategies, we do not distinguish between the two problems here.

Strategy Iteration from above/below differ internally in whether they use Linear Programming/Kleene Iteration to solve the partial problem within them. Kleene Iteration has very little constant overhead, which allows Strategy Iteration from below to outperform Strategy Iteration from above for smaller graphs. Since it scales significantly worse than the solving of Linear programs, Strategy Iteration from above eventually overtakes for all paramterizations. The exact inflection point varies widely, with the tendency of Strategy Iteration from below to scale worse the less connected the graph is while Strategy Iteration from above scales very indepedant from anything but size.

BCDGR outperforms both for all parameterizations. Especially at the low end, but it maintains a $>3$ fold advantage across all cases tested. However, BCDGR and Strategy Iteration from above seem to scale very similarly asymptotically. If one can improve the runtime of Strategy Iteration from above by just a \textit{constant} factor of $>3$, which is easily conceivable, then it stands to be a contender for fastest method for at least some parameterizations.
\begin{figure}[H]
	\centering
	\input{part_graphs/eg_both}
	\caption{Solving both EG problems}
\end{figure}
From looking at the performance of solving MPGs via EGs one might expect BCDGR to underperform compared to Strategy Iteration from above. However the relative performance of the two is heavily dependent on the way the reduction from MPGs to EGs works. The sub-problems spawned during the reduction from MPGs to EGs do no represent a random sample and are significantly skewed in a way that it disadvantages BCDGR.
\subsection{DPGs}
For DPGs we could have two additional parameters: $\lambda$ which is the discount factor and $w$ which again describes the range of the edge-weights. Since all approaches are indifferent to $w$, we omit it.

As with EGs, all algorithms solve for the value as well as optimal strategies, so we do not distinguish them here.

As already hinted at while solving MPGs by Kleene Iteration via DPGs, Kleene Iteration for DPGs has a convergence rate directly tied to the discount factor while being indifferent towards the connectivity of the graph. Meanwhile Strategy Iteration is weakly dependent on the discount factor while being strongly dependent on the connectivity of the graph, especially towards the lower end.\\
This leads to an interesting situation where instead of one approach eventually outperforming all others for some $|V|$ threshold, it equally depends on the connectivity and the discount factor which approach is favourable.\\
Strategy Iteration and Kleene Iteration via SSGs are viable, but consistently slower by some constant factor.
\footnotetext[13]{$S(x)=\frac{1}{1+e^{-x}}$}
\begin{figure}[H]
	\centering
	\input{part_graphs/dpg_both}
	\caption{Solving both DPG problems}
\end{figure}
\subsection{SSGs}
For evaluating SSGs we run into a problem: Strategy Iteration only works for stopping-SSGs and we have no generalized way to generate those. What we do is we compare the runtimes for DPGs converted to SSGs, which \textit{will} be stopping-SSGs but \textit{will not} be a random sample of stopping-SSGs. Here, $p$, $\lambda$ and $n$ refer to the original DPG before reduction. Again, both approaches solve for both value and optimal strategies.

The result is, unsurprisingly, reminiscent of the results for Strategy Iteration and Kleene Iteration for DPGs. Computationally speaking, SSGs are solved very similarly to DPGs for both Strategy Iteration as well as Kleene Iteration. For both, the sink vertices and the introduction of the \textit{Random} vertices don't have significant implications towards the complexity of the underlying Linear Programs or fix-point iterations. As a result, the runtime behaviour is essentially the same as for DPGs when adjusted for the contortions caused by the reduction from DPGs to SSGs. Particularly the diffraction of the runtime of the Kleene Iteration wrt. $p_{DPG}$ is an artefact of the reduction, where a higher $p_{DPG}$, i.e. a higher $|E_{DPG}|$, leads to a higher $|V_{SSG}|$ in the resulting graph which in turn reduces the convergence rate of the fix-point iteration. Whereas the Linear Program \textit{does} grows fourfold in dimensionality with respect to $|E_{DPG}|$, three of those are equality relationships and only one of them is a half-space, effectively only impacting the runtime complexity of the Linear Program by a constant factor as can be seen by the difference between solving DPGs via Strategy Iteration natively on DPGs versus after reducing to SSGs.
\begin{figure}[H]
	\centering
	\input{part_graphs/ssg_both}
	\caption{Solving both stopping-SSG problems}
\end{figure}
\section{Conclusion and Future}
\subsection{Conclusion}
We set out to see how the different ways to solve problems, the reductions between those problems and the combination of the two play out and compare to one another in a real-world implementation.
To that end we implemented all algorithms and reduction and benchmarked all combinations of those.

As laid out in the Introduction, during the reduction the games tend to grow in some way. $PG\rightarrow MPG$ causes the range of the edge-weights in $MPG$ to grow faster than the range of priorities in $PG$, in $DPG\rightarrow SSG$ both $|V_{SSG}|$ and $|E_{SSG}|$ grow faster than $|V_{SSG}|$ and $|E_{SSG}|$ and in $MPG\rightarrow EG$ the underlying graph stays unchanged, but we need to recursively solve multiple instances of EGs to solve one MPG. Additionally, for the algorithms we tested, $MPG\rightarrow DPG$ resulting in a discount factor very close to one means that the resulting DPG becomes more demanding to solve.\\
This may make it seem like approaches that involve reduction are futile, however this is not always the case.\\
One exception is that despite the disadvantage, a reduction and algorithm may still outperform algorithms that are native to the original game. This is the case for $MPG\rightarrow EG$, where BCDGR via EGs, despite the necessary recursion it outperforms Zwick and Paterson's algorithm on the original MPG. This was in fact a major motivation for the BCDGR algorithm \cite{Brim2011} and we can confirm that this theoretical advantage holds in practice.\\
Another is that the algorithm does get outperformed asymptotically in part due to the reduction, but performs well enough for smaller problems that it's still viable for some range of problems. This happens while solving for optimal strategies via $PG\rightarrow EG$. Here BCDGR via EGs suffers from the large range of edge-weights the reduction $PG\rightarrow MPG$ creates, but isn't affected enough for smaller games as to be immediately non-viable.

The most noteable cases are those involving reduction and Strategy Iteration utilizing Linear Programming (i.e. all but Strategy Iteration from below for EGs).\\
For PGs, the reduction to EGs and solving by Strategy Iteration from above, particularly for optimal strategies, does seem to have some use cases. However these are for relatively large games and Zielonka's algorithm is usefull for a wide range of games, in part by a huge margin.\\
The really interesting case seems to be reducing MPGs to EGs and then solving by Strategy Iteration from above. Beyond a certain size of the game, this method seems to outperform all other approaches for all cases.
In theory the same logic should extend to solving MPGs via Strategy Iteration via DPGs, however as laid out in \hyperref[mpg_dpg]{6.2}, this fails due to the inability to get a satisfying amount of numerical precision.

Keep in mind that for all approaches the exact runtimes depend on the specific implementation in question and can thus change to some degree. The general relations between the runtimes of the approaches would be expected to stay broadly the same, but details, particularly the inflection points of when exactly one approach is expected to start to outperform another are subject to the specific implementation and can change in response to a change in implementation.\\
The exception would be BCDGR versus Strategy Iteration from above on EGs, which display similar asymptotic behaviour with seemingly only some constant difference in runtime. If Strategy Iteration from above could be sped up significantly, if could potentially not just change the inflection point but the whole relation of the two where Strategy Iteration from above is strictly faster for some or all EGs above a certain size.

\subsection{Future}
If the objective is the absolute speed of solving any of the problems, the way to go would be an implementation dedicated to that.
Here, we focused on relative speed rather than absolute speed.
A first step would be an implementation in a compiled language, rather than Python. Explorative tests showed a speed-up of about 30 times in C++ for Kleene Iterations and Strategy Iteration with Linear Programming.\\
Another improvement that can be made use of is the continual development and improvement of Linear Programm solvers. For all but the smallest graph, the Strategy Iteration algorithms that utilize Linear Programming do so in a way that Linear Programming represents most of the actual workload. If significant advances are made here, these would be expected to translate very well into the runtime behaviour of Linear Program based Strategy Iteration.\\
Some of the approaches such as all those utilizing Kleene Iteration and Zwick and Paterson's algorithm for MPGs make heavy use of matrix and vector operations in a way that they could greatly benefit from hardware acceleration such as on a GPU.

Ultimately the most significant improvements are probably possible by way of new algorithms altogether. For some problems there are already algorithms that are theoretically better, like \cite{lmcs:8953} for PGs, but a common theme is that those do not actually perform better or even worse in practise than previous approaches.\\
Ideally we'd want a polynomial-time algorithm for any of our problems, but so far those have eluded us \cite{10.1007/978-3-319-41540-6_15}.