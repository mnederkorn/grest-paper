\section{Introduction}
There are practical applications (cite) which are able to be modeled by or reduced to various games. Considering the many ways there are to solve those games, the possibility to (repeatedly) reduce them to one another and various ways to implement any of those algorithms there are many potential means to solve problems on those games. We want to see how the different ways to solve problems, the reductions between those problems and the combination of the two play out and compare to one another in a real-world implementation.
\subsection{Task}
The games we concern ourselves here are \textit{Parity Games} (PG), \textit{Mean Payoff Games} (MPG), \textit{Energy Games} (EG), \textit{Discounted Payoff Games} (DPG) and (stopping) \textit{Simple Stochastic Games} (SSG). For any of those games there are multiple problems, but the ones we concern outselves here are ``what is the value of vertex $v$ under optimal strategies from both players?'' and ``what are the optimal strategies?''. For both of those problems, there may already be multiple algorithms to solve them for any specific game directly, some of which are generalizable to multiple of the games to some degree, such as Kleene Iteration and Strategy Iteration. Additionally we can reduce the games to another as follows:
\begin{center}
	\begin{tikzpicture}
		\node (A) at (0,0) {$PG$};
		\node[right=of A] (B) {$MPG$};
		\node[right=of B] (C) {$DPG$};
		\node[right=of C] (D) {$SSG$};
		\node[below=of B] (E) {$EG$};
		\path [->] (A) edge (B);
		\path [->] (B) edge (C);
		\path [->] (C) edge (D);
		\path [->] (B) edge (E);
	\end{tikzpicture}
\end{center}
The reductions themselves are computationally relatively simple compared to the problems themselves. Some of them, however, produce graphs that are bigger in some way ($PG\rightarrow MPG$, $DPG\rightarrow SSG$), which increases the complexity of subsequently applied algorithms, or recursively reduces to multiple sub-problems ($MPG\rightarrow EG$).
Another part we have control over is how exactly the algorithms are implemented. The biggest factor here being that, due to their nature, the edge-weights can be seen as an incidence matrix and can therefore make use of matrix operations, especially for simple operations such as addition and multiplication.
\subsection{Structure}
The paper is structured as follows:

In Chapter 2 we will first define the different kinds of games we are concerned with and the notion of $value$ as on objective for the respective games as well as (memory-less) strategies as a means to achieve those values.\\
In Chapter 3 we will first explain in 3.1 the idea of value and strategy iteration and then both concrete implementations of those as well as other algorithms for the respective games. In 3.2 we will then show how the different games and their underlying graphs can be translated to one another and how the problems posed on those games are finally reduced.\\
In Chapter 4 we show the specifics as to how the theoretical algorithms were internally implemented and elaborate on the choices and decisions arising in the implementations of the reductions and solutions.\\
In Chapter 5 we show how the implementations embeds to be used to solve specific problems or to demonstrate with the GUI.\\
In Chaper 6 we show the kinds of graphs and problems we used to evaluate the solutions and the potential benefit of prior reduction as well as the results of the evaluation.\\
In Chapter 7 we interpret the results of the evaluation and give suggestions and ideas for further improvement.
\section{Preliminaries}
Foundational to our task are the different kinds of \textit{Infinite Games} and how to determine the outcome of each such game. To do so, we also want a notion of \textit{strategies} on such Infinite Games. To later reduce the games to one another, we furthermore want a definition of a \textit{reduction} in the computational complexity sense.
\subsection{Directed Graphs}
Let $V$ be a finite set of Vertices, let $E\subseteq V \times V$ be a set of Edges, then $G = (V,E)$ is a \textit{Directed Graph}.
%We also define $src\colon E\rightarrow V$ as $src((u,v))=u$ and $tgt\colon E\rightarrow V$ as $tgt((u,v))=v$.
We also define
\begin{align*}
	pre\colon V\rightarrow \mathcal{P}(V) && pre(v)=\{u\in V\mid (u,v)\in E\}\\
	post\colon V\rightarrow \mathcal{P}(V) && post(v)=\{u\in V\mid (v,u)\in E\}
\end{align*}
\subsection{Arenas}
An arena is an extension of Directed Graphs, where the set of Vertices, $V$, is partitioned into two disjunct subsets $V_0$ and $V_1$, respectively denoting the regions where player 0, also represented by $\Square$, and player 1, also represented by $\Circle$, are to play. We also require that the out-degree of every vertex is at least one, so that any play on the Arena can always be prolonged.\newline
Formally, let $(V,E)$ be a non-trivial Directed Graph, $V_0\cup V_1 = V,~V_0\cap V_1 = \emptyset$ be a partition of V and $\forall v\in V\colon post(v)\ne \emptyset$,
then $A=(V,(V_0,V_1),E)$ is an Arena.

\begin{wrapfigure}[5]{l}{4.5cm}
	\begin{tikzpicture}
		\node[style={regular polygon,regular polygon sides=4}, draw=black] (A) at (0,0) {$v_0$};
		\node[style={regular polygon,regular polygon sides=4}, draw=black] (B) at (1.75,0) {$v_1$};
		\node[shape=circle, draw=black] (C) at (3.5,0) {$v_2$};
		\node[shape=circle, draw=black] (D) at (0,-1.75) {$v_3$};
		\node[shape=circle, draw=black] (E) at (3.5,-1.75) {$v_4$};
		\path [->] (A) edge (B);
		\path [->] (B) edge[out=15, in=165] (C);
		\path [->] (C) edge[out=195, in=345] (B);
		\path [->] (C) edge (E);
		\path [->] (E) edge[out=285, in=255,looseness=8] (E);
		\path [->] (A) edge (D);
		\path [->] (B) edge (D);
		\path [->] (D) edge[out=285, in=255,looseness=8] (D);
	\end{tikzpicture}
\caption{Arena A}
\end{wrapfigure}

\textbf{Example 1:}

An Arena
\begin{gather*}
	A=(\{v_0,v_1,v_2,v_3,v_4\},(\{v_0,v_1\},\{v_2,v_3,v_4\}),\\
	\{(v_0,v_1),(v_0,v_3),(v_1,v_2),(v_2,v_1),\\
	(v_2,v_4),(v_4,v_4),(v_1,v_3),(v_3,v_3)\})
\end{gather*}
\subsection{Positions, Moves}
A \textit{position}, $\pi_i=(v_0, v_1, \ldots, v_i)$, in a Game describes a finite path on the underlying graph, i.e. a position is an element of $V^+$. E.g. in Fig. 1 starting at $v_0$, $(v_0,v_1,v_2,v_4)$ could be a play.\\
A \textit{move} is an extension of a position in the graph by one more step. E.g. in Fig. 1 $(v_0,v_1,v_2)\mapsto (v_0,v_1,v_2,v_4)$ could be a move. Player i chooses the n-th move $(\ldots, v_{n-1})\mapsto (\ldots, v_{n-1}, v_n)$ for $v_{n-1}\in V_i\in (V_0,V_1)$.
\subsection{Strategies}
A \textit{strategy}, $V^*\times V_i\rightarrow V$, is a function by which player $i$ choses the next move for any given position.\newline
We call a strategy \textit{memoryless}, if for any given position the next move only depends on the last vertex of the position, i.e. $V_i\rightarrow V$.
We will refer to memoryless strategies of player 0 as $\sigma$ and of player 1 as $\tau$.\\
E.g. in Fig. 1 $\sigma = \{(v_0,v_1),(v_1,v_2)\}$ and $\tau = \{(v_2,v_1),(v_3,v_3),(v_4,v_4)\}$ could be strategies.\\
We call a memoryless strategy \textit{optimal}, if it achieves the best value possible for the respective player regardless of starting vertex. There may be multiple optimal strategies for any given game.
\subsection{Plays}
A \textit{play} describes the path of arbitrary length $\langle v_0, v_1, \ldots\rangle$ the player go trough in the process of playing the game.
We refer to the play generated by applying strategies $\sigma, \tau$ to game $G$ starting at $v_0 \in V$ as $\pi_{\sigma, \tau}(G, v_0)=\langle v_0, v_1, \ldots\rangle$\newline
E.g. if we take the previous example strategies and apply them to $\pi_{\sigma, \tau}(A,v_0)$ we get the play $\langle v_0, v_1, v_2, v_1, \ldots\rangle$. The play can be arbitrarily prolonged by applying the moves $\sigma\colon v_1\mapsto v_2$ and $\tau \colon v_2\mapsto v_1$.
\subsection{Infinite Games}
Infinite Games are a category of games played by two players on a finite, directed graph. They are infinite in the sense that we require the out-degree of every vertex to be at least one. As such, regardless of the strategies chosen by the players, they never terminate.
\subsubsection{Parity Games}
Parity Games are played by two players, \textit{Even} or player 0 ($\Square$) and \textit{Odd} or player 1 ($\Circle$).
A Parity Game, $PG =(A,p)$, is played on an Arena $A$ with a priority function $p\colon V\rightarrow\mathbb{N}_0$.\newline
Let $\pi_{\sigma, \tau}(PG, v_0)=\langle v_0, v_1, \ldots\rangle$ be the \textit{play} resulting from applying the strategies $\sigma$ and $\tau$ to Parity Game $PG$. Let
\begin{gather*}
	\#_\infty(\pi_{\sigma, \tau}(PG, v_0))=\\
	\{i\in \{0, 1, \ldots, \left|V\right|\}\mid\forall j \in \mathbb{N}_0\colon \exists n\in \mathbb{N}_0\colon j<\left|\langle v\in \langle v_0, \ldots, v_n\rangle\colon p(v)=i \rangle\right|\}
\end{gather*}
be the set of priorities that appear arbitrarily often in the play.\\
If $max(\#_\infty(\pi_{\sigma, \tau}(PG, v_0)))\coloneqq v_{\sigma,\tau}(v_0)$, the value of vertex $v_0$ is even, then \textit{Even} wins and vice versa. Optimal strategies for \textit{Even}/\textit{Odd} are those that result in the most/least starting vertices resulting in an even/odd value.

\begin{wrapfigure}[9]{l}{5.5cm}
	\begin{tikzpicture}
		\node[style={regular polygon,regular polygon sides=4}, draw=black,label={5}] (A) at (0,0) {$v_0$};
		\node[style={regular polygon,regular polygon sides=4}, draw=black,label={4}] (C) at (3.5,0) {$v_2$};
		\node[style={regular polygon,regular polygon sides=4}, draw=black,label={below:2}] (D) at (0,-1.75) {$v_3$};
		\node[style={regular polygon,regular polygon sides=4}, draw=black,label={below:3}] (F) at (3.5,-1.75) {$v_5$};
		\node[shape=circle, draw=black,label={2}] (B) at (1.75,0) {$v_1$};
		\node[shape=circle, draw=black,label={below:0}] (E) at (1.75,-1.75) {$v_4$};
		\path [->] (A) edge[out=15, in=165] (B);
		\path [->] (B) edge[out=195, in=345] (A);
		\path [->] (B) edge[out=15, in=165] (C);
		\path [->] (C) edge[out=195, in=345] (B);
		\path [->] (D) edge (A);
		\path [->] (E) edge (B);
		\path [->] (C) edge[out=285, in=75] (F);
		\path [->] (F) edge[out=105, in=255] (C);
		\path [->] (D) edge[out=15, in=165] (E);
		\path [->] (E) edge[out=195, in=345] (D);
		\path [->] (F) edge (E);
	\end{tikzpicture}
	\caption{PG}
\end{wrapfigure}

\textbf{Example 2:}

Playing $PG$ with $\sigma=\{(v_0,v_1),(v_2,v_5),(v_3,v_4),(v_5,v_2)\}$ and $\tau=\{(v_1,v_0),(v_4,v_1)\}$, the respective optimal strategies, results in play $\pi_{\sigma, \tau}(PG, v_0)=\langle v_0, v_1, v_0, ..\rangle$ for which $v_{\sigma,\tau}(v_0)=5$ is odd and the play is therefore winning for \textit{Odd}.
\subsubsection{Mean Payoff Games}
Mean Payoff Games are played by two players, \textit{Max} or player 0 ($\Square$) and \textit{Min} or player 1 ($\Circle$).
A Mean Payoff Game, $MPG = (A,w)$, is played on an Arena $A$ and an edge-weight function $w\colon E\rightarrow\{-d, \ldots, -1, 0, 1, \ldots, d\}$, $d\in \mathbb{N}_0$.\newline
\textit{Max} aims to chose their strategy for a given play $\pi_{\sigma, \tau}(MPG, v_0)=\langle v_0, v_1, \ldots\rangle$ such as to maximize the \textit{mean payoff} 
\begin{align*}
	\liminf\limits_{n\rightarrow \infty} \left(\dfrac{1}{n}\sum_{i=0}^{n-1}w((v_i,v_{i+1}))\right).
\end{align*}
We call this the \textit{value} $v_{\sigma,\tau}(v)$ of a vertex $v$. Given that those values are real numbers, we will round them from here on out. Optimal strategies are those that maximize/minimize the mean payoff for a given MPG regardless of the initial vertex.

\begin{wrapfigure}[9]{l}{4.5cm}
	\begin{tikzpicture}
		\node[style={regular polygon,regular polygon sides=4}, draw=black] (A) at (0,0) {$v_0$};
		\node[shape=circle, draw=black] (B) at (1.75,0) {$v_1$};
		\node[shape=circle, draw=black] (C) at (3.5,0) {$v_2$};
		\node[shape=circle, draw=black] (D) at (0,-1.75) {$v_3$};
		\node[shape=circle, draw=black] (E) at (1.75,-1.75) {$v_4$};
		\path [->] (A) edge[out=105, in=75,looseness=8] node[above]{$2$} (A);
		\path [->] (B) edge node[above]{$4$} (A);
		\path [->] (C) edge node[above]{$-2$} (B);
		\path [->] (C) edge[out=105, in=75,looseness=8] node[above]{$-4$} (C);
		\path [->] (D) edge node[right]{$1$} (A);
		\path [->] (E) edge node[above]{$-1$} (D);
		\path [->] (E) edge node[right]{$2$} (B);
		\path [->] (E) edge[out=15, in=345,looseness=8] node[right]{$1$} (E);
	\end{tikzpicture}
	\caption{MPG}
\end{wrapfigure}

\textbf{Example 3:}

A Mean Payoff Game\\
$MPG=(A,\{((v_0,v_0), 2),((v_1,v_0), 4), \ldots\})$.\\
Playing $MPG$ with $\sigma=\{(v_0,v_0)\}$ and\newline
$\tau=\{(v_1,v_0),(v_2,v_2),(v_3,v_0),(v_4,v_4)\}$, the respective optimal strategies, results in the values\\
$v_{\sigma,\tau}\colon v_0\mapsto 2, v_1\mapsto 2, v_2\mapsto -4, v_3\mapsto 2, v_4\mapsto 1$\\\\\\
\subsubsection{Energy Games}
Energy Games are played by two players, \textit{Charging} or player 0 ($\Square$) and \textit{Depleting} or player 1 ($\Circle$).
An Energy Game, $EG = (A,w)$, is played on an Arena $A$ and an edge-weight function $w\colon E\rightarrow\{-d, \ldots, -1, 0, 1, \ldots, d\}$, $d\in \mathbb{N}_0$.\newline
\textit{Charging} aims to chose their strategy for a given play $\pi_{\sigma, \tau}(EG, v_0)=\langle v_0, v_1, \ldots\rangle$ such as to minimize the \textit{initial credit} $c\in \mathbb{N}_0$ needed to maintain the winning condition
\begin{align*}
	\forall k\in\mathbb{N}_0\colon\left(\sum_{i=0}^{k}w((v_i,v_{i+1}))\right)+c\geq0.
\end{align*}
We call the smallest inital credit that still maintains the winning condition for a given play the \textit{minimum initial credit} or \textit{value} $v_{\sigma,\tau}(v)$ of a vertex $v$. If no such value exists, e.g. when the vertex is of \textit{Depleting} and has a self-loop with negative edge-weight, we write $v_{\sigma,\tau}(v)=\infty$. For any given position $\langle v_0, v_1, \ldots, v_k\rangle$ in a play we call $\left(\sum_{i=0}^{k-1}w((v_i,v_{i+1}))\right) + c$ the \textit{energy level} at that position. Keep in mind that \textit{Charging} doesn't necessarily aim to maximise their energy level at \textit{any} specific position but rather to maintain a non-negative energy level at \textit{every} position.

The goal for \textit{Charging} is to avoid getting trapped in cycles with overall negative edge-weight, as these can deplete any initial credit given, as well as to minimize the initial credit necessary to the compliant with the winning condition in all other cycles.
Conversely, the goal for \textit{Depleting} is to trap \textit{Charging} in negative cycles or at least to maximize the initial credit necessary for \textit{Charging} to reach a non-negative cycle.
Optimal strategies are those that minimize the initial credit necessary for \textit{Charging} and those that either deny the winning condition entirely or maximize the initial credit necessary for \textit{Depleting}.

\textbf{Example 4:}

Let us reappropriate Arena $A$ and edge-weight function $w$ of the previous MPG example and create an Energy Game $EG=(A,0,4,\{((v_0,v_0), 2),((v_1,v_0), 4), \ldots\})$.\\
Playing $EG$ with $\sigma=\{(v_0,v_0)\}$ and $\tau=\{(v_1,v_0),(v_2,v_2),(v_3,v_0),(v_4,v_3)\}$, the respective optimal strategies, results in the values $v_{\sigma,\tau}\colon v_0\mapsto 0,v_1\mapsto 0,v_2\mapsto \infty,v_3\mapsto 0,v_4\mapsto 1$.
\subsubsection{Discounted Payoff Games}
Discounted Payoff Games are played by two players, \textit{Max} or player 0 ($\Square$) and \textit{Min} or player 1 ($\Circle$).
A Discounted Payoff Game, $DPG = (A,w,\lambda)$, is played on an Arena $A$, an edge-weight function $w\colon E\rightarrow\{-d, \ldots, -1, 0, 1, \ldots, d\}$, $d\in \mathbb{N}_0$ and a discount factor $0<\lambda<1$.\newline
\textit{Max} aims to chose their strategy for a given play $\pi_{\sigma, \tau}(DPG, v_0)=\langle v_0, v_1, \ldots\rangle$ such as to maximize the \textit{discounted payoff} 
\begin{align*}
	(1-\lambda)\left(\sum_{i=0}^{\infty}\lambda^i\cdot w((v_i,v_{i+1}))\right).
\end{align*}
Again we call this the \textit{value} $v_{\sigma,\tau}(v)$ of a vertex $v$, we will round them from here on out. Optimal strategies are those that maximize/minimize the discounted payoff for a given MPG regardless of the initial vertex.

\textbf{Example 5:}

Let us again reappropriate Arena $A$ and edge-weight function $w$ and create a Discounted Payoff Game $DPG=(A,\{((v_0,v_0), 2),((v_1,v_0), 4), \ldots\}, 0.95)$.\\
Playing $DPG$ with $\sigma=\{(v_0,v_0)\}$ and $\tau=\{(v_1,v_0),(v_2,v_2),(v_3,v_0),(v_4,v_4)\}$, the respective optimal strategies, results in the values $v_{\sigma,\tau}\colon v_0\mapsto 2,v_1\mapsto 2.1,v_2\mapsto -4,v_3\mapsto 1.95,v_4\mapsto 1$.
\subsection{Simple Stochastic Games}
Simple Stochastic Games are played by two players, \textit{Max} or player 0 ($\Square$) and \textit{Min} or player 1 ($\Circle$).
A Simple Stochastic Game, $SSG = (G, (V_{0}, V_{1}, V_{2}), \mathfrak{0}, \mathfrak{1}, p)$, is played on a Directed Graph G, with a partition $(V_{0}, V_{1}, V_{2})$, two sink vertices $\mathfrak{0}, \mathfrak{1}$ and a probability function $p\colon V_2\rightarrow (V\rightarrow [0,1])$. We require that the probabilites of all outgoing edges of each $V_2$ vertex sum to 1: $\forall v\in V_2\colon\sum_{u \in V}p(v)(u)=1$ and that $\forall (u,v)\in(V_2\times V)\setminus E\colon p_u(v)=0$.
On vertices $v\in V_2$, also called \textit{Average} or \textit{Random} ($\Diamond$) vertices, the next vertex gets chosen according to the probability function $p_v\colon V\rightarrow[0,1]$ rather than a player.
We also require, like for Arenas, that the out-degree of every vertex is at least one, with the exception of $\mathfrak{0}, \mathfrak{1}$, for which it is zero.\newline
\textit{Max} wins if the $\mathfrak{1}$ sink is reached, \textit{Min} wins if the $\mathfrak{0}$ sink is reached or the game doesn't terminate.
Since the result of a play $\pi_{\sigma, \tau}(SSG, v_0)$ can be probablistic, it is assigned a probabilty to reach the $\mathfrak{1}$ sink rather than a fixed value. We display this probability as a rounded real number.
We say that a SSG is \textit{stopping} if for every possible position in a play, so given two fixed strategies $\sigma, \tau$, there is a path to one of the sink vertices. All SSGs we will consider will be stopping-SSGs.
Since stopping-SSGs terminate, they are not Infinite Games. For simplicity, we will still include SSGs under the \textit{Infinite Games} label from here on out.
The goal for \textit{Max} in a stopping-SSG is to maximize the chance to reach the $\mathfrak{1}$ vertex, the goal for \textit{Min} to reach the $\mathfrak{0}$ vertex. Optimal strategies are those that maximize the chance to reach the desired sink-vertex of the respective player regardless of the initial vertex.
\begin{wrapfigure}[7]{l}{4.5cm}
	\begin{tikzpicture}
		\node[style={regular polygon,regular polygon sides=4}, draw=black] (A) at (0,0) {$v_0$};
		\node[shape=diamond, draw=black] (B) at (1.75,0) {$v_1$};
		\node[shape=diamond, double, draw=black] (C) at (3.5,0) {$\mathfrak{0}$};
		\node[shape=circle, draw=black] (D) at (0,-1.75) {$v_2$};
		\node[shape=diamond, draw=black] (E) at (1.75,-1.75) {$v_3$};
		\node[shape=diamond, double, draw=black] (F) at (3.5,-1.75) {$\mathfrak{1}$};
		\path [->] (A) edge (B);
		\path [->] (A) edge[out=45, in=135] (C);
		\path [->] (B) edge[color=red] node[above,color=red]{$.53$} (C);
		\path [->] (B) edge[color=green] node[right,color=green]{$.42$} (E);
		\path [->] (B) edge[color=blue] node[above,color=blue]{$.05$} (F);
		\path [->] (D) edge (A);
		\path [->] (D) edge (B);
		\path [->] (E) edge[color=yellow] node[right,color=yellow]{$.16$} (A);
		\path [->] (E) edge[color=cyan] node[below,color=cyan]{$.3$} (C);
		\path [->] (E) edge[color=magenta] node[below,color=magenta]{$.55$} (D);
	\end{tikzpicture}
	\caption{SSG}
\end{wrapfigure}

\textbf{Example 6:}

A Simple Stochastic Game
\begin{gather*}
	SSG=(G,(\{v_0\},\{v_2\},\{v_1,v_3\}),\mathfrak{0},\mathfrak{1},\\
	\{(v_1,\{(\mathfrak{0},.53),(\mathfrak{1},.05),(v_3,.42)\}),\\
	(v_3,\{(\mathfrak{0},.3),(v_0,.16),(v_2,.55)\})\}).\\
\end{gather*}\\
Playing $SSG$ with $\sigma=\{(v_0,v_1)\}$ and $\tau=\{(v_2,v_0)\}$, the respective optimal strategies, results in the values $v_{\sigma,\tau}\colon v_0\mapsto .07,v_1\mapsto .07,v_2\mapsto .07,v_3\mapsto .05,\mathfrak{0}\mapsto 0,\mathfrak{1}\mapsto 1$.
\subsection{Reductions}
A \textit{reduction} describes an algorithm with which we can transform (reduce) one class of problems to another. For our purposes, we choose target problem classes that are already solved. Aside from reducing to an already solved problem class and therefore, per definition, also solving the original problem class, reducing and subsequent solving of instances of the original problem may be faster than any attempt to directly solve the original problem without reducing to another intermediary infinite game problem.\newline
Formally, we express our problem classes as formal languages $A$ and $B$ over the alphabets $\Sigma^*$ and $\Gamma^*$. If $f$ is totally computable and
\begin{gather*}
	f\colon \Sigma^*\rightarrow\Gamma^*, A\subseteq \Sigma^*, B\subseteq \Gamma^*\\
	\forall w\in\Sigma^*\colon w\in A \iff f(w)\in B
\end{gather*}
then $f$ is a reduction from $\Sigma^*$ to $\Gamma^*$.\newline
In practice our formal languages $A$ and $B$ will be some instances of types of infinite games together with statements about such games, such as the values under perfect play or perfect strategies. The reduction will then draw an equivalence to the other infinite game, e.g. if $w=``v(v_u)=5$ in $G$'' is a word in $A$ then $f(w) = ``v(v_v)=16$ in $H$'' is a word in $B$.
\section{Solutions and Reductions in Theory}
Before we go into the specifics of the implementation of the algorithms used to solve the problems and the reductions we first want to comprehensively explain the theory behind each of them.
\subsection{Solutions in Theory}
One of the key parts of our work is the solving of problems on our games, be it in conjunction with a reduction or not.
First we shall explain the general idea of Kleene Iteration and Strategy Iteration, as they can be generalized and used to solve multiple of the problems posed. Then we will explain the specifics of each implementation of those as well as unique algorithms used to solve each of the games we are interested in.
\subsubsection{Kleene Iteration}
Kleene
\subsubsection{Strategy Iteration}
\subsubsection{PGs}
\subsubsection{MPGs}
\subsubsection{DPGs}
\subsubsection{EGs}
\subsubsection{SSGs}
\subsection{Reductions in Theory}
\subsubsection{PGs to MPGs}
The reduction from PGs to MPGs is based on the idea of \cite{DBLP:journals/ipl/Jurdzinski98}.
For a given $PG = (A,p)$ we  transform into a $MPG = (A,w)$, with the underlying arena $A$ being the same and the weights $w$ of $MPG$ based directly on the priority function $p$ of $PG$. Optimal strategies in $MPG$ translate one to one to $PG$ and the vertex $v$ having value $\geq0$ in $MPG$ translates to it being winning for \textit{Even} in $PG$. Recall that for both $PGs$ and $MPGs$ winning strategies can be memoryless.

Consider any play $\pi_{\sigma,\tau}$ in $A$ with $n=|V|$. Given that $A$ is finite and $\pi_{\sigma,\tau}$ can continue infinitely, any such play, regardless of starting vertex $v_0$ will necessarily have a repeat vertex after a path of length $k\leq n$ and will then, by virtue of the strategies being memoryless, indefinitely continue in a cycle of length $l\leq n-k$.

From the definition of the value of a $PG$ it follows directly that only the priorities of the nodes in the cycle matter for the outcome to the game, specifically the highest priority. For MPGs we have:
\begin{align*}
	&\liminf\limits_{n\rightarrow \infty} \left(\dfrac{1}{n}\sum_{i=0}^{n-1}w((v_i,v_{i+1}))\right)=\\
	&\liminf\limits_{n\rightarrow \infty} \left(\dfrac{1}{n}\sum_{i=k}^{n-1}w((v_i,v_{i+1}))\right)+
	\liminf\limits_{n\rightarrow \infty} \left(\dfrac{1}{n}\sum_{i=0}^{k-1}w((v_i,v_{i+1}))\right)=\\
	&\liminf\limits_{n\rightarrow \infty} \left(\dfrac{1}{n}\sum_{i=0}^{n-1}w((v_{i+k},v_{i+1+k}))\right)=\\
	&\liminf\limits_{n\rightarrow \infty} \left(\dfrac{1}{n}\sum_{i=0}^{n-1}\dfrac{1}{l}\sum_{j=0}^{l-1}w((v_{j+k},v_{j+1+k}))\right)=\\
	&\dfrac{1}{l}\sum_{j=0}^{l-1}w((v_{j+k},v_{j+1+k}))
\end{align*}
So again, the only edge-weights that matter for the value of a play are those that are part of the terminal cycle.

To equate the values of the games we want to set the edge-weights in such a way that the outgoing edge of any vertex of a given cycle alone can set the parity of the value of the game if and only if the priority of the vertex is the highest in the cycle.\\
\cite{DBLP:journals/ipl/Jurdzinski98} does this by setting $w((u,v)) = (-n)^{p(u)}$. This way, the absolute weight of the outgoing edge of the vertex with highest priority is always higher than any possible make-up of the rest of the cycle, i.e. $|(-n)^{p(u)}|>(n-1)|(-n)^{p(u)-1}|$.\\
This, however, is unsatisfying since the complexity of solving MPGs with the algorithms we use is dependent on $d$, the maximum absolute weight in the MPG. We therefore want to be conservative with the edge-weights and try to minimize them.\\
To that end, we set our weights as follows:
\begin{align*}
	w((u,v)) = (-1)^{p(u)}\cdot (sum\langle z\in \{|w((x,y))|\mid (x,y)\in E\}\mid x\in V_u\footnotemark\rangle +1^{p(u)~mod~2})
\end{align*}
\footnotetext{$V_u = \{ v\in V\mid p(x)<p(u)\land (p(x)\neq p(u)~\mathrm{mod}~2)\}$ and therefore $V_u = \emptyset$ for $u = min\{p(v)\mid v\in V\}$}
This way the vertex with highest priority has an outgoing edge which, per definition, dominates the value of the MPG since it has a higher absolute weight than all vertices of lower priority of the opponent, regardless of their counterstrategy $\tau$. Thus any play on PG whose value is \textit{Even/Odd} will have a value of $\geq 0/\leq -1$ when translated to an MPG in such a way and any optimal strategy in \textit{PG} will accordingly be an optimal strategy in \textit{MPG}.
Conversely, any MPG that will be reduced-to in such a way will have an edge in it's terminal cycle whose absolute weight will be greater than the sum of all the edges of the opponent in said cycle. The vertex said greatest edge emanates from being exactly equivalent to the vertex with highest priority in the original PG. Thus any play on such MPG whose value, or equivalently whose greatest edge, is $\geq 0/\leq -1$ will have \textit{Even/Odd} value in the original MPG and any optimal strategy in MPG will be a optimal strategy in PG.
\begin{wrapfigure}[8]{l}{4.5cm}
	\begin{tikzpicture}
		\node[style=circle, draw=black, label={1}] (A) at (0,0) {$v_0$};
		\node[style={regular polygon,regular polygon sides=4}, draw=black,label={5}] (B) at (1.75,0) {$v_1$};
		\node[shape=circle, draw=black,label={4}] (C) at (3.5,0) {$v_2$};
		\node[shape=circle, draw=black,label={below:4}] (D) at (0,-1.75) {$v_3$};
		\node[style=circle, draw=black,label={below:0}] (E) at (1.75,-1.75) {$v_4$};
		\node[style={regular polygon,regular polygon sides=4}, draw=black,label={below:3}] (F) at (3.5,-1.75) {$v_5$};
		\path [->] (A) edge (B);
		\path [->] (B) edge[out=15, in=165] (C);
		\path [->] (C) edge[out=195, in=345] (B);
		\path [->] (B) edge (E);
		\path [->] (F) edge (C);
		\path [->] (E) edge[out=60, in=210] (C);
		\path [->] (C) edge[out=240, in=30] (E);
		\path [->] (D) edge (A);
		\path [->] (D) edge (E);
		\path [->] (E) edge[out=345, in=285, looseness=4] (E);
	\end{tikzpicture}
	\caption{PG}
	\begin{tikzpicture}
		\node[style=circle, draw=black] (A) at (0,0) {$v_0$};
		\node[style={regular polygon,regular polygon sides=4}, draw=black] (B) at (1.75,0) {$v_1$};
		\node[shape=circle, draw=black] (C) at (3.5,0) {$v_2$};
		\node[shape=circle, draw=black] (D) at (0,-1.75) {$v_3$};
		\node[style=circle, draw=black] (E) at (1.75,-1.75) {$v_4$};
		\node[style={regular polygon,regular polygon sides=4}, draw=black] (F) at (3.5,-1.75) {$v_5$};
		\path [->] (A) edge node[above]{-1} (B);
		\path [->] (B) edge[out=25, in=155] node[above]{-5} (C);
		\path [->] (C) edge[out=195, in=345] node[above]{2} (B);
		\path [->] (B) edge node[left]{-5} (E);
		\path [->] (F) edge node[right]{-1} (C);
		\path [->] (E) edge[out=60, in=210] node[left]{0} (C);
		\path [->] (C) edge[out=240, in=30] node[right]{2} (E);
		\path [->] (D) edge node[left]{2} (A);
		\path [->] (D) edge node[below]{2} (E);
		\path [->] (E) edge[out=345, in=285, looseness=4] node[right]{0} (E);
	\end{tikzpicture}
	\caption{MPG}
\end{wrapfigure}\\
\textbf{Example 7:} We translate $PG$ to $MPG$.\\
The edge-weights are successively built as follows:\\\\
\begin{tabular}{c|c|c|c|c}
	$u\in V$ & $p(u)$ & $V_u$ & \footnotemark & $w((u,\_))$\\\hline
	$v_4$ & $0$ & $\emptyset$ & $\emptyset$ & 0 \\\hline
	$v_0$ & $1$ & $\{v_4\}$ & $\langle 0\rangle$ & -1 \\\hline
	$v_5$ & $3$ & $\{v_4\}$ & $\langle 0\rangle$ & -1 \\\hline
	$v_2, v_3$ & $4$ & $\{v_0, v_5\}$ & $\langle 1, 1\rangle$ & 2 \\\hline
	$v_1$ & $5$ & $\{v_2, v_3, v_4\}$ & $\langle 2, 2, 0\rangle$ & -5 \\\hline
\end{tabular}\\\\\\
\footnotetext{$\langle z\in \{|w((x,y))|\mid (x,y)\in E\}\mid x\in V_u\rangle$}
Here we can limit $d$ to $5$ instead of $|(-6)^5|=7776$.\newpage
\subsubsection{MPGs to DPGs}
The reduction from MPGs to DPGs is the one described in \cite{ZWICK1996343}. The underlying Arena $A$ and edge-weights $w$ stay the same.
Recall from 3.2.1 that a play in an Arena eventually results in a repeat vertex after a path of length $k\leq n$ and then indefinitely continues in a cycle of length $l\leq n-k$. The value of such play being:
\begin{align*}
	\dfrac{1}{l}\sum_{j=0}^{l-1}w((v_{j+k},v_{j+1+k}))
\end{align*}
For DPGs we have the value being:
\begin{align}
	v(v_0)=~&(1-\lambda)\left(\sum_{i=0}^{\infty}\lambda^i\cdot w((v_i,v_{i+1}))\right)=\nonumber\\
	&(1-\lambda)\left(\sum_{i=k}^{\infty}\lambda^i\cdot w((v_i,v_{i+1}))\right)+(1-\lambda)\left(\sum_{i=0}^{k-1}\lambda^i\cdot w((v_i,v_{i+1}))\right)\nonumber\\
	&(1-\lambda)\left(\sum_{i=0}^{\infty}\lambda^{i+k}\cdot w((v_{i+k},v_{i+1+k}))\right)+(1-\lambda)\left(\sum_{i=0}^{k-1}\lambda^i\cdot w((v_i,v_{i+1}))\right)\nonumber\\
	&(1-\lambda)\lambda^k\left(\sum_{i=0}^{\infty}\lambda^{il}\cdot \sum_{j=0}^{l-1}\lambda^{j}\cdot w((v_{j+k},v_{j+1+k}))\right)+(1-\lambda)\left(\sum_{i=0}^{k-1}\lambda^i\cdot w((v_i,v_{i+1}))\right)\nonumber\\
	&\frac{(1-\lambda)\lambda^k}{1-\lambda^l}\left(\sum_{j=0}^{l-1}\lambda^{j}\cdot w((v_{j+k},v_{j+1+k}))\right)+(1-\lambda)\left(\sum_{i=0}^{k-1}\lambda^i\cdot w((v_i,v_{i+1}))\right)
\end{align}
Now if we let $\lambda\to 1^-$ we get:
\begin{align*}
	&\lim_{\lambda\to 1^-}\left(\frac{(1-\lambda)\lambda^k}{1-\lambda^l}\left(\sum_{j=0}^{l-1}\lambda^{j}\cdot w((v_{j+k},v_{j+1+k}))\right)+(1-\lambda)\left(\sum_{i=0}^{k-1}\lambda^i\cdot w((v_i,v_{i+1}))\right)\right)=\\
	&\lim_{\lambda\to 1^-}\left(\frac{1-\lambda}{1-\lambda^l}\sum_{j=0}^{l-1}w((v_{j+k},v_{j+1+k}))\right)=\\
	&\frac{1}{l}\sum_{j=0}^{l-1}w((v_{j+k},v_{j+1+k}))
\end{align*}
Since we can't actually let $\lambda$ be 1, we have to use a $\lambda$ that is sufficiently large enough as to enable us to truncate to the nearest rational number as was done in 3.1.4.1.
We can take (2) and rescale all the weights by W, such that all weights are $\geq0$:
\begin{align*}
	&v(v_0)+W=\\
	&\frac{(1-\lambda)\lambda^k}{1-\lambda^l}\left(\sum_{j=0}^{l-1}\lambda^{j}\cdot \left(w((v_{j+k},v_{j+1+k}))+W\right)\right)+(1-\lambda)\left(\sum_{i=0}^{k-1}\lambda^i\cdot \left(w((v_i,v_{i+1}))+W\right)\right)\geq\\
	&\frac{(1-\lambda)\lambda^k}{1-\lambda^l}\left(\sum_{j=0}^{l-1}\lambda^{j}\cdot \left(w((v_{j+k},v_{j+1+k}))+W\right)\right)\geq\\
	&\frac{(1-\lambda)\lambda^{k+l-1}}{1-\lambda^l}\left(\sum_{j=0}^{l-1}\left(w((v_{j+k},v_{j+1+k}))+W\right)\right)\geq\\
	&(1-n(1-\lambda))\left(\sum_{j=0}^{l-1}\left(w((v_{j+k},v_{j+1+k}))+W\right)\right)\geq\\
	&(1-n(1-\lambda))\left(W+\sum_{j=0}^{l-1}w((v_{j+k},v_{j+1+k}))\right)
\end{align*}
Now if we revert the scaling we arrive at:
\begin{align*}
	&v(v_0)+W\geq(1-n(1-\lambda))\left(W+\sum_{j=0}^{l-1}w((v_{j+k},v_{j+1+k}))\right)\Leftrightarrow\\
	&v(v_0)+W\geq W+\sum_{j=0}^{l-1}w((v_{j+k},v_{j+1+k}))-n(1-\lambda)\left(W+\sum_{j=0}^{l-1}w((v_{j+k},v_{j+1+k}))\right)\Leftrightarrow\\
	&v(v_0)\geq\sum_{j=0}^{l-1}w((v_{j+k},v_{j+1+k}))-n(1-\lambda)\left(W+\sum_{j=0}^{l-1}w((v_{j+k},v_{j+1+k}))\right)\geq\\
	&\sum_{j=0}^{l-1}w((v_{j+k},v_{j+1+k}))-n(1-\lambda)\cdot 2W
\end{align*}
Similarly, we can rescale by -W, such that all weights are $\leq0$ and in total we arrive at:
\begin{align*}
	\sum_{j=0}^{l-1}w((v_{j+k},v_{j+1+k}))+n(1-\lambda)\cdot 2W\geq v(v_0)\geq\sum_{j=0}^{l-1}w((v_{j+k},v_{j+1+k}))-n(1-\lambda)\cdot 2W
\end{align*}
From 3.1.4 we know that the minimum distance between two valid values of a MPG is $\frac{1}{n(n-1)}$, therefore we want to set $\lambda$ in such a way as to constrict the size of the just derived interval to:
\begin{align*}
	|2n(1-\lambda)\cdot 2W|\leq\frac{1}{n(n-1)}
\end{align*}
We get this precision with $\lambda = 1-\frac{1}{4n^3W}$.
If we reduce a MPG to a DPG with such $\lambda$, we can deduce the value for a given vertex of the original MPG by taking the corresponding value $v$ in the DPG and rounding to the unique rational number, with denominator at most $n$, in the closed interval $v\pm \frac{1}{2n(n-1)}$.

\subsubsection{MPGs to EGs}
The reduction from MPGs to EGs is the one described in \cite{Brim2011}.\\
If we play an EG on $A, w$ of a MPG, any vertex for which the value is $v_{\sigma, \tau}(v)\neq\infty$ is equal to a vertex in the MPG whose value is $v_{\sigma, \tau}(v)\geq0$, since the player can force a terminal cycle whose edge-weights sum up to $\geq0$. We can use this dichotomy together with the fact that we can rescale the edge-weights of MPGs, allowing us to precisely determine any value of the MPG in the EG, to repeatedly solve rescaled MPGs and restrict the possible values until only one possible solution remains.\\\\
For a given set of strategies $\sigma, \tau$, any move will land on a vertex with the same value as the previous since the play ultimately always lands in the same terminal cycle with the same mean weight. This means that, for a fixed set of strategies, vertices with different values will necessarily be disconnected if one removes the edges that aren't in play under $\sigma, \tau$ and the values of all vertices stay the same if those edges are removed.\\
What this allows us to do is that after every dichotomy wrt. the values, where we split the set of vertices into three sets, one with value $>\nu$, one with value $<\nu$ and one with known value $\nu$. We can restrict subsequent dichotomies to disjoint subsets of the graph, since we know that vertices with different value, as shown to be the case by the dichotomy, have to be disconnected if unused edges are removed.
\subsubsection{DPGs to SSGs}
\section{Solutions and Reductions in Practice}
\subsection{Solutions in Practice}
\subsection{Reductions in Practice}
\section{Implementation}\newpage
\section{Evaluation}
Our goal was to compare the different algorithms used to solve problems, especially under reduction to other games, and to see how they compare. To that end we generated random games with multiple parameters and benchmarked all methods we had to solve the respective problems. For any combination of problem, parameters for generation and algorithm used to solve the problem we have $N = 100$ and the times shown repesent the average time of those 100 runs. For their generation, all games have a parameter $n =|V|$ that measures the size of the graph by means of the number of vertices and a parameter $p$ which describes the probability that any possible edge $e\in (V\times V)$ is part of the generated game, meaning the expected out-degree of every vertex is $|V|\cdot p$. Since we require for all games that each vertex has outdegree $\geq 1$, we consequently require that $\frac{1}{|V|} \leq p \leq 1$. We then assign each vertex one random edge and all other potential outgoing edges of that vertex have probability $\frac{(p\cdot |V|)-1}{|V|-1}$ of existing. $p$ can be understood as a measure of how connected a graph is. In fact, towards the extreme ends, for $p=\frac{1}{|V|}$ and sufficiently large $|V|$ one would expect a disconnected graph on one end and for $p=1$ a complete graph on the other end. Each vertex is randomly assigned to a player with equal probability, including the ``Random'' player in SSGs. Games may have additional parameters that are described seperately for each game. The parameters were chosen in a way that they intuitively represent extremes towards each end and then some reasonable middle ground. For all cases, the runtimes are either weakly dependent on $p$ other parameters or they scale monotonously wrt. to those parameters. One can therefore think of intermediate parameterizations as some interpolation between the shown values. For readability we only show some algorithms here. Those that work without reduction to another game and any other that are of interest. The full results are in the appendix.
\subsection{PGs}
For Parity games, the additional parameter is $w$, which describes the range over which the priorities are handed out. Similar to the ownership of the vertices, the priority of vertices is randomly chosen from the possible range with equal chance for each. In theory as well as in this implementation, for the purpose of solving the problems one can rescale the range of the priorities to $\{0, 1, ... |V|\}$ or smaller without affecting the values or perfect strategies of the game. A higher $w$ however decreases the overlap of priorities between vertices on generation, which has a minor impact on the runtime of Zielonka's algorithm.

In theory, Strategy Iteration via DPGs ought to be a contender for large enough $n$ for both the value and stratey problem. The reason for why it isn't depends on the reduction from MPGs to DPGs and is laid out in the subsection for MPGs.

Strategy Iteration from above via EGs can outperform Zielonka's algorithm for large $|V|$ and reasonably dense graphs. However, the break-even point is only reached for such large $|V|$ and consequently such long runtimes that gathering statistically significant runtime data is impractical, it can however be confirmed for individual instances. The exact inflection points for the parameterization when the Strategy Iteration starts outperforming Zielonka's algorithm are therefore not known.
\subsubsection{Value Problem of PGs}
For solving the Value Problem of PGs the result are very clear. For all cases tested, Zielonka's Algorithm is straight up the fastest. It's simplicity demands very little overhead and as such small graphs are solved near instantaneously. It's recursive nature allows it to scale very well to big graphs as well. Less connectivity, as shown by smaller $p$, mean the attractors as laid out in the algorithm tend to not reach as far -- meaning more levels of recursion are needed to cover the entire graph. Similargly, bigger $w$ means that the winning regions that act as a crystallisation point for the attracters tend to start out smaller and thus form smaller attractors.
\begin{figure}[H]
	\centering
	\input{pg_value}
	\caption{Solving the value problem of PGs}
\end{figure}\newpage
\subsubsection{Strategy Problem of PGs}
Unfortunately our naive approach to finding optimal strategies via Zielonka's algorithm is $|V|\cdot log_2(\frac{|E|}{|V|})$ times slower than finding the values alone, meaning the big lead it had in finding the values of games is diminished significantly, bringing it more in line with other algorithms. Depending on the exact $p$ and $w$, the BCDGR algorithm via EGs, which has similarly low overhead to Zielonka's algorithm, but doesn't require repeated invokation to deliver optimal stragies, has better runtimes for reasonably small graphs. However, since it scales worse, such advantage is quickly lost with growing $|V|$. After that, Zielonka's algorithm, even with the malus it suffer for finding optimal strategies, is the fastest available method until Strategy Iteration from above via EGs takes over.
\begin{figure}[H]
	\centering
\input{pg_strat}
\caption{Solving the strategy problem of PGs}
\end{figure}\newpage
\subsection{MPGs}
As with Parity games, the additional parameter is $w$ describing the range over which the priorities are handed out and for our random generation they are once again randomly uniformly distributed over their range.
Unlike with PGs, in this case the meaning of $w$ cannot be scaled down to make subsequently applied algorithms faster. As a result we see some algorithms scale strongly in response to changing $w$ across it's range and even beyond.

In theory, Strategy Iteration via DPGs is faster than other algorithms for large enough $n$. In practise however this isn't easily achieved. The method by which MPGs are reduced to DPGs results in discount factors very close to $1$. So close in fact, that for big enough MPGs the difference $1-\lambda$ approaches the limit of the precision standard floating point implementations can represent \cite{754}. As a result, standard linear program solvers are unable to solve the linear programs that are part of the strategy iteration with adequate precision. One could conceive a linear program solver that works with larger floats that enable higher precision. However this only raises the ceiling of what is solveable marginally. It also eventually forces the floats to exceed standard register sizes, meaning the floating point arithmetic fundamental to the linear program solver would have to resort to emulating floating point arithmetic for larger floats, which would take more clock cycles per operation and ultimately longer runtimes, undermining the usefullness of the approach in the first place. Effectively this mean that, even if this approach is possible, it is never pratically viable for MPGs reduced to DPGs.\newpage
\subsubsection{Value Problem of MPGs}
Unlike for PGs the native approach used here, Zwick and Paterson's algorithm isn't a clear winner. Intuitively the $k = 4\cdot |V|^3\cdot d$ iterations the algorithm goes through to guarantee correcteness are way in excess of what's practically needed. This means it's only competetive for graphs with very small $|V|^3\cdot d$. As with PGs, solving with BCDGR via EG offers low overhead and thus reasonably fast runtimes for smaller graphs, but again as with PGs the approach scales mediocrely wrt. $|V|$.\\
As with PGs, Strategy Iteration from above via EGs eventually outperforms BCDGR via EG for some parameterizations. \textit{Unlike} with PGs, we don't suffer from an as large $d$ any more, meaning the approach becomes competitive much sooner. The exact inflection point depend on the specific MPG in question, with a relatively large overlap where either may be superior.
\begin{figure}[H]
	\centering
	\input{mpg_value}
	\caption{Solving the value problem of MPGs}
\end{figure}\newpage
\subsubsection{Strategy Problem of MPGs}
As with Zielonka's algorithm on PGs, Zwick and Paterson's algorithm for optimal strategies has an additional cost of $|V|\cdot log_2(\frac{|E|}{|V|})$ over finding the values of the game. Unlike with Zielonka's algorithm on PGs this makes Zwick and Paterson's algorithm unviable for all conditions for finding strategies. Since BCDGR and Strategy Iteration from above via EG both don't have any malus when finding optimal strategies over values, their relative performance stays the same is was for finding values.
\begin{figure}[H]
	\centering
	\input{mpg_strat}
	\caption{Solving the strategy problem of MPGs}
\end{figure}
\subsection{EGs}
\begin{figure}[H]
	\centering
	\input{eg_both}
	\caption{Solving both EG problems}
\end{figure}
\subsection{DPGs}
\begin{figure}[H]
	\centering
	\input{dpg_both}
	\caption{Solving both DPG problems}
\end{figure}
\footnotetext{$S(x)=\frac{1}{1+e^{-x}}$}
\subsection{SSGs}
sinks not included in p
no w
\begin{figure}[H]
	\centering
	\input{ssg_both}
	\caption{Solving both stopping-SSG problems}
\end{figure}
\section{Conclusion and further approaches}
